{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.2.0-cp310-none-macosx_11_0_arm64.whl.metadata (25 kB)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.8.0)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Using cached typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Collecting mpmath>=0.19 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.2.0-cp310-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, typing-extensions, sympy, fsspec, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed fsspec-2024.2.0 mpmath-1.3.0 sympy-1.12 torch-2.2.0 typing-extensions-4.9.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8008f9fe8842d49488e71ef8f66d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\".gitattributes\";:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8b8b267a574b6cabd476f0f7195eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ooling/config.json\";:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346ce60e178143edb811081c79436a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"README.md\";:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1245afbbdc3e45e0a7fb94542ab7824e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"config.json\";:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c04a5a9c2be4c939616ed76d140d541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_transformers.json\";:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbc7fcf875846ec8de51fa00d0a8462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"data_config.json\";:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4bc5d7e5a64dfb8e115f478b64e88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b2303668f34a66b888f08161503f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e_bert_config.json\";:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919af119fd034c0ba5c777954115e92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)al_tokens_map.json\";:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab03534e7894108868fa78af278e095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"tokenizer.json\";:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43c4496cea544efcb810400011506466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enizer_config.json\";:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "309484e9a9b24bca8d22f82fc49dcb2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"train_script.py\";:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0514fcbcaa7f40daaf8610c8e6227c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"vocab.txt\";:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9b10479e8f4e84a5104effd48bd72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"modules.json\";:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoder = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"clarify\", \"email\", \"link\", \"schedule\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([750, 768])\n"
     ]
    }
   ],
   "source": [
    "# Walk through all files in a directory\n",
    "import os\n",
    "lines = []\n",
    "for class_ in classes:\n",
    "    # Read contents of file\n",
    "    with open('data/action_classification/' + class_ + '.txt', 'r') as f:\n",
    "        lines += f.readlines()\n",
    "\n",
    "inputs = encoder.encode(lines)\n",
    "inputs = torch.Tensor(inputs)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([750, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for class_ in classes:\n",
    "    output_1he = torch.Tensor([0] * len(classes))\n",
    "    output_1he[classes.index(class_)] = 1\n",
    "    for i in range(150):\n",
    "        outputs.append(output_1he)\n",
    "\n",
    "outputs = torch.stack(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 384)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(384, 384)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(384, 384)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(384, 384)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc5 = nn.Linear(384, len(classes))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.gelu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.gelu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.gelu(self.fc5(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = ActionClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(data):\n",
    "    unique_elements, counts = np.unique(data, return_counts=True)\n",
    "    probabilities = counts / len(data)\n",
    "    entropy = -np.sum(probabilities * np.log2(probabilities))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 1.6092833280563354 train acc: 0.2150000035762787 test loss: 1.6100573539733887 best test loss: 1.6100573539733887 test acc: 0.14000000059604645\n",
      "Epoch 100 train loss: 1.4280565977096558 train acc: 0.4449999928474426 test loss: 1.5340031385421753 best test loss: 1.5340031385421753 test acc: 0.27000001072883606\n",
      "Epoch 200 train loss: 1.0811355113983154 train acc: 0.8450000286102295 test loss: 1.1846754550933838 best test loss: 1.1846754550933838 test acc: 0.75\n",
      "Epoch 300 train loss: 0.9466697573661804 train acc: 0.9599999785423279 test loss: 1.0640063285827637 best test loss: 1.058962345123291 test acc: 0.8500000238418579\n",
      "Epoch 400 train loss: 0.9405715465545654 train acc: 0.9649999737739563 test loss: 1.0655317306518555 best test loss: 1.0533347129821777 test acc: 0.8299999833106995\n",
      "Epoch 500 train loss: 0.9352471232414246 train acc: 0.9700000286102295 test loss: 1.0620434284210205 best test loss: 1.0394728183746338 test acc: 0.8399999737739563\n",
      "Epoch 600 train loss: 0.9341735243797302 train acc: 0.9700000286102295 test loss: 1.0683172941207886 best test loss: 1.0394728183746338 test acc: 0.8199999928474426\n",
      "Epoch 700 train loss: 0.9322068691253662 train acc: 0.9725000262260437 test loss: 1.0514086484909058 best test loss: 1.0330356359481812 test acc: 0.8399999737739563\n",
      "Epoch 800 train loss: 0.9323719143867493 train acc: 0.9725000262260437 test loss: 1.0543882846832275 best test loss: 1.0313228368759155 test acc: 0.8500000238418579\n",
      "Epoch 900 train loss: 0.9322285652160645 train acc: 0.9725000262260437 test loss: 1.0480798482894897 best test loss: 1.029375433921814 test acc: 0.8399999737739563\n",
      "Epoch 1000 train loss: 0.9323561191558838 train acc: 0.9725000262260437 test loss: 1.053602933883667 best test loss: 1.0289909839630127 test acc: 0.8500000238418579\n",
      "Epoch 1100 train loss: 0.9322139024734497 train acc: 0.9725000262260437 test loss: 1.0538463592529297 best test loss: 1.0289909839630127 test acc: 0.8500000238418579\n",
      "Epoch 1200 train loss: 0.932071328163147 train acc: 0.9725000262260437 test loss: 1.0475562810897827 best test loss: 1.0289909839630127 test acc: 0.8600000143051147\n",
      "Epoch 1300 train loss: 0.9323128461837769 train acc: 0.9725000262260437 test loss: 1.0555261373519897 best test loss: 1.0289909839630127 test acc: 0.8399999737739563\n",
      "Epoch 1400 train loss: 0.9321897029876709 train acc: 0.9725000262260437 test loss: 1.0443801879882812 best test loss: 1.0289909839630127 test acc: 0.8600000143051147\n",
      "Epoch 1500 train loss: 0.9321738481521606 train acc: 0.9725000262260437 test loss: 1.0419785976409912 best test loss: 1.0289909839630127 test acc: 0.8600000143051147\n",
      "Epoch 1600 train loss: 0.932170033454895 train acc: 0.9725000262260437 test loss: 1.0438518524169922 best test loss: 1.0289909839630127 test acc: 0.8600000143051147\n",
      "Epoch 1700 train loss: 0.9321392178535461 train acc: 0.9725000262260437 test loss: 1.0517369508743286 best test loss: 1.0272886753082275 test acc: 0.8500000238418579\n",
      "Epoch 1800 train loss: 0.9321588277816772 train acc: 0.9725000262260437 test loss: 1.0507280826568604 best test loss: 1.0272886753082275 test acc: 0.8500000238418579\n",
      "Epoch 1900 train loss: 0.9321890473365784 train acc: 0.9725000262260437 test loss: 1.0572786331176758 best test loss: 1.0272886753082275 test acc: 0.8399999737739563\n",
      "Epoch 2000 train loss: 0.9320713877677917 train acc: 0.9725000262260437 test loss: 1.0473911762237549 best test loss: 1.0272886753082275 test acc: 0.8600000143051147\n",
      "Epoch 2100 train loss: 0.9321746826171875 train acc: 0.9725000262260437 test loss: 1.048294186592102 best test loss: 1.027022123336792 test acc: 0.8500000238418579\n",
      "Epoch 2200 train loss: 0.932081401348114 train acc: 0.9725000262260437 test loss: 1.040696382522583 best test loss: 1.027022123336792 test acc: 0.8600000143051147\n",
      "Epoch 2300 train loss: 0.9322398900985718 train acc: 0.9725000262260437 test loss: 1.049977421760559 best test loss: 1.027022123336792 test acc: 0.8500000238418579\n",
      "Epoch 2400 train loss: 0.9320676326751709 train acc: 0.9725000262260437 test loss: 1.0548255443572998 best test loss: 1.027022123336792 test acc: 0.8500000238418579\n",
      "Epoch 2500 train loss: 0.9249104261398315 train acc: 0.9800000190734863 test loss: 1.0420907735824585 best test loss: 1.027022123336792 test acc: 0.8700000047683716\n",
      "Epoch 2600 train loss: 0.9248042106628418 train acc: 0.9800000190734863 test loss: 1.0365933179855347 best test loss: 1.027022123336792 test acc: 0.8700000047683716\n",
      "Epoch 2700 train loss: 0.9248156547546387 train acc: 0.9800000190734863 test loss: 1.0357866287231445 best test loss: 1.027022123336792 test acc: 0.8700000047683716\n",
      "Epoch 2800 train loss: 0.9246905446052551 train acc: 0.9800000190734863 test loss: 1.0452730655670166 best test loss: 1.027022123336792 test acc: 0.8600000143051147\n",
      "Epoch 2900 train loss: 0.9246927499771118 train acc: 0.9800000190734863 test loss: 1.0511504411697388 best test loss: 1.027022123336792 test acc: 0.8600000143051147\n",
      "Epoch 3000 train loss: 0.9247867465019226 train acc: 0.9800000190734863 test loss: 1.0567759275436401 best test loss: 1.025773048400879 test acc: 0.8500000238418579\n",
      "Epoch 3100 train loss: 0.9247182607650757 train acc: 0.9800000190734863 test loss: 1.055917501449585 best test loss: 1.025773048400879 test acc: 0.8500000238418579\n",
      "Epoch 3200 train loss: 0.9247459173202515 train acc: 0.9800000190734863 test loss: 1.0357052087783813 best test loss: 1.025773048400879 test acc: 0.8700000047683716\n",
      "Epoch 3300 train loss: 0.9248529672622681 train acc: 0.9800000190734863 test loss: 1.039318323135376 best test loss: 1.025773048400879 test acc: 0.8700000047683716\n",
      "Epoch 3400 train loss: 0.924816906452179 train acc: 0.9800000190734863 test loss: 1.0403404235839844 best test loss: 1.025773048400879 test acc: 0.8600000143051147\n",
      "Epoch 3500 train loss: 0.9248270988464355 train acc: 0.9800000190734863 test loss: 1.0388895273208618 best test loss: 1.025773048400879 test acc: 0.8700000047683716\n",
      "Epoch 3600 train loss: 0.9246839880943298 train acc: 0.9800000190734863 test loss: 1.0344836711883545 best test loss: 1.025773048400879 test acc: 0.8700000047683716\n",
      "Epoch 3700 train loss: 0.9246994256973267 train acc: 0.9800000190734863 test loss: 1.0467406511306763 best test loss: 1.025773048400879 test acc: 0.8600000143051147\n",
      "Epoch 3800 train loss: 0.9247229099273682 train acc: 0.9800000190734863 test loss: 1.0246165990829468 best test loss: 1.0246165990829468 test acc: 0.8799999952316284\n",
      "Epoch 3900 train loss: 0.9223735928535461 train acc: 0.9825000166893005 test loss: 1.0360790491104126 best test loss: 1.0246165990829468 test acc: 0.8700000047683716\n",
      "Epoch 4000 train loss: 0.9222316145896912 train acc: 0.9825000166893005 test loss: 1.037499189376831 best test loss: 1.0243583917617798 test acc: 0.8600000143051147\n",
      "Epoch 4100 train loss: 0.9221926331520081 train acc: 0.9825000166893005 test loss: 1.0370854139328003 best test loss: 1.0240039825439453 test acc: 0.8700000047683716\n",
      "Epoch 4200 train loss: 0.9222180843353271 train acc: 0.9825000166893005 test loss: 1.0508975982666016 best test loss: 1.0235306024551392 test acc: 0.8600000143051147\n",
      "Epoch 4300 train loss: 0.9221984148025513 train acc: 0.9825000166893005 test loss: 1.0495672225952148 best test loss: 1.0223525762557983 test acc: 0.8600000143051147\n",
      "Epoch 4400 train loss: 0.9172393083572388 train acc: 0.987500011920929 test loss: 1.0231025218963623 best test loss: 1.0223525762557983 test acc: 0.8899999856948853\n",
      "Epoch 4500 train loss: 0.9177854061126709 train acc: 0.987500011920929 test loss: 1.039347529411316 best test loss: 1.0223525762557983 test acc: 0.8700000047683716\n",
      "Epoch 4600 train loss: 0.9171774983406067 train acc: 0.987500011920929 test loss: 1.0506658554077148 best test loss: 1.0223525762557983 test acc: 0.8500000238418579\n",
      "Epoch 4700 train loss: 0.9172064065933228 train acc: 0.987500011920929 test loss: 1.0273884534835815 best test loss: 1.0204423666000366 test acc: 0.8799999952316284\n",
      "Epoch 4800 train loss: 0.9172104001045227 train acc: 0.987500011920929 test loss: 1.0537164211273193 best test loss: 1.0204423666000366 test acc: 0.8500000238418579\n",
      "Epoch 4900 train loss: 0.9172250628471375 train acc: 0.987500011920929 test loss: 1.0434471368789673 best test loss: 1.0204423666000366 test acc: 0.8600000143051147\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model(inputs_train)\n",
    "    \n",
    "    entropy = calculate_entropy(preds_train)\n",
    "    if entropy > 0.6:\n",
    "        \n",
    "        \n",
    "    train_loss = criterion(preds_train, outputs_train)\n",
    "    train_loss.backward()\n",
    "    train_acc = (preds_train.argmax(1) == outputs_train.argmax(1)).float().mean()\n",
    "    optimizer.step()\n",
    "\n",
    "    preds_test = model(inputs_test)\n",
    "    \n",
    "    test_loss = criterion(preds_test, outputs_test)\n",
    "    test_acc = (preds_test.argmax(1) == outputs_test.argmax(1)).float().mean()\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'server/action_classifier.pt')\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {} train loss: {} train acc: {} test loss: {} best test loss: {} test acc: {}'.format(epoch, train_loss, train_acc, test_loss, best_test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActionClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (fc4): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout4): Dropout(p=0.2, inplace=False)\n",
       "  (fc5): Linear(in_features=384, out_features=5, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_classifier = ActionClassifier()\n",
    "action_classifier.load_state_dict(torch.load('server/action_classifier.pt'))\n",
    "action_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    input_ = torch.Tensor(encoder.encode([text]))\n",
    "    output = action_classifier(input_)\n",
    "    print(output)\n",
    "    return classes[output.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4797e-07, 2.1970e-04, 1.4797e-07, 9.9978e-01, 1.4797e-07]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'schedule'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('Let\\'s all meet sometime to discuss sprint planning for this project. Who\\'s available?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
