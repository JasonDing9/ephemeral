{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvind/miniconda3/envs/treehacks24/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"clarify\", \"email\", \"link\", \"schedule\", \"todos\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 768])\n"
     ]
    }
   ],
   "source": [
    "# Walk through all files in a directory\n",
    "import os\n",
    "lines = []\n",
    "for class_ in classes:\n",
    "    # Read contents of file\n",
    "    with open('data/action_classification/' + class_ + '.txt', 'r') as f:\n",
    "        lines += f.readlines()\n",
    "\n",
    "inputs = encoder.encode(lines)\n",
    "inputs = torch.Tensor(inputs)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 6])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for class_ in classes:\n",
    "    output_1he = torch.Tensor([0] * len(classes))\n",
    "    output_1he[classes.index(class_)] = 1\n",
    "    for i in range(100):\n",
    "        outputs.append(output_1he)\n",
    "\n",
    "outputs = torch.stack(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 384)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(384, 384)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(384, 384)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(384, 384)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc5 = nn.Linear(384, len(classes))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.gelu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.gelu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.gelu(self.fc5(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = ActionClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 1.7918013334274292 train acc: 0.17499999701976776 test loss: 1.791577935218811 best test loss: 1.791577935218811 test acc: 0.20000000298023224\n",
      "Epoch 1 train loss: 1.7918206453323364 train acc: 0.15208333730697632 test loss: 1.7916284799575806 best test loss: 1.791577935218811 test acc: 0.1666666716337204\n",
      "Epoch 2 train loss: 1.7918038368225098 train acc: 0.15416666865348816 test loss: 1.7915575504302979 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 3 train loss: 1.7918046712875366 train acc: 0.15833333134651184 test loss: 1.7916004657745361 best test loss: 1.7915575504302979 test acc: 0.18333333730697632\n",
      "Epoch 4 train loss: 1.791772723197937 train acc: 0.18958333134651184 test loss: 1.7915751934051514 best test loss: 1.7915575504302979 test acc: 0.2083333283662796\n",
      "Epoch 5 train loss: 1.7917861938476562 train acc: 0.17499999701976776 test loss: 1.7916170358657837 best test loss: 1.7915575504302979 test acc: 0.15000000596046448\n",
      "Epoch 6 train loss: 1.7917760610580444 train acc: 0.16458334028720856 test loss: 1.7916016578674316 best test loss: 1.7915575504302979 test acc: 0.17499999701976776\n",
      "Epoch 7 train loss: 1.7917333841323853 train acc: 0.17083333432674408 test loss: 1.7916200160980225 best test loss: 1.7915575504302979 test acc: 0.15833333134651184\n",
      "Epoch 8 train loss: 1.7917650938034058 train acc: 0.18541666865348816 test loss: 1.7916243076324463 best test loss: 1.7915575504302979 test acc: 0.17499999701976776\n",
      "Epoch 9 train loss: 1.7917176485061646 train acc: 0.18125000596046448 test loss: 1.7916946411132812 best test loss: 1.7915575504302979 test acc: 0.17499999701976776\n",
      "Epoch 10 train loss: 1.7917307615280151 train acc: 0.15416666865348816 test loss: 1.791639804840088 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 11 train loss: 1.7917085886001587 train acc: 0.19374999403953552 test loss: 1.791638970375061 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 12 train loss: 1.791702151298523 train acc: 0.17916665971279144 test loss: 1.791613221168518 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 13 train loss: 1.79165780544281 train acc: 0.18125000596046448 test loss: 1.791656732559204 best test loss: 1.7915575504302979 test acc: 0.17499999701976776\n",
      "Epoch 14 train loss: 1.7916858196258545 train acc: 0.15833333134651184 test loss: 1.7916240692138672 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 15 train loss: 1.7916854619979858 train acc: 0.18125000596046448 test loss: 1.791717529296875 best test loss: 1.7915575504302979 test acc: 0.13333334028720856\n",
      "Epoch 16 train loss: 1.791619896888733 train acc: 0.1979166716337204 test loss: 1.7916959524154663 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 17 train loss: 1.791637897491455 train acc: 0.1770833283662796 test loss: 1.791643738746643 best test loss: 1.7915575504302979 test acc: 0.1666666716337204\n",
      "Epoch 18 train loss: 1.7916170358657837 train acc: 0.20416666567325592 test loss: 1.791638970375061 best test loss: 1.7915575504302979 test acc: 0.1666666716337204\n",
      "Epoch 19 train loss: 1.7915949821472168 train acc: 0.20624999701976776 test loss: 1.7916486263275146 best test loss: 1.7915575504302979 test acc: 0.18333333730697632\n",
      "Epoch 20 train loss: 1.7916027307510376 train acc: 0.18125000596046448 test loss: 1.7916967868804932 best test loss: 1.7915575504302979 test acc: 0.18333333730697632\n",
      "Epoch 21 train loss: 1.7915759086608887 train acc: 0.19583334028720856 test loss: 1.7916982173919678 best test loss: 1.7915575504302979 test acc: 0.2083333283662796\n",
      "Epoch 22 train loss: 1.7915894985198975 train acc: 0.20000000298023224 test loss: 1.79170560836792 best test loss: 1.7915575504302979 test acc: 0.19166666269302368\n",
      "Epoch 23 train loss: 1.7915600538253784 train acc: 0.20416666567325592 test loss: 1.7916938066482544 best test loss: 1.7915575504302979 test acc: 0.19166666269302368\n",
      "Epoch 24 train loss: 1.7915284633636475 train acc: 0.17916665971279144 test loss: 1.791614055633545 best test loss: 1.7915575504302979 test acc: 0.2083333283662796\n",
      "Epoch 25 train loss: 1.7914942502975464 train acc: 0.20624999701976776 test loss: 1.79157555103302 best test loss: 1.7915575504302979 test acc: 0.22499999403953552\n",
      "Epoch 26 train loss: 1.7914830446243286 train acc: 0.19166666269302368 test loss: 1.7916206121444702 best test loss: 1.7915575504302979 test acc: 0.15833333134651184\n",
      "Epoch 27 train loss: 1.7914364337921143 train acc: 0.2083333283662796 test loss: 1.7916245460510254 best test loss: 1.7915575504302979 test acc: 0.17499999701976776\n",
      "Epoch 28 train loss: 1.7913858890533447 train acc: 0.19583334028720856 test loss: 1.7916349172592163 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 29 train loss: 1.7913707494735718 train acc: 0.1979166716337204 test loss: 1.7915626764297485 best test loss: 1.7915575504302979 test acc: 0.20000000298023224\n",
      "Epoch 30 train loss: 1.791324257850647 train acc: 0.20416666567325592 test loss: 1.7916114330291748 best test loss: 1.7915575504302979 test acc: 0.21666666865348816\n",
      "Epoch 31 train loss: 1.7912729978561401 train acc: 0.20416666567325592 test loss: 1.791575312614441 best test loss: 1.7915575504302979 test acc: 0.17499999701976776\n",
      "Epoch 32 train loss: 1.791236400604248 train acc: 0.20208333432674408 test loss: 1.791599154472351 best test loss: 1.7915575504302979 test acc: 0.22499999403953552\n",
      "Epoch 33 train loss: 1.7911581993103027 train acc: 0.21875 test loss: 1.7915022373199463 best test loss: 1.7915022373199463 test acc: 0.23333333432674408\n",
      "Epoch 34 train loss: 1.7911438941955566 train acc: 0.23333333432674408 test loss: 1.7915407419204712 best test loss: 1.7915022373199463 test acc: 0.21666666865348816\n",
      "Epoch 35 train loss: 1.791052222251892 train acc: 0.23541666567325592 test loss: 1.7914994955062866 best test loss: 1.7914994955062866 test acc: 0.21666666865348816\n",
      "Epoch 36 train loss: 1.7909704446792603 train acc: 0.30000001192092896 test loss: 1.791351318359375 best test loss: 1.791351318359375 test acc: 0.25\n",
      "Epoch 37 train loss: 1.7909319400787354 train acc: 0.30000001192092896 test loss: 1.7914180755615234 best test loss: 1.791351318359375 test acc: 0.30000001192092896\n",
      "Epoch 38 train loss: 1.790873408317566 train acc: 0.3166666626930237 test loss: 1.7913771867752075 best test loss: 1.791351318359375 test acc: 0.32499998807907104\n",
      "Epoch 39 train loss: 1.7907092571258545 train acc: 0.36666667461395264 test loss: 1.7912178039550781 best test loss: 1.7912178039550781 test acc: 0.3166666626930237\n",
      "Epoch 40 train loss: 1.790649175643921 train acc: 0.36666667461395264 test loss: 1.7912132740020752 best test loss: 1.7912132740020752 test acc: 0.3333333432674408\n",
      "Epoch 41 train loss: 1.7905330657958984 train acc: 0.3916666805744171 test loss: 1.7910712957382202 best test loss: 1.7910712957382202 test acc: 0.32499998807907104\n",
      "Epoch 42 train loss: 1.7904151678085327 train acc: 0.3895833194255829 test loss: 1.7910113334655762 best test loss: 1.7910113334655762 test acc: 0.3583333194255829\n",
      "Epoch 43 train loss: 1.7902464866638184 train acc: 0.3708333373069763 test loss: 1.7909715175628662 best test loss: 1.7909715175628662 test acc: 0.30000001192092896\n",
      "Epoch 44 train loss: 1.790113091468811 train acc: 0.3541666567325592 test loss: 1.7908040285110474 best test loss: 1.7908040285110474 test acc: 0.3166666626930237\n",
      "Epoch 45 train loss: 1.7899515628814697 train acc: 0.33541667461395264 test loss: 1.7906655073165894 best test loss: 1.7906655073165894 test acc: 0.30000001192092896\n",
      "Epoch 46 train loss: 1.7897140979766846 train acc: 0.32499998807907104 test loss: 1.790632963180542 best test loss: 1.790632963180542 test acc: 0.28333333134651184\n",
      "Epoch 47 train loss: 1.789527177810669 train acc: 0.28958332538604736 test loss: 1.7903273105621338 best test loss: 1.7903273105621338 test acc: 0.25\n",
      "Epoch 48 train loss: 1.7893117666244507 train acc: 0.2541666626930237 test loss: 1.790162444114685 best test loss: 1.790162444114685 test acc: 0.2666666805744171\n",
      "Epoch 49 train loss: 1.7890015840530396 train acc: 0.2541666626930237 test loss: 1.790131688117981 best test loss: 1.790131688117981 test acc: 0.18333333730697632\n",
      "Epoch 50 train loss: 1.7887948751449585 train acc: 0.21041665971279144 test loss: 1.7897276878356934 best test loss: 1.7897276878356934 test acc: 0.22499999403953552\n",
      "Epoch 51 train loss: 1.7883344888687134 train acc: 0.21458333730697632 test loss: 1.7895427942276 best test loss: 1.7895427942276 test acc: 0.1666666716337204\n",
      "Epoch 52 train loss: 1.7879815101623535 train acc: 0.19166666269302368 test loss: 1.789100170135498 best test loss: 1.789100170135498 test acc: 0.15833333134651184\n",
      "Epoch 53 train loss: 1.7875144481658936 train acc: 0.19166666269302368 test loss: 1.788727879524231 best test loss: 1.788727879524231 test acc: 0.15833333134651184\n",
      "Epoch 54 train loss: 1.7870662212371826 train acc: 0.18958333134651184 test loss: 1.7885854244232178 best test loss: 1.7885854244232178 test acc: 0.1666666716337204\n",
      "Epoch 55 train loss: 1.7864915132522583 train acc: 0.18541666865348816 test loss: 1.7879501581192017 best test loss: 1.7879501581192017 test acc: 0.17499999701976776\n",
      "Epoch 56 train loss: 1.7858185768127441 train acc: 0.18125000596046448 test loss: 1.787424921989441 best test loss: 1.787424921989441 test acc: 0.1666666716337204\n",
      "Epoch 57 train loss: 1.785125732421875 train acc: 0.1770833283662796 test loss: 1.7868083715438843 best test loss: 1.7868083715438843 test acc: 0.15833333134651184\n",
      "Epoch 58 train loss: 1.7842243909835815 train acc: 0.1770833283662796 test loss: 1.7862708568572998 best test loss: 1.7862708568572998 test acc: 0.14166666567325592\n",
      "Epoch 59 train loss: 1.783357858657837 train acc: 0.17916665971279144 test loss: 1.7854820489883423 best test loss: 1.7854820489883423 test acc: 0.15833333134651184\n",
      "Epoch 60 train loss: 1.7823519706726074 train acc: 0.17499999701976776 test loss: 1.784227967262268 best test loss: 1.784227967262268 test acc: 0.14166666567325592\n",
      "Epoch 61 train loss: 1.78099524974823 train acc: 0.17916665971279144 test loss: 1.7833974361419678 best test loss: 1.7833974361419678 test acc: 0.14166666567325592\n",
      "Epoch 62 train loss: 1.7796440124511719 train acc: 0.17291666567325592 test loss: 1.7822142839431763 best test loss: 1.7822142839431763 test acc: 0.14166666567325592\n",
      "Epoch 63 train loss: 1.7780354022979736 train acc: 0.17499999701976776 test loss: 1.7802242040634155 best test loss: 1.7802242040634155 test acc: 0.14166666567325592\n",
      "Epoch 64 train loss: 1.7759491205215454 train acc: 0.17291666567325592 test loss: 1.778326153755188 best test loss: 1.778326153755188 test acc: 0.14166666567325592\n",
      "Epoch 65 train loss: 1.7739931344985962 train acc: 0.17499999701976776 test loss: 1.7765048742294312 best test loss: 1.7765048742294312 test acc: 0.14166666567325592\n",
      "Epoch 66 train loss: 1.7716068029403687 train acc: 0.17499999701976776 test loss: 1.7752214670181274 best test loss: 1.7752214670181274 test acc: 0.14166666567325592\n",
      "Epoch 67 train loss: 1.7682942152023315 train acc: 0.17916665971279144 test loss: 1.7711186408996582 best test loss: 1.7711186408996582 test acc: 0.14166666567325592\n",
      "Epoch 68 train loss: 1.7654263973236084 train acc: 0.17916665971279144 test loss: 1.7695388793945312 best test loss: 1.7695388793945312 test acc: 0.14166666567325592\n",
      "Epoch 69 train loss: 1.7614821195602417 train acc: 0.18333333730697632 test loss: 1.7670021057128906 best test loss: 1.7670021057128906 test acc: 0.14166666567325592\n",
      "Epoch 70 train loss: 1.7566598653793335 train acc: 0.18541666865348816 test loss: 1.7635635137557983 best test loss: 1.7635635137557983 test acc: 0.14166666567325592\n",
      "Epoch 71 train loss: 1.7528742551803589 train acc: 0.18333333730697632 test loss: 1.7612427473068237 best test loss: 1.7612427473068237 test acc: 0.14166666567325592\n",
      "Epoch 72 train loss: 1.7492222785949707 train acc: 0.1875 test loss: 1.7550164461135864 best test loss: 1.7550164461135864 test acc: 0.14166666567325592\n",
      "Epoch 73 train loss: 1.7436065673828125 train acc: 0.18541666865348816 test loss: 1.7536146640777588 best test loss: 1.7536146640777588 test acc: 0.15000000596046448\n",
      "Epoch 74 train loss: 1.73933744430542 train acc: 0.1875 test loss: 1.75309157371521 best test loss: 1.75309157371521 test acc: 0.15833333134651184\n",
      "Epoch 75 train loss: 1.7344164848327637 train acc: 0.19583334028720856 test loss: 1.750103235244751 best test loss: 1.750103235244751 test acc: 0.15000000596046448\n",
      "Epoch 76 train loss: 1.7318379878997803 train acc: 0.19166666269302368 test loss: 1.7478997707366943 best test loss: 1.7478997707366943 test acc: 0.15000000596046448\n",
      "Epoch 77 train loss: 1.728111982345581 train acc: 0.20208333432674408 test loss: 1.7460005283355713 best test loss: 1.7460005283355713 test acc: 0.1666666716337204\n",
      "Epoch 78 train loss: 1.7232645750045776 train acc: 0.20624999701976776 test loss: 1.744877576828003 best test loss: 1.744877576828003 test acc: 0.15833333134651184\n",
      "Epoch 79 train loss: 1.7213857173919678 train acc: 0.21875 test loss: 1.7441037893295288 best test loss: 1.7441037893295288 test acc: 0.18333333730697632\n",
      "Epoch 80 train loss: 1.718306303024292 train acc: 0.22291666269302368 test loss: 1.742037057876587 best test loss: 1.742037057876587 test acc: 0.18333333730697632\n",
      "Epoch 81 train loss: 1.716853380203247 train acc: 0.23749999701976776 test loss: 1.7416610717773438 best test loss: 1.7416610717773438 test acc: 0.2083333283662796\n",
      "Epoch 82 train loss: 1.7137293815612793 train acc: 0.24583333730697632 test loss: 1.741767406463623 best test loss: 1.7416610717773438 test acc: 0.20000000298023224\n",
      "Epoch 83 train loss: 1.7111468315124512 train acc: 0.2666666805744171 test loss: 1.737924337387085 best test loss: 1.737924337387085 test acc: 0.23333333432674408\n",
      "Epoch 84 train loss: 1.7090280055999756 train acc: 0.26875001192092896 test loss: 1.7358613014221191 best test loss: 1.7358613014221191 test acc: 0.23333333432674408\n",
      "Epoch 85 train loss: 1.7064883708953857 train acc: 0.27916666865348816 test loss: 1.7337850332260132 best test loss: 1.7337850332260132 test acc: 0.25833332538604736\n",
      "Epoch 86 train loss: 1.7037882804870605 train acc: 0.2770833373069763 test loss: 1.7316279411315918 best test loss: 1.7316279411315918 test acc: 0.25\n",
      "Epoch 87 train loss: 1.7003285884857178 train acc: 0.2874999940395355 test loss: 1.7300277948379517 best test loss: 1.7300277948379517 test acc: 0.24166665971279144\n",
      "Epoch 88 train loss: 1.6985002756118774 train acc: 0.2979166805744171 test loss: 1.7288306951522827 best test loss: 1.7288306951522827 test acc: 0.2750000059604645\n",
      "Epoch 89 train loss: 1.6953035593032837 train acc: 0.30416667461395264 test loss: 1.725890040397644 best test loss: 1.725890040397644 test acc: 0.25833332538604736\n",
      "Epoch 90 train loss: 1.693053126335144 train acc: 0.32083332538604736 test loss: 1.723313808441162 best test loss: 1.723313808441162 test acc: 0.2916666567325592\n",
      "Epoch 91 train loss: 1.6895040273666382 train acc: 0.33125001192092896 test loss: 1.720888614654541 best test loss: 1.720888614654541 test acc: 0.2916666567325592\n",
      "Epoch 92 train loss: 1.6867610216140747 train acc: 0.3583333194255829 test loss: 1.7181378602981567 best test loss: 1.7181378602981567 test acc: 0.3083333373069763\n",
      "Epoch 93 train loss: 1.6843457221984863 train acc: 0.36250001192092896 test loss: 1.716779112815857 best test loss: 1.716779112815857 test acc: 0.32499998807907104\n",
      "Epoch 94 train loss: 1.682981014251709 train acc: 0.3583333194255829 test loss: 1.7147018909454346 best test loss: 1.7147018909454346 test acc: 0.3166666626930237\n",
      "Epoch 95 train loss: 1.6808744668960571 train acc: 0.36666667461395264 test loss: 1.7137892246246338 best test loss: 1.7137892246246338 test acc: 0.3166666626930237\n",
      "Epoch 96 train loss: 1.6781423091888428 train acc: 0.3541666567325592 test loss: 1.710849404335022 best test loss: 1.710849404335022 test acc: 0.32499998807907104\n",
      "Epoch 97 train loss: 1.675920009613037 train acc: 0.3604166805744171 test loss: 1.7090104818344116 best test loss: 1.7090104818344116 test acc: 0.34166666865348816\n",
      "Epoch 98 train loss: 1.6744346618652344 train acc: 0.3708333373069763 test loss: 1.7079254388809204 best test loss: 1.7079254388809204 test acc: 0.3333333432674408\n",
      "Epoch 99 train loss: 1.6733285188674927 train acc: 0.36666667461395264 test loss: 1.705620288848877 best test loss: 1.705620288848877 test acc: 0.3333333432674408\n",
      "Epoch 100 train loss: 1.670554280281067 train acc: 0.36250001192092896 test loss: 1.7030092477798462 best test loss: 1.7030092477798462 test acc: 0.32499998807907104\n",
      "Epoch 101 train loss: 1.6687169075012207 train acc: 0.3645833432674408 test loss: 1.702399492263794 best test loss: 1.702399492263794 test acc: 0.3166666626930237\n",
      "Epoch 102 train loss: 1.6659910678863525 train acc: 0.36250001192092896 test loss: 1.7012079954147339 best test loss: 1.7012079954147339 test acc: 0.32499998807907104\n",
      "Epoch 103 train loss: 1.6644036769866943 train acc: 0.375 test loss: 1.6973860263824463 best test loss: 1.6973860263824463 test acc: 0.3166666626930237\n",
      "Epoch 104 train loss: 1.6614779233932495 train acc: 0.375 test loss: 1.6974105834960938 best test loss: 1.6973860263824463 test acc: 0.3083333373069763\n",
      "Epoch 105 train loss: 1.6594281196594238 train acc: 0.375 test loss: 1.6980798244476318 best test loss: 1.6973860263824463 test acc: 0.32499998807907104\n",
      "Epoch 106 train loss: 1.656579613685608 train acc: 0.3687500059604645 test loss: 1.6944798231124878 best test loss: 1.6944798231124878 test acc: 0.3166666626930237\n",
      "Epoch 107 train loss: 1.6536693572998047 train acc: 0.37708333134651184 test loss: 1.6910569667816162 best test loss: 1.6910569667816162 test acc: 0.3166666626930237\n",
      "Epoch 108 train loss: 1.6504273414611816 train acc: 0.37708333134651184 test loss: 1.689012050628662 best test loss: 1.689012050628662 test acc: 0.3166666626930237\n",
      "Epoch 109 train loss: 1.6472115516662598 train acc: 0.38749998807907104 test loss: 1.68839693069458 best test loss: 1.68839693069458 test acc: 0.32499998807907104\n",
      "Epoch 110 train loss: 1.6441402435302734 train acc: 0.3958333432674408 test loss: 1.6853653192520142 best test loss: 1.6853653192520142 test acc: 0.3166666626930237\n",
      "Epoch 111 train loss: 1.6412811279296875 train acc: 0.39375001192092896 test loss: 1.68442702293396 best test loss: 1.68442702293396 test acc: 0.3166666626930237\n",
      "Epoch 112 train loss: 1.6385587453842163 train acc: 0.38333332538604736 test loss: 1.6784192323684692 best test loss: 1.6784192323684692 test acc: 0.3333333432674408\n",
      "Epoch 113 train loss: 1.6355615854263306 train acc: 0.3854166567325592 test loss: 1.6788243055343628 best test loss: 1.6784192323684692 test acc: 0.32499998807907104\n",
      "Epoch 114 train loss: 1.6319173574447632 train acc: 0.3916666805744171 test loss: 1.6744753122329712 best test loss: 1.6744753122329712 test acc: 0.3333333432674408\n",
      "Epoch 115 train loss: 1.628592848777771 train acc: 0.39791667461395264 test loss: 1.675411343574524 best test loss: 1.6744753122329712 test acc: 0.32499998807907104\n",
      "Epoch 116 train loss: 1.6251673698425293 train acc: 0.38749998807907104 test loss: 1.669203281402588 best test loss: 1.669203281402588 test acc: 0.32499998807907104\n",
      "Epoch 117 train loss: 1.6216076612472534 train acc: 0.38333332538604736 test loss: 1.6711395978927612 best test loss: 1.669203281402588 test acc: 0.3333333432674408\n",
      "Epoch 118 train loss: 1.619822382926941 train acc: 0.40416666865348816 test loss: 1.6698601245880127 best test loss: 1.669203281402588 test acc: 0.34166666865348816\n",
      "Epoch 119 train loss: 1.6169499158859253 train acc: 0.39791667461395264 test loss: 1.6666926145553589 best test loss: 1.6666926145553589 test acc: 0.32499998807907104\n",
      "Epoch 120 train loss: 1.6144003868103027 train acc: 0.40416666865348816 test loss: 1.66252601146698 best test loss: 1.66252601146698 test acc: 0.34166666865348816\n",
      "Epoch 121 train loss: 1.6116433143615723 train acc: 0.40625 test loss: 1.6598562002182007 best test loss: 1.6598562002182007 test acc: 0.34166666865348816\n",
      "Epoch 122 train loss: 1.609342098236084 train acc: 0.40833333134651184 test loss: 1.6579930782318115 best test loss: 1.6579930782318115 test acc: 0.3499999940395355\n",
      "Epoch 123 train loss: 1.606164574623108 train acc: 0.4104166626930237 test loss: 1.6565415859222412 best test loss: 1.6565415859222412 test acc: 0.3583333194255829\n",
      "Epoch 124 train loss: 1.6032081842422485 train acc: 0.4104166626930237 test loss: 1.6525431871414185 best test loss: 1.6525431871414185 test acc: 0.3583333194255829\n",
      "Epoch 125 train loss: 1.599755048751831 train acc: 0.4270833432674408 test loss: 1.6516458988189697 best test loss: 1.6516458988189697 test acc: 0.3583333194255829\n",
      "Epoch 126 train loss: 1.5977202653884888 train acc: 0.41874998807907104 test loss: 1.6491552591323853 best test loss: 1.6491552591323853 test acc: 0.36666667461395264\n",
      "Epoch 127 train loss: 1.5933996438980103 train acc: 0.4333333373069763 test loss: 1.6426103115081787 best test loss: 1.6426103115081787 test acc: 0.375\n",
      "Epoch 128 train loss: 1.5884912014007568 train acc: 0.42916667461395264 test loss: 1.6393910646438599 best test loss: 1.6393910646438599 test acc: 0.38333332538604736\n",
      "Epoch 129 train loss: 1.5865962505340576 train acc: 0.4333333373069763 test loss: 1.6361006498336792 best test loss: 1.6361006498336792 test acc: 0.4000000059604645\n",
      "Epoch 130 train loss: 1.5816404819488525 train acc: 0.4437499940395355 test loss: 1.6324790716171265 best test loss: 1.6324790716171265 test acc: 0.3916666805744171\n",
      "Epoch 131 train loss: 1.5779979228973389 train acc: 0.4583333432674408 test loss: 1.6228892803192139 best test loss: 1.6228892803192139 test acc: 0.4166666567325592\n",
      "Epoch 132 train loss: 1.5722415447235107 train acc: 0.4729166626930237 test loss: 1.6162497997283936 best test loss: 1.6162497997283936 test acc: 0.42500001192092896\n",
      "Epoch 133 train loss: 1.5671309232711792 train acc: 0.47083333134651184 test loss: 1.6111912727355957 best test loss: 1.6111912727355957 test acc: 0.4583333432674408\n",
      "Epoch 134 train loss: 1.5619380474090576 train acc: 0.48124998807907104 test loss: 1.6127694845199585 best test loss: 1.6111912727355957 test acc: 0.4333333373069763\n",
      "Epoch 135 train loss: 1.5556973218917847 train acc: 0.4833333194255829 test loss: 1.60541570186615 best test loss: 1.60541570186615 test acc: 0.4333333373069763\n",
      "Epoch 136 train loss: 1.5504066944122314 train acc: 0.4895833432674408 test loss: 1.5979081392288208 best test loss: 1.5979081392288208 test acc: 0.4749999940395355\n",
      "Epoch 137 train loss: 1.5449413061141968 train acc: 0.4958333373069763 test loss: 1.5921233892440796 best test loss: 1.5921233892440796 test acc: 0.4583333432674408\n",
      "Epoch 138 train loss: 1.5442017316818237 train acc: 0.49791666865348816 test loss: 1.5890114307403564 best test loss: 1.5890114307403564 test acc: 0.4749999940395355\n",
      "Epoch 139 train loss: 1.540046215057373 train acc: 0.4937500059604645 test loss: 1.5862005949020386 best test loss: 1.5862005949020386 test acc: 0.4583333432674408\n",
      "Epoch 140 train loss: 1.5367175340652466 train acc: 0.49166667461395264 test loss: 1.5796492099761963 best test loss: 1.5796492099761963 test acc: 0.4583333432674408\n",
      "Epoch 141 train loss: 1.533337950706482 train acc: 0.4895833432674408 test loss: 1.574777603149414 best test loss: 1.574777603149414 test acc: 0.4833333194255829\n",
      "Epoch 142 train loss: 1.5296714305877686 train acc: 0.4937500059604645 test loss: 1.5713104009628296 best test loss: 1.5713104009628296 test acc: 0.46666666865348816\n",
      "Epoch 143 train loss: 1.526055097579956 train acc: 0.49791666865348816 test loss: 1.5670255422592163 best test loss: 1.5670255422592163 test acc: 0.4749999940395355\n",
      "Epoch 144 train loss: 1.5227807760238647 train acc: 0.5 test loss: 1.5670720338821411 best test loss: 1.5670255422592163 test acc: 0.46666666865348816\n",
      "Epoch 145 train loss: 1.5203758478164673 train acc: 0.5 test loss: 1.564172387123108 best test loss: 1.564172387123108 test acc: 0.4749999940395355\n",
      "Epoch 146 train loss: 1.5149133205413818 train acc: 0.5062500238418579 test loss: 1.5583412647247314 best test loss: 1.5583412647247314 test acc: 0.4833333194255829\n",
      "Epoch 147 train loss: 1.5114067792892456 train acc: 0.5145833492279053 test loss: 1.5560762882232666 best test loss: 1.5560762882232666 test acc: 0.49166667461395264\n",
      "Epoch 148 train loss: 1.5053954124450684 train acc: 0.5270833373069763 test loss: 1.5529797077178955 best test loss: 1.5529797077178955 test acc: 0.49166667461395264\n",
      "Epoch 149 train loss: 1.503221869468689 train acc: 0.5270833373069763 test loss: 1.5518875122070312 best test loss: 1.5518875122070312 test acc: 0.5\n",
      "Epoch 150 train loss: 1.4973835945129395 train acc: 0.5291666388511658 test loss: 1.5541359186172485 best test loss: 1.5518875122070312 test acc: 0.49166667461395264\n",
      "Epoch 151 train loss: 1.4926800727844238 train acc: 0.5354166626930237 test loss: 1.5466135740280151 best test loss: 1.5466135740280151 test acc: 0.5083333253860474\n",
      "Epoch 152 train loss: 1.4930821657180786 train acc: 0.5375000238418579 test loss: 1.5499272346496582 best test loss: 1.5466135740280151 test acc: 0.5083333253860474\n",
      "Epoch 153 train loss: 1.4868980646133423 train acc: 0.5479166507720947 test loss: 1.543798565864563 best test loss: 1.543798565864563 test acc: 0.5166666507720947\n",
      "Epoch 154 train loss: 1.4837799072265625 train acc: 0.5541666746139526 test loss: 1.5370299816131592 best test loss: 1.5370299816131592 test acc: 0.5083333253860474\n",
      "Epoch 155 train loss: 1.4806647300720215 train acc: 0.5458333492279053 test loss: 1.5399363040924072 best test loss: 1.5370299816131592 test acc: 0.5083333253860474\n",
      "Epoch 156 train loss: 1.4754371643066406 train acc: 0.5604166388511658 test loss: 1.5386602878570557 best test loss: 1.5370299816131592 test acc: 0.5166666507720947\n",
      "Epoch 157 train loss: 1.475036859512329 train acc: 0.5666666626930237 test loss: 1.5329636335372925 best test loss: 1.5329636335372925 test acc: 0.5083333253860474\n",
      "Epoch 158 train loss: 1.4705291986465454 train acc: 0.574999988079071 test loss: 1.533284068107605 best test loss: 1.5329636335372925 test acc: 0.5\n",
      "Epoch 159 train loss: 1.4665807485580444 train acc: 0.5729166865348816 test loss: 1.534999132156372 best test loss: 1.5329636335372925 test acc: 0.5083333253860474\n",
      "Epoch 160 train loss: 1.4631092548370361 train acc: 0.5833333134651184 test loss: 1.5306813716888428 best test loss: 1.5306813716888428 test acc: 0.5\n",
      "Epoch 161 train loss: 1.4573873281478882 train acc: 0.581250011920929 test loss: 1.5289623737335205 best test loss: 1.5289623737335205 test acc: 0.49166667461395264\n",
      "Epoch 162 train loss: 1.4576319456100464 train acc: 0.581250011920929 test loss: 1.5313715934753418 best test loss: 1.5289623737335205 test acc: 0.5083333253860474\n",
      "Epoch 163 train loss: 1.450223684310913 train acc: 0.5874999761581421 test loss: 1.526877522468567 best test loss: 1.526877522468567 test acc: 0.5166666507720947\n",
      "Epoch 164 train loss: 1.4493426084518433 train acc: 0.581250011920929 test loss: 1.5200434923171997 best test loss: 1.5200434923171997 test acc: 0.5249999761581421\n",
      "Epoch 165 train loss: 1.4467384815216064 train acc: 0.5791666507720947 test loss: 1.5251328945159912 best test loss: 1.5200434923171997 test acc: 0.5249999761581421\n",
      "Epoch 166 train loss: 1.4450782537460327 train acc: 0.5874999761581421 test loss: 1.518156886100769 best test loss: 1.518156886100769 test acc: 0.5083333253860474\n",
      "Epoch 167 train loss: 1.4454189538955688 train acc: 0.5874999761581421 test loss: 1.5195984840393066 best test loss: 1.518156886100769 test acc: 0.5166666507720947\n",
      "Epoch 168 train loss: 1.4396599531173706 train acc: 0.59375 test loss: 1.5143656730651855 best test loss: 1.5143656730651855 test acc: 0.5083333253860474\n",
      "Epoch 169 train loss: 1.435540795326233 train acc: 0.59375 test loss: 1.5147265195846558 best test loss: 1.5143656730651855 test acc: 0.5249999761581421\n",
      "Epoch 170 train loss: 1.4315420389175415 train acc: 0.6020833253860474 test loss: 1.5176914930343628 best test loss: 1.5143656730651855 test acc: 0.5083333253860474\n",
      "Epoch 171 train loss: 1.434122920036316 train acc: 0.5895833373069763 test loss: 1.5119311809539795 best test loss: 1.5119311809539795 test acc: 0.5249999761581421\n",
      "Epoch 172 train loss: 1.4287570714950562 train acc: 0.6000000238418579 test loss: 1.5046998262405396 best test loss: 1.5046998262405396 test acc: 0.5166666507720947\n",
      "Epoch 173 train loss: 1.4267526865005493 train acc: 0.6020833253860474 test loss: 1.5070797204971313 best test loss: 1.5046998262405396 test acc: 0.5249999761581421\n",
      "Epoch 174 train loss: 1.423169732093811 train acc: 0.6000000238418579 test loss: 1.5007323026657104 best test loss: 1.5007323026657104 test acc: 0.5249999761581421\n",
      "Epoch 175 train loss: 1.4254119396209717 train acc: 0.6041666865348816 test loss: 1.5017008781433105 best test loss: 1.5007323026657104 test acc: 0.5249999761581421\n",
      "Epoch 176 train loss: 1.420939326286316 train acc: 0.6020833253860474 test loss: 1.5033310651779175 best test loss: 1.5007323026657104 test acc: 0.5166666507720947\n",
      "Epoch 177 train loss: 1.4192566871643066 train acc: 0.6145833134651184 test loss: 1.506536841392517 best test loss: 1.5007323026657104 test acc: 0.5333333611488342\n",
      "Epoch 178 train loss: 1.4156440496444702 train acc: 0.6166666746139526 test loss: 1.4976818561553955 best test loss: 1.4976818561553955 test acc: 0.5333333611488342\n",
      "Epoch 179 train loss: 1.414823055267334 train acc: 0.6145833134651184 test loss: 1.495354175567627 best test loss: 1.495354175567627 test acc: 0.5249999761581421\n",
      "Epoch 180 train loss: 1.411317229270935 train acc: 0.6104166507720947 test loss: 1.4985191822052002 best test loss: 1.495354175567627 test acc: 0.5333333611488342\n",
      "Epoch 181 train loss: 1.4103102684020996 train acc: 0.6208333373069763 test loss: 1.4877262115478516 best test loss: 1.4877262115478516 test acc: 0.5249999761581421\n",
      "Epoch 182 train loss: 1.4090197086334229 train acc: 0.6208333373069763 test loss: 1.4951035976409912 best test loss: 1.4877262115478516 test acc: 0.5249999761581421\n",
      "Epoch 183 train loss: 1.4101412296295166 train acc: 0.6166666746139526 test loss: 1.4943655729293823 best test loss: 1.4877262115478516 test acc: 0.5249999761581421\n",
      "Epoch 184 train loss: 1.4063323736190796 train acc: 0.6208333373069763 test loss: 1.4916480779647827 best test loss: 1.4877262115478516 test acc: 0.5249999761581421\n",
      "Epoch 185 train loss: 1.4041403532028198 train acc: 0.6187499761581421 test loss: 1.4860732555389404 best test loss: 1.4860732555389404 test acc: 0.5249999761581421\n",
      "Epoch 186 train loss: 1.401056170463562 train acc: 0.6270833611488342 test loss: 1.4902454614639282 best test loss: 1.4860732555389404 test acc: 0.5166666507720947\n",
      "Epoch 187 train loss: 1.4007285833358765 train acc: 0.625 test loss: 1.4842109680175781 best test loss: 1.4842109680175781 test acc: 0.5333333611488342\n",
      "Epoch 188 train loss: 1.396456003189087 train acc: 0.6270833611488342 test loss: 1.4805068969726562 best test loss: 1.4805068969726562 test acc: 0.5249999761581421\n",
      "Epoch 189 train loss: 1.395592451095581 train acc: 0.637499988079071 test loss: 1.4828441143035889 best test loss: 1.4805068969726562 test acc: 0.5333333611488342\n",
      "Epoch 190 train loss: 1.3946692943572998 train acc: 0.637499988079071 test loss: 1.4798823595046997 best test loss: 1.4798823595046997 test acc: 0.5249999761581421\n",
      "Epoch 191 train loss: 1.3903356790542603 train acc: 0.6291666626930237 test loss: 1.4827200174331665 best test loss: 1.4798823595046997 test acc: 0.5249999761581421\n",
      "Epoch 192 train loss: 1.3901866674423218 train acc: 0.6333333253860474 test loss: 1.4838677644729614 best test loss: 1.4798823595046997 test acc: 0.5166666507720947\n",
      "Epoch 193 train loss: 1.3835195302963257 train acc: 0.6458333134651184 test loss: 1.480262041091919 best test loss: 1.4798823595046997 test acc: 0.5249999761581421\n",
      "Epoch 194 train loss: 1.3838863372802734 train acc: 0.6458333134651184 test loss: 1.4838416576385498 best test loss: 1.4798823595046997 test acc: 0.5249999761581421\n",
      "Epoch 195 train loss: 1.3818928003311157 train acc: 0.6499999761581421 test loss: 1.4778831005096436 best test loss: 1.4778831005096436 test acc: 0.5249999761581421\n",
      "Epoch 196 train loss: 1.3839786052703857 train acc: 0.6520833373069763 test loss: 1.4770500659942627 best test loss: 1.4770500659942627 test acc: 0.5416666865348816\n",
      "Epoch 197 train loss: 1.3793131113052368 train acc: 0.6499999761581421 test loss: 1.4791158437728882 best test loss: 1.4770500659942627 test acc: 0.5333333611488342\n",
      "Epoch 198 train loss: 1.3806278705596924 train acc: 0.6479166746139526 test loss: 1.475036859512329 best test loss: 1.475036859512329 test acc: 0.5249999761581421\n",
      "Epoch 199 train loss: 1.3769042491912842 train acc: 0.6520833373069763 test loss: 1.4794766902923584 best test loss: 1.475036859512329 test acc: 0.5249999761581421\n",
      "Epoch 200 train loss: 1.373236894607544 train acc: 0.6604166626930237 test loss: 1.4801032543182373 best test loss: 1.475036859512329 test acc: 0.5333333611488342\n",
      "Epoch 201 train loss: 1.372609257698059 train acc: 0.65625 test loss: 1.4756475687026978 best test loss: 1.475036859512329 test acc: 0.5333333611488342\n",
      "Epoch 202 train loss: 1.368827223777771 train acc: 0.6583333611488342 test loss: 1.471854329109192 best test loss: 1.471854329109192 test acc: 0.5416666865348816\n",
      "Epoch 203 train loss: 1.366969347000122 train acc: 0.6604166626930237 test loss: 1.4720302820205688 best test loss: 1.471854329109192 test acc: 0.550000011920929\n",
      "Epoch 204 train loss: 1.3672220706939697 train acc: 0.6645833253860474 test loss: 1.4723021984100342 best test loss: 1.471854329109192 test acc: 0.5583333373069763\n",
      "Epoch 205 train loss: 1.3659619092941284 train acc: 0.6729166507720947 test loss: 1.4663504362106323 best test loss: 1.4663504362106323 test acc: 0.550000011920929\n",
      "Epoch 206 train loss: 1.3633469343185425 train acc: 0.6604166626930237 test loss: 1.4680449962615967 best test loss: 1.4663504362106323 test acc: 0.550000011920929\n",
      "Epoch 207 train loss: 1.3590644598007202 train acc: 0.675000011920929 test loss: 1.471186876296997 best test loss: 1.4663504362106323 test acc: 0.550000011920929\n",
      "Epoch 208 train loss: 1.3593140840530396 train acc: 0.675000011920929 test loss: 1.4719417095184326 best test loss: 1.4663504362106323 test acc: 0.5333333611488342\n",
      "Epoch 209 train loss: 1.3555628061294556 train acc: 0.675000011920929 test loss: 1.4636965990066528 best test loss: 1.4636965990066528 test acc: 0.5583333373069763\n",
      "Epoch 210 train loss: 1.3555779457092285 train acc: 0.6729166507720947 test loss: 1.4656633138656616 best test loss: 1.4636965990066528 test acc: 0.5583333373069763\n",
      "Epoch 211 train loss: 1.3514639139175415 train acc: 0.6833333373069763 test loss: 1.4698911905288696 best test loss: 1.4636965990066528 test acc: 0.550000011920929\n",
      "Epoch 212 train loss: 1.3515934944152832 train acc: 0.675000011920929 test loss: 1.4570412635803223 best test loss: 1.4570412635803223 test acc: 0.5833333134651184\n",
      "Epoch 213 train loss: 1.346442699432373 train acc: 0.6875 test loss: 1.4679911136627197 best test loss: 1.4570412635803223 test acc: 0.5666666626930237\n",
      "Epoch 214 train loss: 1.3466366529464722 train acc: 0.6875 test loss: 1.4631049633026123 best test loss: 1.4570412635803223 test acc: 0.574999988079071\n",
      "Epoch 215 train loss: 1.3445936441421509 train acc: 0.6916666626930237 test loss: 1.4630928039550781 best test loss: 1.4570412635803223 test acc: 0.5583333373069763\n",
      "Epoch 216 train loss: 1.342297077178955 train acc: 0.7041666507720947 test loss: 1.4661754369735718 best test loss: 1.4570412635803223 test acc: 0.5666666626930237\n",
      "Epoch 217 train loss: 1.3398468494415283 train acc: 0.6979166865348816 test loss: 1.4579377174377441 best test loss: 1.4570412635803223 test acc: 0.5583333373069763\n",
      "Epoch 218 train loss: 1.3368148803710938 train acc: 0.7104166746139526 test loss: 1.462174892425537 best test loss: 1.4570412635803223 test acc: 0.5666666626930237\n",
      "Epoch 219 train loss: 1.3344711065292358 train acc: 0.7104166746139526 test loss: 1.4588836431503296 best test loss: 1.4570412635803223 test acc: 0.5583333373069763\n",
      "Epoch 220 train loss: 1.3335087299346924 train acc: 0.731249988079071 test loss: 1.4531327486038208 best test loss: 1.4531327486038208 test acc: 0.5583333373069763\n",
      "Epoch 221 train loss: 1.329871654510498 train acc: 0.71875 test loss: 1.4577773809432983 best test loss: 1.4531327486038208 test acc: 0.5666666626930237\n",
      "Epoch 222 train loss: 1.3260141611099243 train acc: 0.7291666865348816 test loss: 1.4577997922897339 best test loss: 1.4531327486038208 test acc: 0.5583333373069763\n",
      "Epoch 223 train loss: 1.3239389657974243 train acc: 0.7333333492279053 test loss: 1.4514158964157104 best test loss: 1.4514158964157104 test acc: 0.5666666626930237\n",
      "Epoch 224 train loss: 1.3167341947555542 train acc: 0.7437499761581421 test loss: 1.4468722343444824 best test loss: 1.4468722343444824 test acc: 0.5916666388511658\n",
      "Epoch 225 train loss: 1.3169922828674316 train acc: 0.7437499761581421 test loss: 1.4527134895324707 best test loss: 1.4468722343444824 test acc: 0.5833333134651184\n",
      "Epoch 226 train loss: 1.3153843879699707 train acc: 0.7583333253860474 test loss: 1.4507415294647217 best test loss: 1.4468722343444824 test acc: 0.625\n",
      "Epoch 227 train loss: 1.3132916688919067 train acc: 0.7583333253860474 test loss: 1.4448468685150146 best test loss: 1.4448468685150146 test acc: 0.5916666388511658\n",
      "Epoch 228 train loss: 1.3087834119796753 train acc: 0.7749999761581421 test loss: 1.4472825527191162 best test loss: 1.4448468685150146 test acc: 0.6000000238418579\n",
      "Epoch 229 train loss: 1.3034412860870361 train acc: 0.768750011920929 test loss: 1.4488513469696045 best test loss: 1.4448468685150146 test acc: 0.6083333492279053\n",
      "Epoch 230 train loss: 1.3024762868881226 train acc: 0.7729166746139526 test loss: 1.444101095199585 best test loss: 1.444101095199585 test acc: 0.6083333492279053\n",
      "Epoch 231 train loss: 1.3024547100067139 train acc: 0.78125 test loss: 1.434238314628601 best test loss: 1.434238314628601 test acc: 0.6416666507720947\n",
      "Epoch 232 train loss: 1.2961591482162476 train acc: 0.7895833253860474 test loss: 1.4372334480285645 best test loss: 1.434238314628601 test acc: 0.625\n",
      "Epoch 233 train loss: 1.2917873859405518 train acc: 0.78125 test loss: 1.4352344274520874 best test loss: 1.434238314628601 test acc: 0.625\n",
      "Epoch 234 train loss: 1.2891883850097656 train acc: 0.7916666865348816 test loss: 1.4316593408584595 best test loss: 1.4316593408584595 test acc: 0.625\n",
      "Epoch 235 train loss: 1.2850044965744019 train acc: 0.793749988079071 test loss: 1.4331661462783813 best test loss: 1.4316593408584595 test acc: 0.6333333253860474\n",
      "Epoch 236 train loss: 1.2826310396194458 train acc: 0.793749988079071 test loss: 1.4368103742599487 best test loss: 1.4316593408584595 test acc: 0.6333333253860474\n",
      "Epoch 237 train loss: 1.2787178754806519 train acc: 0.800000011920929 test loss: 1.4273601770401 best test loss: 1.4273601770401 test acc: 0.6416666507720947\n",
      "Epoch 238 train loss: 1.2754063606262207 train acc: 0.8020833134651184 test loss: 1.4249628782272339 best test loss: 1.4249628782272339 test acc: 0.625\n",
      "Epoch 239 train loss: 1.2715723514556885 train acc: 0.8104166388511658 test loss: 1.4246052503585815 best test loss: 1.4246052503585815 test acc: 0.6166666746139526\n",
      "Epoch 240 train loss: 1.2699382305145264 train acc: 0.8145833611488342 test loss: 1.4285194873809814 best test loss: 1.4246052503585815 test acc: 0.6333333253860474\n",
      "Epoch 241 train loss: 1.2638320922851562 train acc: 0.8166666626930237 test loss: 1.4277352094650269 best test loss: 1.4246052503585815 test acc: 0.6166666746139526\n",
      "Epoch 242 train loss: 1.261101484298706 train acc: 0.8145833611488342 test loss: 1.4248945713043213 best test loss: 1.4246052503585815 test acc: 0.6166666746139526\n",
      "Epoch 243 train loss: 1.257605791091919 train acc: 0.8166666626930237 test loss: 1.4193801879882812 best test loss: 1.4193801879882812 test acc: 0.6166666746139526\n",
      "Epoch 244 train loss: 1.2583674192428589 train acc: 0.8125 test loss: 1.4233497381210327 best test loss: 1.4193801879882812 test acc: 0.6166666746139526\n",
      "Epoch 245 train loss: 1.2544938325881958 train acc: 0.8166666626930237 test loss: 1.4142000675201416 best test loss: 1.4142000675201416 test acc: 0.6166666746139526\n",
      "Epoch 246 train loss: 1.253250241279602 train acc: 0.8187500238418579 test loss: 1.420479416847229 best test loss: 1.4142000675201416 test acc: 0.6083333492279053\n",
      "Epoch 247 train loss: 1.2478474378585815 train acc: 0.8333333134651184 test loss: 1.4136825799942017 best test loss: 1.4136825799942017 test acc: 0.625\n",
      "Epoch 248 train loss: 1.246048092842102 train acc: 0.8208333253860474 test loss: 1.4131853580474854 best test loss: 1.4131853580474854 test acc: 0.625\n",
      "Epoch 249 train loss: 1.2453566789627075 train acc: 0.8333333134651184 test loss: 1.4201428890228271 best test loss: 1.4131853580474854 test acc: 0.625\n",
      "Epoch 250 train loss: 1.2407162189483643 train acc: 0.8333333134651184 test loss: 1.40738844871521 best test loss: 1.40738844871521 test acc: 0.6166666746139526\n",
      "Epoch 251 train loss: 1.2355362176895142 train acc: 0.8374999761581421 test loss: 1.4070428609848022 best test loss: 1.4070428609848022 test acc: 0.625\n",
      "Epoch 252 train loss: 1.232832431793213 train acc: 0.8458333611488342 test loss: 1.4138996601104736 best test loss: 1.4070428609848022 test acc: 0.6083333492279053\n",
      "Epoch 253 train loss: 1.2348463535308838 train acc: 0.84375 test loss: 1.4142417907714844 best test loss: 1.4070428609848022 test acc: 0.6166666746139526\n",
      "Epoch 254 train loss: 1.2328071594238281 train acc: 0.8395833373069763 test loss: 1.4094374179840088 best test loss: 1.4070428609848022 test acc: 0.6166666746139526\n",
      "Epoch 255 train loss: 1.230337142944336 train acc: 0.8395833373069763 test loss: 1.4000420570373535 best test loss: 1.4000420570373535 test acc: 0.625\n",
      "Epoch 256 train loss: 1.2234694957733154 train acc: 0.8520833253860474 test loss: 1.4094009399414062 best test loss: 1.4000420570373535 test acc: 0.625\n",
      "Epoch 257 train loss: 1.2263973951339722 train acc: 0.8520833253860474 test loss: 1.397053837776184 best test loss: 1.397053837776184 test acc: 0.625\n",
      "Epoch 258 train loss: 1.217403769493103 train acc: 0.862500011920929 test loss: 1.4041088819503784 best test loss: 1.397053837776184 test acc: 0.6333333253860474\n",
      "Epoch 259 train loss: 1.2214341163635254 train acc: 0.8583333492279053 test loss: 1.3926022052764893 best test loss: 1.3926022052764893 test acc: 0.6416666507720947\n",
      "Epoch 260 train loss: 1.2186026573181152 train acc: 0.8541666865348816 test loss: 1.4053064584732056 best test loss: 1.3926022052764893 test acc: 0.625\n",
      "Epoch 261 train loss: 1.218329906463623 train acc: 0.8583333492279053 test loss: 1.3883945941925049 best test loss: 1.3883945941925049 test acc: 0.6416666507720947\n",
      "Epoch 262 train loss: 1.2179080247879028 train acc: 0.8458333611488342 test loss: 1.396091103553772 best test loss: 1.3883945941925049 test acc: 0.6416666507720947\n",
      "Epoch 263 train loss: 1.2140601873397827 train acc: 0.8604166507720947 test loss: 1.3925586938858032 best test loss: 1.3883945941925049 test acc: 0.6333333253860474\n",
      "Epoch 264 train loss: 1.2109161615371704 train acc: 0.8604166507720947 test loss: 1.3943819999694824 best test loss: 1.3883945941925049 test acc: 0.6416666507720947\n",
      "Epoch 265 train loss: 1.2093183994293213 train acc: 0.8604166507720947 test loss: 1.387017011642456 best test loss: 1.387017011642456 test acc: 0.6583333611488342\n",
      "Epoch 266 train loss: 1.204904317855835 train acc: 0.8645833134651184 test loss: 1.3951374292373657 best test loss: 1.387017011642456 test acc: 0.6333333253860474\n",
      "Epoch 267 train loss: 1.2047148942947388 train acc: 0.8645833134651184 test loss: 1.3920936584472656 best test loss: 1.387017011642456 test acc: 0.6499999761581421\n",
      "Epoch 268 train loss: 1.201470136642456 train acc: 0.8708333373069763 test loss: 1.4035929441452026 best test loss: 1.387017011642456 test acc: 0.6166666746139526\n",
      "Epoch 269 train loss: 1.2009178400039673 train acc: 0.8645833134651184 test loss: 1.379485845565796 best test loss: 1.379485845565796 test acc: 0.6666666865348816\n",
      "Epoch 270 train loss: 1.1992465257644653 train acc: 0.8708333373069763 test loss: 1.396201252937317 best test loss: 1.379485845565796 test acc: 0.6333333253860474\n",
      "Epoch 271 train loss: 1.1993558406829834 train acc: 0.8729166388511658 test loss: 1.392322301864624 best test loss: 1.379485845565796 test acc: 0.6499999761581421\n",
      "Epoch 272 train loss: 1.1975277662277222 train acc: 0.8770833611488342 test loss: 1.391140341758728 best test loss: 1.379485845565796 test acc: 0.625\n",
      "Epoch 273 train loss: 1.1985470056533813 train acc: 0.8729166388511658 test loss: 1.382152795791626 best test loss: 1.379485845565796 test acc: 0.6666666865348816\n",
      "Epoch 274 train loss: 1.1913974285125732 train acc: 0.8812500238418579 test loss: 1.3836127519607544 best test loss: 1.379485845565796 test acc: 0.6583333611488342\n",
      "Epoch 275 train loss: 1.1914864778518677 train acc: 0.8812500238418579 test loss: 1.3914066553115845 best test loss: 1.379485845565796 test acc: 0.6416666507720947\n",
      "Epoch 276 train loss: 1.1902434825897217 train acc: 0.8708333373069763 test loss: 1.3670932054519653 best test loss: 1.3670932054519653 test acc: 0.6666666865348816\n",
      "Epoch 277 train loss: 1.189223051071167 train acc: 0.8833333253860474 test loss: 1.3775856494903564 best test loss: 1.3670932054519653 test acc: 0.6416666507720947\n",
      "Epoch 278 train loss: 1.18747079372406 train acc: 0.8770833611488342 test loss: 1.3816806077957153 best test loss: 1.3670932054519653 test acc: 0.6416666507720947\n",
      "Epoch 279 train loss: 1.18644380569458 train acc: 0.8770833611488342 test loss: 1.3744903802871704 best test loss: 1.3670932054519653 test acc: 0.6666666865348816\n",
      "Epoch 280 train loss: 1.183293104171753 train acc: 0.875 test loss: 1.377947211265564 best test loss: 1.3670932054519653 test acc: 0.6583333611488342\n",
      "Epoch 281 train loss: 1.182978868484497 train acc: 0.8770833611488342 test loss: 1.3815207481384277 best test loss: 1.3670932054519653 test acc: 0.675000011920929\n",
      "Epoch 282 train loss: 1.1828482151031494 train acc: 0.8833333253860474 test loss: 1.3707880973815918 best test loss: 1.3670932054519653 test acc: 0.6666666865348816\n",
      "Epoch 283 train loss: 1.1820157766342163 train acc: 0.8812500238418579 test loss: 1.3737566471099854 best test loss: 1.3670932054519653 test acc: 0.699999988079071\n",
      "Epoch 284 train loss: 1.1779556274414062 train acc: 0.887499988079071 test loss: 1.3672420978546143 best test loss: 1.3670932054519653 test acc: 0.6833333373069763\n",
      "Epoch 285 train loss: 1.183276653289795 train acc: 0.8812500238418579 test loss: 1.382752537727356 best test loss: 1.3670932054519653 test acc: 0.6583333611488342\n",
      "Epoch 286 train loss: 1.1804907321929932 train acc: 0.8854166865348816 test loss: 1.3731805086135864 best test loss: 1.3670932054519653 test acc: 0.6833333373069763\n",
      "Epoch 287 train loss: 1.1800191402435303 train acc: 0.8812500238418579 test loss: 1.3622686862945557 best test loss: 1.3622686862945557 test acc: 0.699999988079071\n",
      "Epoch 288 train loss: 1.1753331422805786 train acc: 0.8833333253860474 test loss: 1.362514853477478 best test loss: 1.3622686862945557 test acc: 0.6916666626930237\n",
      "Epoch 289 train loss: 1.1712524890899658 train acc: 0.893750011920929 test loss: 1.3631093502044678 best test loss: 1.3622686862945557 test acc: 0.6916666626930237\n",
      "Epoch 290 train loss: 1.1737678050994873 train acc: 0.8854166865348816 test loss: 1.35642671585083 best test loss: 1.35642671585083 test acc: 0.699999988079071\n",
      "Epoch 291 train loss: 1.1717661619186401 train acc: 0.8916666507720947 test loss: 1.3641880750656128 best test loss: 1.35642671585083 test acc: 0.6916666626930237\n",
      "Epoch 292 train loss: 1.1720608472824097 train acc: 0.8854166865348816 test loss: 1.3743021488189697 best test loss: 1.35642671585083 test acc: 0.6666666865348816\n",
      "Epoch 293 train loss: 1.1653122901916504 train acc: 0.893750011920929 test loss: 1.3545498847961426 best test loss: 1.3545498847961426 test acc: 0.6833333373069763\n",
      "Epoch 294 train loss: 1.1660054922103882 train acc: 0.893750011920929 test loss: 1.3601051568984985 best test loss: 1.3545498847961426 test acc: 0.699999988079071\n",
      "Epoch 295 train loss: 1.1646785736083984 train acc: 0.8979166746139526 test loss: 1.3566659688949585 best test loss: 1.3545498847961426 test acc: 0.6916666626930237\n",
      "Epoch 296 train loss: 1.1671454906463623 train acc: 0.8895833492279053 test loss: 1.3443018198013306 best test loss: 1.3443018198013306 test acc: 0.7083333134651184\n",
      "Epoch 297 train loss: 1.16187584400177 train acc: 0.8958333134651184 test loss: 1.3545218706130981 best test loss: 1.3443018198013306 test acc: 0.6833333373069763\n",
      "Epoch 298 train loss: 1.164478063583374 train acc: 0.8958333134651184 test loss: 1.3466670513153076 best test loss: 1.3443018198013306 test acc: 0.7083333134651184\n",
      "Epoch 299 train loss: 1.1617405414581299 train acc: 0.8958333134651184 test loss: 1.3598629236221313 best test loss: 1.3443018198013306 test acc: 0.6666666865348816\n",
      "Epoch 300 train loss: 1.1643208265304565 train acc: 0.893750011920929 test loss: 1.3535417318344116 best test loss: 1.3443018198013306 test acc: 0.6916666626930237\n",
      "Epoch 301 train loss: 1.1610132455825806 train acc: 0.8958333134651184 test loss: 1.3611589670181274 best test loss: 1.3443018198013306 test acc: 0.6833333373069763\n",
      "Epoch 302 train loss: 1.164150595664978 train acc: 0.893750011920929 test loss: 1.3521711826324463 best test loss: 1.3443018198013306 test acc: 0.6916666626930237\n",
      "Epoch 303 train loss: 1.1590458154678345 train acc: 0.8958333134651184 test loss: 1.3648632764816284 best test loss: 1.3443018198013306 test acc: 0.6666666865348816\n",
      "Epoch 304 train loss: 1.1585814952850342 train acc: 0.8958333134651184 test loss: 1.3416531085968018 best test loss: 1.3416531085968018 test acc: 0.7083333134651184\n",
      "Epoch 305 train loss: 1.1579700708389282 train acc: 0.8979166746139526 test loss: 1.3596240282058716 best test loss: 1.3416531085968018 test acc: 0.6833333373069763\n",
      "Epoch 306 train loss: 1.156962275505066 train acc: 0.8979166746139526 test loss: 1.3418729305267334 best test loss: 1.3416531085968018 test acc: 0.7166666388511658\n",
      "Epoch 307 train loss: 1.1567447185516357 train acc: 0.8999999761581421 test loss: 1.3421381711959839 best test loss: 1.3416531085968018 test acc: 0.7166666388511658\n",
      "Epoch 308 train loss: 1.1558340787887573 train acc: 0.9041666388511658 test loss: 1.3415652513504028 best test loss: 1.3415652513504028 test acc: 0.7250000238418579\n",
      "Epoch 309 train loss: 1.1553568840026855 train acc: 0.9020833373069763 test loss: 1.3350337743759155 best test loss: 1.3350337743759155 test acc: 0.7250000238418579\n",
      "Epoch 310 train loss: 1.1504483222961426 train acc: 0.9041666388511658 test loss: 1.342633605003357 best test loss: 1.3350337743759155 test acc: 0.7083333134651184\n",
      "Epoch 311 train loss: 1.152901291847229 train acc: 0.9020833373069763 test loss: 1.3531609773635864 best test loss: 1.3350337743759155 test acc: 0.6916666626930237\n",
      "Epoch 312 train loss: 1.148607850074768 train acc: 0.9041666388511658 test loss: 1.350776195526123 best test loss: 1.3350337743759155 test acc: 0.6916666626930237\n",
      "Epoch 313 train loss: 1.1534712314605713 train acc: 0.8979166746139526 test loss: 1.340098261833191 best test loss: 1.3350337743759155 test acc: 0.7166666388511658\n",
      "Epoch 314 train loss: 1.1501761674880981 train acc: 0.9041666388511658 test loss: 1.3513200283050537 best test loss: 1.3350337743759155 test acc: 0.6916666626930237\n",
      "Epoch 315 train loss: 1.149831771850586 train acc: 0.90625 test loss: 1.3464466333389282 best test loss: 1.3350337743759155 test acc: 0.7083333134651184\n",
      "Epoch 316 train loss: 1.1496081352233887 train acc: 0.90625 test loss: 1.3330366611480713 best test loss: 1.3330366611480713 test acc: 0.699999988079071\n",
      "Epoch 317 train loss: 1.149110198020935 train acc: 0.90625 test loss: 1.3575172424316406 best test loss: 1.3330366611480713 test acc: 0.6833333373069763\n",
      "Epoch 318 train loss: 1.149428129196167 train acc: 0.90625 test loss: 1.3514515161514282 best test loss: 1.3330366611480713 test acc: 0.7083333134651184\n",
      "Epoch 319 train loss: 1.1491926908493042 train acc: 0.9041666388511658 test loss: 1.336158275604248 best test loss: 1.3330366611480713 test acc: 0.7250000238418579\n",
      "Epoch 320 train loss: 1.1483137607574463 train acc: 0.9041666388511658 test loss: 1.3398343324661255 best test loss: 1.3330366611480713 test acc: 0.7083333134651184\n",
      "Epoch 321 train loss: 1.1475211381912231 train acc: 0.90625 test loss: 1.3482511043548584 best test loss: 1.3330366611480713 test acc: 0.6916666626930237\n",
      "Epoch 322 train loss: 1.145940899848938 train acc: 0.9083333611488342 test loss: 1.343285322189331 best test loss: 1.3330366611480713 test acc: 0.7166666388511658\n",
      "Epoch 323 train loss: 1.1436715126037598 train acc: 0.9083333611488342 test loss: 1.3492759466171265 best test loss: 1.3330366611480713 test acc: 0.675000011920929\n",
      "Epoch 324 train loss: 1.146998643875122 train acc: 0.9041666388511658 test loss: 1.3546370267868042 best test loss: 1.3330366611480713 test acc: 0.699999988079071\n",
      "Epoch 325 train loss: 1.1448979377746582 train acc: 0.90625 test loss: 1.340290904045105 best test loss: 1.3330366611480713 test acc: 0.7250000238418579\n",
      "Epoch 326 train loss: 1.146185040473938 train acc: 0.9083333611488342 test loss: 1.3442579507827759 best test loss: 1.3330366611480713 test acc: 0.7083333134651184\n",
      "Epoch 327 train loss: 1.1445549726486206 train acc: 0.9104166626930237 test loss: 1.3494949340820312 best test loss: 1.3330366611480713 test acc: 0.699999988079071\n",
      "Epoch 328 train loss: 1.1437846422195435 train acc: 0.9083333611488342 test loss: 1.3388503789901733 best test loss: 1.3330366611480713 test acc: 0.699999988079071\n",
      "Epoch 329 train loss: 1.1441978216171265 train acc: 0.90625 test loss: 1.3431661128997803 best test loss: 1.3330366611480713 test acc: 0.699999988079071\n",
      "Epoch 330 train loss: 1.1436563730239868 train acc: 0.90625 test loss: 1.3321627378463745 best test loss: 1.3321627378463745 test acc: 0.7250000238418579\n",
      "Epoch 331 train loss: 1.1440109014511108 train acc: 0.9083333611488342 test loss: 1.340203046798706 best test loss: 1.3321627378463745 test acc: 0.699999988079071\n",
      "Epoch 332 train loss: 1.1435457468032837 train acc: 0.9083333611488342 test loss: 1.3499184846878052 best test loss: 1.3321627378463745 test acc: 0.699999988079071\n",
      "Epoch 333 train loss: 1.1392085552215576 train acc: 0.9104166626930237 test loss: 1.3290793895721436 best test loss: 1.3290793895721436 test acc: 0.7166666388511658\n",
      "Epoch 334 train loss: 1.144054889678955 train acc: 0.90625 test loss: 1.3334516286849976 best test loss: 1.3290793895721436 test acc: 0.7166666388511658\n",
      "Epoch 335 train loss: 1.140788197517395 train acc: 0.9083333611488342 test loss: 1.334568738937378 best test loss: 1.3290793895721436 test acc: 0.7166666388511658\n",
      "Epoch 336 train loss: 1.1432603597640991 train acc: 0.9104166626930237 test loss: 1.3301342725753784 best test loss: 1.3290793895721436 test acc: 0.7250000238418579\n",
      "Epoch 337 train loss: 1.142322301864624 train acc: 0.9125000238418579 test loss: 1.3311704397201538 best test loss: 1.3290793895721436 test acc: 0.7083333134651184\n",
      "Epoch 338 train loss: 1.1388697624206543 train acc: 0.9125000238418579 test loss: 1.3500539064407349 best test loss: 1.3290793895721436 test acc: 0.675000011920929\n",
      "Epoch 339 train loss: 1.1387611627578735 train acc: 0.9104166626930237 test loss: 1.3267899751663208 best test loss: 1.3267899751663208 test acc: 0.7083333134651184\n",
      "Epoch 340 train loss: 1.14158296585083 train acc: 0.90625 test loss: 1.3451999425888062 best test loss: 1.3267899751663208 test acc: 0.6833333373069763\n",
      "Epoch 341 train loss: 1.1367605924606323 train acc: 0.9125000238418579 test loss: 1.3493062257766724 best test loss: 1.3267899751663208 test acc: 0.6833333373069763\n",
      "Epoch 342 train loss: 1.1392254829406738 train acc: 0.9104166626930237 test loss: 1.3377432823181152 best test loss: 1.3267899751663208 test acc: 0.6833333373069763\n",
      "Epoch 343 train loss: 1.1370317935943604 train acc: 0.9083333611488342 test loss: 1.3320159912109375 best test loss: 1.3267899751663208 test acc: 0.699999988079071\n",
      "Epoch 344 train loss: 1.1353111267089844 train acc: 0.9125000238418579 test loss: 1.3453630208969116 best test loss: 1.3267899751663208 test acc: 0.6833333373069763\n",
      "Epoch 345 train loss: 1.138179898262024 train acc: 0.9145833253860474 test loss: 1.333985686302185 best test loss: 1.3267899751663208 test acc: 0.7083333134651184\n",
      "Epoch 346 train loss: 1.1366221904754639 train acc: 0.9145833253860474 test loss: 1.3380218744277954 best test loss: 1.3267899751663208 test acc: 0.6833333373069763\n",
      "Epoch 347 train loss: 1.1360594034194946 train acc: 0.9145833253860474 test loss: 1.3408268690109253 best test loss: 1.3267899751663208 test acc: 0.6916666626930237\n",
      "Epoch 348 train loss: 1.134313941001892 train acc: 0.9145833253860474 test loss: 1.3412991762161255 best test loss: 1.3267899751663208 test acc: 0.7166666388511658\n",
      "Epoch 349 train loss: 1.136886477470398 train acc: 0.9145833253860474 test loss: 1.34903883934021 best test loss: 1.3267899751663208 test acc: 0.6833333373069763\n",
      "Epoch 350 train loss: 1.1369987726211548 train acc: 0.9104166626930237 test loss: 1.326525330543518 best test loss: 1.326525330543518 test acc: 0.7083333134651184\n",
      "Epoch 351 train loss: 1.1366828680038452 train acc: 0.9125000238418579 test loss: 1.3310362100601196 best test loss: 1.326525330543518 test acc: 0.7083333134651184\n",
      "Epoch 352 train loss: 1.1352020502090454 train acc: 0.9145833253860474 test loss: 1.329804539680481 best test loss: 1.326525330543518 test acc: 0.7083333134651184\n",
      "Epoch 353 train loss: 1.1388745307922363 train acc: 0.9125000238418579 test loss: 1.314027190208435 best test loss: 1.314027190208435 test acc: 0.7333333492279053\n",
      "Epoch 354 train loss: 1.138195276260376 train acc: 0.9145833253860474 test loss: 1.3319751024246216 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 355 train loss: 1.1356863975524902 train acc: 0.9125000238418579 test loss: 1.3311233520507812 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 356 train loss: 1.133252739906311 train acc: 0.9166666865348816 test loss: 1.3348495960235596 best test loss: 1.314027190208435 test acc: 0.7166666388511658\n",
      "Epoch 357 train loss: 1.1354295015335083 train acc: 0.9125000238418579 test loss: 1.357221007347107 best test loss: 1.314027190208435 test acc: 0.675000011920929\n",
      "Epoch 358 train loss: 1.1343621015548706 train acc: 0.9145833253860474 test loss: 1.3481851816177368 best test loss: 1.314027190208435 test acc: 0.675000011920929\n",
      "Epoch 359 train loss: 1.1349273920059204 train acc: 0.9166666865348816 test loss: 1.3377540111541748 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 360 train loss: 1.1347442865371704 train acc: 0.9145833253860474 test loss: 1.328296422958374 best test loss: 1.314027190208435 test acc: 0.7166666388511658\n",
      "Epoch 361 train loss: 1.1329407691955566 train acc: 0.9166666865348816 test loss: 1.3368321657180786 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 362 train loss: 1.1337676048278809 train acc: 0.9145833253860474 test loss: 1.3431427478790283 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 363 train loss: 1.13229238986969 train acc: 0.9166666865348816 test loss: 1.33027982711792 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 364 train loss: 1.1327109336853027 train acc: 0.9166666865348816 test loss: 1.3293176889419556 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 365 train loss: 1.1336007118225098 train acc: 0.9166666865348816 test loss: 1.3268828392028809 best test loss: 1.314027190208435 test acc: 0.7166666388511658\n",
      "Epoch 366 train loss: 1.1330686807632446 train acc: 0.9125000238418579 test loss: 1.3345755338668823 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 367 train loss: 1.1317485570907593 train acc: 0.9145833253860474 test loss: 1.3554348945617676 best test loss: 1.314027190208435 test acc: 0.6833333373069763\n",
      "Epoch 368 train loss: 1.1341665983200073 train acc: 0.9145833253860474 test loss: 1.3317350149154663 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 369 train loss: 1.1308815479278564 train acc: 0.9166666865348816 test loss: 1.3251690864562988 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 370 train loss: 1.132239580154419 train acc: 0.9166666865348816 test loss: 1.3381288051605225 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 371 train loss: 1.1330207586288452 train acc: 0.9166666865348816 test loss: 1.339401125907898 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 372 train loss: 1.1323378086090088 train acc: 0.9166666865348816 test loss: 1.3315218687057495 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 373 train loss: 1.1323944330215454 train acc: 0.9145833253860474 test loss: 1.3344918489456177 best test loss: 1.314027190208435 test acc: 0.7083333134651184\n",
      "Epoch 374 train loss: 1.131473422050476 train acc: 0.9166666865348816 test loss: 1.328991413116455 best test loss: 1.314027190208435 test acc: 0.7166666388511658\n",
      "Epoch 375 train loss: 1.1274617910385132 train acc: 0.918749988079071 test loss: 1.3380699157714844 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 376 train loss: 1.1286803483963013 train acc: 0.9166666865348816 test loss: 1.337867021560669 best test loss: 1.314027190208435 test acc: 0.6833333373069763\n",
      "Epoch 377 train loss: 1.1307048797607422 train acc: 0.9166666865348816 test loss: 1.3344238996505737 best test loss: 1.314027190208435 test acc: 0.7166666388511658\n",
      "Epoch 378 train loss: 1.1287099123001099 train acc: 0.918749988079071 test loss: 1.3401392698287964 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 379 train loss: 1.131781816482544 train acc: 0.9145833253860474 test loss: 1.3229366540908813 best test loss: 1.314027190208435 test acc: 0.7250000238418579\n",
      "Epoch 380 train loss: 1.130315899848938 train acc: 0.9145833253860474 test loss: 1.3356215953826904 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 381 train loss: 1.1316115856170654 train acc: 0.9166666865348816 test loss: 1.3379541635513306 best test loss: 1.314027190208435 test acc: 0.699999988079071\n",
      "Epoch 382 train loss: 1.1297948360443115 train acc: 0.9166666865348816 test loss: 1.3409042358398438 best test loss: 1.314027190208435 test acc: 0.6833333373069763\n",
      "Epoch 383 train loss: 1.1285228729248047 train acc: 0.9208333492279053 test loss: 1.3253189325332642 best test loss: 1.314027190208435 test acc: 0.7166666388511658\n",
      "Epoch 384 train loss: 1.126911997795105 train acc: 0.918749988079071 test loss: 1.3549158573150635 best test loss: 1.314027190208435 test acc: 0.6666666865348816\n",
      "Epoch 385 train loss: 1.127912998199463 train acc: 0.9208333492279053 test loss: 1.3104861974716187 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 386 train loss: 1.126869559288025 train acc: 0.9208333492279053 test loss: 1.3357518911361694 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 387 train loss: 1.1280323266983032 train acc: 0.918749988079071 test loss: 1.3381980657577515 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 388 train loss: 1.1271166801452637 train acc: 0.918749988079071 test loss: 1.3443390130996704 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 389 train loss: 1.1262965202331543 train acc: 0.9208333492279053 test loss: 1.3347529172897339 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 390 train loss: 1.1259127855300903 train acc: 0.9208333492279053 test loss: 1.3366260528564453 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 391 train loss: 1.1283117532730103 train acc: 0.9208333492279053 test loss: 1.3436051607131958 best test loss: 1.3104861974716187 test acc: 0.6833333373069763\n",
      "Epoch 392 train loss: 1.1270298957824707 train acc: 0.9208333492279053 test loss: 1.3266559839248657 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 393 train loss: 1.1258021593093872 train acc: 0.9208333492279053 test loss: 1.3234089612960815 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 394 train loss: 1.126041054725647 train acc: 0.9208333492279053 test loss: 1.3239413499832153 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 395 train loss: 1.1289499998092651 train acc: 0.9208333492279053 test loss: 1.3413439989089966 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 396 train loss: 1.125372290611267 train acc: 0.9208333492279053 test loss: 1.3235701322555542 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 397 train loss: 1.1259288787841797 train acc: 0.918749988079071 test loss: 1.3267968893051147 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 398 train loss: 1.1240333318710327 train acc: 0.9229166507720947 test loss: 1.3426114320755005 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 399 train loss: 1.125073790550232 train acc: 0.9229166507720947 test loss: 1.3172427415847778 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 400 train loss: 1.1279183626174927 train acc: 0.918749988079071 test loss: 1.3202693462371826 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 401 train loss: 1.1249622106552124 train acc: 0.9229166507720947 test loss: 1.3409278392791748 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 402 train loss: 1.1239242553710938 train acc: 0.9229166507720947 test loss: 1.3313337564468384 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 403 train loss: 1.1239426136016846 train acc: 0.9229166507720947 test loss: 1.324681043624878 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 404 train loss: 1.1241612434387207 train acc: 0.9229166507720947 test loss: 1.3276231288909912 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 405 train loss: 1.1239516735076904 train acc: 0.9229166507720947 test loss: 1.332355260848999 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 406 train loss: 1.1243185997009277 train acc: 0.9208333492279053 test loss: 1.3482035398483276 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 407 train loss: 1.1246986389160156 train acc: 0.9208333492279053 test loss: 1.348707914352417 best test loss: 1.3104861974716187 test acc: 0.6833333373069763\n",
      "Epoch 408 train loss: 1.1249910593032837 train acc: 0.9229166507720947 test loss: 1.3156859874725342 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 409 train loss: 1.1235311031341553 train acc: 0.9208333492279053 test loss: 1.3147722482681274 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 410 train loss: 1.1247847080230713 train acc: 0.9208333492279053 test loss: 1.3309123516082764 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 411 train loss: 1.1238256692886353 train acc: 0.9229166507720947 test loss: 1.339506983757019 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 412 train loss: 1.122729778289795 train acc: 0.925000011920929 test loss: 1.3300100564956665 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 413 train loss: 1.122708797454834 train acc: 0.9229166507720947 test loss: 1.3277608156204224 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 414 train loss: 1.1227948665618896 train acc: 0.925000011920929 test loss: 1.32444167137146 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 415 train loss: 1.1237924098968506 train acc: 0.9229166507720947 test loss: 1.3165111541748047 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 416 train loss: 1.1238377094268799 train acc: 0.9208333492279053 test loss: 1.331222653388977 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 417 train loss: 1.122774362564087 train acc: 0.925000011920929 test loss: 1.3255735635757446 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 418 train loss: 1.1224627494812012 train acc: 0.9229166507720947 test loss: 1.3320561647415161 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 419 train loss: 1.1237348318099976 train acc: 0.9208333492279053 test loss: 1.3258494138717651 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 420 train loss: 1.121635913848877 train acc: 0.9229166507720947 test loss: 1.3161180019378662 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 421 train loss: 1.1196328401565552 train acc: 0.925000011920929 test loss: 1.3281551599502563 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 422 train loss: 1.1237685680389404 train acc: 0.9208333492279053 test loss: 1.3308202028274536 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 423 train loss: 1.1218405961990356 train acc: 0.9229166507720947 test loss: 1.3438410758972168 best test loss: 1.3104861974716187 test acc: 0.6833333373069763\n",
      "Epoch 424 train loss: 1.1232397556304932 train acc: 0.9229166507720947 test loss: 1.33566415309906 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 425 train loss: 1.1226567029953003 train acc: 0.9229166507720947 test loss: 1.333533763885498 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 426 train loss: 1.1200079917907715 train acc: 0.925000011920929 test loss: 1.3487021923065186 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 427 train loss: 1.1201083660125732 train acc: 0.925000011920929 test loss: 1.3241026401519775 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 428 train loss: 1.1203782558441162 train acc: 0.925000011920929 test loss: 1.3296257257461548 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 429 train loss: 1.1212226152420044 train acc: 0.925000011920929 test loss: 1.3214303255081177 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 430 train loss: 1.1210787296295166 train acc: 0.925000011920929 test loss: 1.330457329750061 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 431 train loss: 1.1214343309402466 train acc: 0.9229166507720947 test loss: 1.327703595161438 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 432 train loss: 1.1201008558273315 train acc: 0.925000011920929 test loss: 1.3243671655654907 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 433 train loss: 1.123504638671875 train acc: 0.9208333492279053 test loss: 1.314841389656067 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 434 train loss: 1.1212331056594849 train acc: 0.925000011920929 test loss: 1.3137133121490479 best test loss: 1.3104861974716187 test acc: 0.7416666746139526\n",
      "Epoch 435 train loss: 1.1211097240447998 train acc: 0.925000011920929 test loss: 1.3420045375823975 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 436 train loss: 1.1217589378356934 train acc: 0.9229166507720947 test loss: 1.330100655555725 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 437 train loss: 1.1207469701766968 train acc: 0.925000011920929 test loss: 1.3215196132659912 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 438 train loss: 1.1205482482910156 train acc: 0.925000011920929 test loss: 1.3208539485931396 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 439 train loss: 1.119078516960144 train acc: 0.925000011920929 test loss: 1.3312321901321411 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 440 train loss: 1.1191569566726685 train acc: 0.925000011920929 test loss: 1.3112788200378418 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 441 train loss: 1.1188539266586304 train acc: 0.925000011920929 test loss: 1.3254222869873047 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 442 train loss: 1.1189849376678467 train acc: 0.925000011920929 test loss: 1.3299611806869507 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 443 train loss: 1.122308373451233 train acc: 0.9229166507720947 test loss: 1.3323194980621338 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 444 train loss: 1.1188921928405762 train acc: 0.925000011920929 test loss: 1.3362430334091187 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 445 train loss: 1.1198773384094238 train acc: 0.925000011920929 test loss: 1.3197203874588013 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 446 train loss: 1.1223289966583252 train acc: 0.9208333492279053 test loss: 1.3232898712158203 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 447 train loss: 1.1205297708511353 train acc: 0.925000011920929 test loss: 1.3449692726135254 best test loss: 1.3104861974716187 test acc: 0.6833333373069763\n",
      "Epoch 448 train loss: 1.1194782257080078 train acc: 0.925000011920929 test loss: 1.3281456232070923 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 449 train loss: 1.1191859245300293 train acc: 0.925000011920929 test loss: 1.3110970258712769 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 450 train loss: 1.118492841720581 train acc: 0.925000011920929 test loss: 1.3394618034362793 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 451 train loss: 1.119889736175537 train acc: 0.9270833134651184 test loss: 1.3314818143844604 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 452 train loss: 1.1194987297058105 train acc: 0.925000011920929 test loss: 1.341577410697937 best test loss: 1.3104861974716187 test acc: 0.6916666626930237\n",
      "Epoch 453 train loss: 1.1188182830810547 train acc: 0.9270833134651184 test loss: 1.3398269414901733 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 454 train loss: 1.1199800968170166 train acc: 0.9270833134651184 test loss: 1.3155938386917114 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 455 train loss: 1.1193292140960693 train acc: 0.9291666746139526 test loss: 1.3350902795791626 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 456 train loss: 1.1199465990066528 train acc: 0.9229166507720947 test loss: 1.317025065422058 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 457 train loss: 1.1202011108398438 train acc: 0.925000011920929 test loss: 1.3364497423171997 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 458 train loss: 1.1185929775238037 train acc: 0.9270833134651184 test loss: 1.3232053518295288 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 459 train loss: 1.117156982421875 train acc: 0.9270833134651184 test loss: 1.3303883075714111 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 460 train loss: 1.1156845092773438 train acc: 0.9312499761581421 test loss: 1.3403880596160889 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 461 train loss: 1.1181471347808838 train acc: 0.9270833134651184 test loss: 1.3307541608810425 best test loss: 1.3104861974716187 test acc: 0.699999988079071\n",
      "Epoch 462 train loss: 1.1174036264419556 train acc: 0.9270833134651184 test loss: 1.3331502676010132 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 463 train loss: 1.1191757917404175 train acc: 0.925000011920929 test loss: 1.33031165599823 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 464 train loss: 1.1184645891189575 train acc: 0.9291666746139526 test loss: 1.3180783987045288 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 465 train loss: 1.1178545951843262 train acc: 0.9291666746139526 test loss: 1.316223382949829 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 466 train loss: 1.1167302131652832 train acc: 0.9291666746139526 test loss: 1.342875599861145 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 467 train loss: 1.115059733390808 train acc: 0.9312499761581421 test loss: 1.3132858276367188 best test loss: 1.3104861974716187 test acc: 0.7250000238418579\n",
      "Epoch 468 train loss: 1.1156425476074219 train acc: 0.9312499761581421 test loss: 1.3350050449371338 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 469 train loss: 1.116188406944275 train acc: 0.925000011920929 test loss: 1.3125685453414917 best test loss: 1.3104861974716187 test acc: 0.7333333492279053\n",
      "Epoch 470 train loss: 1.1175872087478638 train acc: 0.9270833134651184 test loss: 1.3225337266921997 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 471 train loss: 1.1162415742874146 train acc: 0.9312499761581421 test loss: 1.3285473585128784 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 472 train loss: 1.117499589920044 train acc: 0.9291666746139526 test loss: 1.3309792280197144 best test loss: 1.3104861974716187 test acc: 0.7083333134651184\n",
      "Epoch 473 train loss: 1.115476131439209 train acc: 0.9291666746139526 test loss: 1.3223934173583984 best test loss: 1.3104861974716187 test acc: 0.7166666388511658\n",
      "Epoch 474 train loss: 1.116086483001709 train acc: 0.9291666746139526 test loss: 1.2974205017089844 best test loss: 1.2974205017089844 test acc: 0.7583333253860474\n",
      "Epoch 475 train loss: 1.1167032718658447 train acc: 0.9291666746139526 test loss: 1.3464791774749756 best test loss: 1.2974205017089844 test acc: 0.6916666626930237\n",
      "Epoch 476 train loss: 1.1142786741256714 train acc: 0.9291666746139526 test loss: 1.3411959409713745 best test loss: 1.2974205017089844 test acc: 0.699999988079071\n",
      "Epoch 477 train loss: 1.1157327890396118 train acc: 0.9312499761581421 test loss: 1.3351576328277588 best test loss: 1.2974205017089844 test acc: 0.6916666626930237\n",
      "Epoch 478 train loss: 1.1131751537322998 train acc: 0.9312499761581421 test loss: 1.314362645149231 best test loss: 1.2974205017089844 test acc: 0.7166666388511658\n",
      "Epoch 479 train loss: 1.1143214702606201 train acc: 0.9312499761581421 test loss: 1.3266336917877197 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 480 train loss: 1.114373803138733 train acc: 0.9312499761581421 test loss: 1.309664011001587 best test loss: 1.2974205017089844 test acc: 0.7416666746139526\n",
      "Epoch 481 train loss: 1.1129848957061768 train acc: 0.9312499761581421 test loss: 1.318828821182251 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 482 train loss: 1.115416407585144 train acc: 0.9312499761581421 test loss: 1.311968207359314 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 483 train loss: 1.113763689994812 train acc: 0.9312499761581421 test loss: 1.3131046295166016 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 484 train loss: 1.1151691675186157 train acc: 0.9312499761581421 test loss: 1.3289215564727783 best test loss: 1.2974205017089844 test acc: 0.7083333134651184\n",
      "Epoch 485 train loss: 1.1143555641174316 train acc: 0.9312499761581421 test loss: 1.321872353553772 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 486 train loss: 1.1141364574432373 train acc: 0.9291666746139526 test loss: 1.337172031402588 best test loss: 1.2974205017089844 test acc: 0.699999988079071\n",
      "Epoch 487 train loss: 1.1151787042617798 train acc: 0.9312499761581421 test loss: 1.3108497858047485 best test loss: 1.2974205017089844 test acc: 0.7416666746139526\n",
      "Epoch 488 train loss: 1.114048719406128 train acc: 0.9312499761581421 test loss: 1.3051302433013916 best test loss: 1.2974205017089844 test acc: 0.7416666746139526\n",
      "Epoch 489 train loss: 1.114475965499878 train acc: 0.9312499761581421 test loss: 1.3283580541610718 best test loss: 1.2974205017089844 test acc: 0.7166666388511658\n",
      "Epoch 490 train loss: 1.1131377220153809 train acc: 0.9333333373069763 test loss: 1.3244223594665527 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 491 train loss: 1.113994836807251 train acc: 0.9312499761581421 test loss: 1.3075579404830933 best test loss: 1.2974205017089844 test acc: 0.7416666746139526\n",
      "Epoch 492 train loss: 1.1135823726654053 train acc: 0.9312499761581421 test loss: 1.3272958993911743 best test loss: 1.2974205017089844 test acc: 0.7166666388511658\n",
      "Epoch 493 train loss: 1.115021824836731 train acc: 0.9291666746139526 test loss: 1.30624258518219 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 494 train loss: 1.113149881362915 train acc: 0.9312499761581421 test loss: 1.309746503829956 best test loss: 1.2974205017089844 test acc: 0.7166666388511658\n",
      "Epoch 495 train loss: 1.113209843635559 train acc: 0.9312499761581421 test loss: 1.315046787261963 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 496 train loss: 1.115299105644226 train acc: 0.9312499761581421 test loss: 1.3276857137680054 best test loss: 1.2974205017089844 test acc: 0.7083333134651184\n",
      "Epoch 497 train loss: 1.1128101348876953 train acc: 0.9312499761581421 test loss: 1.3140517473220825 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 498 train loss: 1.1147189140319824 train acc: 0.9291666746139526 test loss: 1.3168604373931885 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 499 train loss: 1.1124874353408813 train acc: 0.9312499761581421 test loss: 1.3187859058380127 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 500 train loss: 1.1127641201019287 train acc: 0.9312499761581421 test loss: 1.322599172592163 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 501 train loss: 1.1130777597427368 train acc: 0.9312499761581421 test loss: 1.3282654285430908 best test loss: 1.2974205017089844 test acc: 0.699999988079071\n",
      "Epoch 502 train loss: 1.1144726276397705 train acc: 0.9291666746139526 test loss: 1.3239771127700806 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 503 train loss: 1.1123255491256714 train acc: 0.9354166388511658 test loss: 1.3201690912246704 best test loss: 1.2974205017089844 test acc: 0.7333333492279053\n",
      "Epoch 504 train loss: 1.112848162651062 train acc: 0.9312499761581421 test loss: 1.3140736818313599 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 505 train loss: 1.1115285158157349 train acc: 0.9354166388511658 test loss: 1.3268468379974365 best test loss: 1.2974205017089844 test acc: 0.699999988079071\n",
      "Epoch 506 train loss: 1.1116836071014404 train acc: 0.9333333373069763 test loss: 1.3059192895889282 best test loss: 1.2974205017089844 test acc: 0.75\n",
      "Epoch 507 train loss: 1.110080599784851 train acc: 0.9333333373069763 test loss: 1.3077425956726074 best test loss: 1.2974205017089844 test acc: 0.7250000238418579\n",
      "Epoch 508 train loss: 1.1128926277160645 train acc: 0.9312499761581421 test loss: 1.295072317123413 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 509 train loss: 1.1128379106521606 train acc: 0.9333333373069763 test loss: 1.3231059312820435 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 510 train loss: 1.1143959760665894 train acc: 0.9312499761581421 test loss: 1.3237677812576294 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 511 train loss: 1.1107120513916016 train acc: 0.9333333373069763 test loss: 1.3090298175811768 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 512 train loss: 1.1102855205535889 train acc: 0.9354166388511658 test loss: 1.300081729888916 best test loss: 1.295072317123413 test acc: 0.7583333253860474\n",
      "Epoch 513 train loss: 1.111013412475586 train acc: 0.9354166388511658 test loss: 1.306427001953125 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 514 train loss: 1.111365556716919 train acc: 0.9354166388511658 test loss: 1.3331040143966675 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 515 train loss: 1.1105066537857056 train acc: 0.9354166388511658 test loss: 1.3098864555358887 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 516 train loss: 1.1122186183929443 train acc: 0.9333333373069763 test loss: 1.3152438402175903 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 517 train loss: 1.1111787557601929 train acc: 0.9333333373069763 test loss: 1.3016656637191772 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 518 train loss: 1.112369418144226 train acc: 0.9333333373069763 test loss: 1.3062703609466553 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 519 train loss: 1.1113125085830688 train acc: 0.9354166388511658 test loss: 1.3018646240234375 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 520 train loss: 1.1114696264266968 train acc: 0.9354166388511658 test loss: 1.3259729146957397 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 521 train loss: 1.110421895980835 train acc: 0.9333333373069763 test loss: 1.3034805059432983 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 522 train loss: 1.1105326414108276 train acc: 0.9354166388511658 test loss: 1.3231092691421509 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 523 train loss: 1.1114234924316406 train acc: 0.9354166388511658 test loss: 1.3242213726043701 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 524 train loss: 1.110458493232727 train acc: 0.9333333373069763 test loss: 1.3286223411560059 best test loss: 1.295072317123413 test acc: 0.6833333373069763\n",
      "Epoch 525 train loss: 1.110076904296875 train acc: 0.9354166388511658 test loss: 1.3170092105865479 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 526 train loss: 1.1097657680511475 train acc: 0.9354166388511658 test loss: 1.3062515258789062 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 527 train loss: 1.110230565071106 train acc: 0.9354166388511658 test loss: 1.3152472972869873 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 528 train loss: 1.1091368198394775 train acc: 0.9354166388511658 test loss: 1.3119323253631592 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 529 train loss: 1.1111973524093628 train acc: 0.9354166388511658 test loss: 1.329024076461792 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 530 train loss: 1.1099827289581299 train acc: 0.9354166388511658 test loss: 1.300534725189209 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 531 train loss: 1.1115926504135132 train acc: 0.9333333373069763 test loss: 1.3118830919265747 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 532 train loss: 1.1094146966934204 train acc: 0.9354166388511658 test loss: 1.3114559650421143 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 533 train loss: 1.1102874279022217 train acc: 0.9354166388511658 test loss: 1.3132816553115845 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 534 train loss: 1.1111645698547363 train acc: 0.9354166388511658 test loss: 1.3094595670700073 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 535 train loss: 1.109958291053772 train acc: 0.9354166388511658 test loss: 1.3048837184906006 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 536 train loss: 1.1098389625549316 train acc: 0.9354166388511658 test loss: 1.3047184944152832 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 537 train loss: 1.109869122505188 train acc: 0.9354166388511658 test loss: 1.30048668384552 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 538 train loss: 1.1098048686981201 train acc: 0.9354166388511658 test loss: 1.3196654319763184 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 539 train loss: 1.11012864112854 train acc: 0.9333333373069763 test loss: 1.3173854351043701 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 540 train loss: 1.109690546989441 train acc: 0.9354166388511658 test loss: 1.320618987083435 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 541 train loss: 1.1103612184524536 train acc: 0.9354166388511658 test loss: 1.3152707815170288 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 542 train loss: 1.1096817255020142 train acc: 0.9354166388511658 test loss: 1.3165580034255981 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 543 train loss: 1.1091984510421753 train acc: 0.9354166388511658 test loss: 1.3141417503356934 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 544 train loss: 1.1095006465911865 train acc: 0.9354166388511658 test loss: 1.3148146867752075 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 545 train loss: 1.1096304655075073 train acc: 0.9354166388511658 test loss: 1.3271986246109009 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 546 train loss: 1.1119800806045532 train acc: 0.9333333373069763 test loss: 1.3137913942337036 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 547 train loss: 1.1087753772735596 train acc: 0.9354166388511658 test loss: 1.30540931224823 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 548 train loss: 1.1108992099761963 train acc: 0.9333333373069763 test loss: 1.3221789598464966 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 549 train loss: 1.1093087196350098 train acc: 0.9354166388511658 test loss: 1.328199863433838 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 550 train loss: 1.10877525806427 train acc: 0.9354166388511658 test loss: 1.3098640441894531 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 551 train loss: 1.1083945035934448 train acc: 0.9354166388511658 test loss: 1.3329352140426636 best test loss: 1.295072317123413 test acc: 0.699999988079071\n",
      "Epoch 552 train loss: 1.1088258028030396 train acc: 0.9354166388511658 test loss: 1.3145034313201904 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 553 train loss: 1.1085360050201416 train acc: 0.9354166388511658 test loss: 1.315760850906372 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 554 train loss: 1.1101136207580566 train acc: 0.9354166388511658 test loss: 1.3089171648025513 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 555 train loss: 1.1106489896774292 train acc: 0.9354166388511658 test loss: 1.3296974897384644 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 556 train loss: 1.109441876411438 train acc: 0.9354166388511658 test loss: 1.2996681928634644 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 557 train loss: 1.1088581085205078 train acc: 0.9354166388511658 test loss: 1.329351782798767 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 558 train loss: 1.1088645458221436 train acc: 0.9354166388511658 test loss: 1.3269145488739014 best test loss: 1.295072317123413 test acc: 0.699999988079071\n",
      "Epoch 559 train loss: 1.109014630317688 train acc: 0.9354166388511658 test loss: 1.3223663568496704 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 560 train loss: 1.1092897653579712 train acc: 0.9354166388511658 test loss: 1.3025678396224976 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 561 train loss: 1.1089714765548706 train acc: 0.9354166388511658 test loss: 1.321326494216919 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 562 train loss: 1.1089056730270386 train acc: 0.9354166388511658 test loss: 1.3265656232833862 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 563 train loss: 1.1091187000274658 train acc: 0.9354166388511658 test loss: 1.3183636665344238 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 564 train loss: 1.1096622943878174 train acc: 0.9354166388511658 test loss: 1.312059998512268 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 565 train loss: 1.1089107990264893 train acc: 0.9354166388511658 test loss: 1.3072700500488281 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 566 train loss: 1.1084551811218262 train acc: 0.9354166388511658 test loss: 1.3047677278518677 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 567 train loss: 1.1086369752883911 train acc: 0.9354166388511658 test loss: 1.326859712600708 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 568 train loss: 1.1089049577713013 train acc: 0.9354166388511658 test loss: 1.3115044832229614 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 569 train loss: 1.1081938743591309 train acc: 0.9354166388511658 test loss: 1.3288600444793701 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 570 train loss: 1.108404278755188 train acc: 0.9354166388511658 test loss: 1.3088383674621582 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 571 train loss: 1.1087380647659302 train acc: 0.9354166388511658 test loss: 1.32858407497406 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 572 train loss: 1.1092087030410767 train acc: 0.9354166388511658 test loss: 1.3109325170516968 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 573 train loss: 1.1088470220565796 train acc: 0.9354166388511658 test loss: 1.3136132955551147 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 574 train loss: 1.1086736917495728 train acc: 0.9354166388511658 test loss: 1.3075346946716309 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 575 train loss: 1.108402132987976 train acc: 0.9354166388511658 test loss: 1.3112932443618774 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 576 train loss: 1.1084800958633423 train acc: 0.9354166388511658 test loss: 1.3038021326065063 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 577 train loss: 1.1083489656448364 train acc: 0.9354166388511658 test loss: 1.3069572448730469 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 578 train loss: 1.109688401222229 train acc: 0.9354166388511658 test loss: 1.29857337474823 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 579 train loss: 1.1082452535629272 train acc: 0.9354166388511658 test loss: 1.306122899055481 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 580 train loss: 1.1087466478347778 train acc: 0.9354166388511658 test loss: 1.3065353631973267 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 581 train loss: 1.1084554195404053 train acc: 0.9354166388511658 test loss: 1.3259025812149048 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 582 train loss: 1.1083340644836426 train acc: 0.9354166388511658 test loss: 1.3208534717559814 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 583 train loss: 1.1090854406356812 train acc: 0.9354166388511658 test loss: 1.3055006265640259 best test loss: 1.295072317123413 test acc: 0.7583333253860474\n",
      "Epoch 584 train loss: 1.1088826656341553 train acc: 0.9354166388511658 test loss: 1.3263728618621826 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 585 train loss: 1.1091781854629517 train acc: 0.9354166388511658 test loss: 1.306026577949524 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 586 train loss: 1.1095757484436035 train acc: 0.9354166388511658 test loss: 1.3057259321212769 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 587 train loss: 1.1102900505065918 train acc: 0.9333333373069763 test loss: 1.3115001916885376 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 588 train loss: 1.1089084148406982 train acc: 0.9354166388511658 test loss: 1.3085033893585205 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 589 train loss: 1.1086822748184204 train acc: 0.9354166388511658 test loss: 1.317928433418274 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 590 train loss: 1.1092090606689453 train acc: 0.9354166388511658 test loss: 1.319986343383789 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 591 train loss: 1.1082820892333984 train acc: 0.9354166388511658 test loss: 1.3278119564056396 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 592 train loss: 1.1083292961120605 train acc: 0.9354166388511658 test loss: 1.3114134073257446 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 593 train loss: 1.1097935438156128 train acc: 0.9354166388511658 test loss: 1.3216476440429688 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 594 train loss: 1.1086713075637817 train acc: 0.9354166388511658 test loss: 1.3044630289077759 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 595 train loss: 1.1086620092391968 train acc: 0.9354166388511658 test loss: 1.3041751384735107 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 596 train loss: 1.1086848974227905 train acc: 0.9354166388511658 test loss: 1.3095582723617554 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 597 train loss: 1.1090034246444702 train acc: 0.9354166388511658 test loss: 1.3216675519943237 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 598 train loss: 1.108309268951416 train acc: 0.9354166388511658 test loss: 1.3273118734359741 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 599 train loss: 1.1082271337509155 train acc: 0.9354166388511658 test loss: 1.319724440574646 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 600 train loss: 1.1086084842681885 train acc: 0.9354166388511658 test loss: 1.3072251081466675 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 601 train loss: 1.1066774129867554 train acc: 0.9375 test loss: 1.305780053138733 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 602 train loss: 1.1108156442642212 train acc: 0.9333333373069763 test loss: 1.3022918701171875 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 603 train loss: 1.1083818674087524 train acc: 0.9354166388511658 test loss: 1.3251639604568481 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 604 train loss: 1.1080422401428223 train acc: 0.9354166388511658 test loss: 1.3107459545135498 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 605 train loss: 1.1085389852523804 train acc: 0.9354166388511658 test loss: 1.301693081855774 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 606 train loss: 1.1088346242904663 train acc: 0.9354166388511658 test loss: 1.3087910413742065 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 607 train loss: 1.1083400249481201 train acc: 0.9354166388511658 test loss: 1.3078023195266724 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 608 train loss: 1.1085094213485718 train acc: 0.9354166388511658 test loss: 1.3030275106430054 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 609 train loss: 1.1094022989273071 train acc: 0.9333333373069763 test loss: 1.3220231533050537 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 610 train loss: 1.1091941595077515 train acc: 0.9333333373069763 test loss: 1.327530026435852 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 611 train loss: 1.1084551811218262 train acc: 0.9354166388511658 test loss: 1.3073656558990479 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 612 train loss: 1.1084061861038208 train acc: 0.9354166388511658 test loss: 1.305759310722351 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 613 train loss: 1.108151912689209 train acc: 0.9354166388511658 test loss: 1.328806757926941 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 614 train loss: 1.1086241006851196 train acc: 0.9354166388511658 test loss: 1.3041877746582031 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 615 train loss: 1.1080458164215088 train acc: 0.9354166388511658 test loss: 1.324130654335022 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 616 train loss: 1.1072293519973755 train acc: 0.9354166388511658 test loss: 1.3215817213058472 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 617 train loss: 1.1083245277404785 train acc: 0.9354166388511658 test loss: 1.3468444347381592 best test loss: 1.295072317123413 test acc: 0.6833333373069763\n",
      "Epoch 618 train loss: 1.1063711643218994 train acc: 0.9375 test loss: 1.300150752067566 best test loss: 1.295072317123413 test acc: 0.75\n",
      "Epoch 619 train loss: 1.1085933446884155 train acc: 0.9375 test loss: 1.3168851137161255 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 620 train loss: 1.1075937747955322 train acc: 0.9354166388511658 test loss: 1.3225973844528198 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 621 train loss: 1.1071058511734009 train acc: 0.9375 test loss: 1.3398574590682983 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 622 train loss: 1.1061843633651733 train acc: 0.9354166388511658 test loss: 1.3254421949386597 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 623 train loss: 1.1050591468811035 train acc: 0.9395833611488342 test loss: 1.3391327857971191 best test loss: 1.295072317123413 test acc: 0.6916666626930237\n",
      "Epoch 624 train loss: 1.104615569114685 train acc: 0.9395833611488342 test loss: 1.3323508501052856 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 625 train loss: 1.1057380437850952 train acc: 0.9416666626930237 test loss: 1.3327428102493286 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 626 train loss: 1.105509638786316 train acc: 0.9375 test loss: 1.3191096782684326 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 627 train loss: 1.1036092042922974 train acc: 0.9416666626930237 test loss: 1.331520676612854 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 628 train loss: 1.1048684120178223 train acc: 0.9395833611488342 test loss: 1.3152190446853638 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 629 train loss: 1.1068689823150635 train acc: 0.9375 test loss: 1.3211455345153809 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 630 train loss: 1.1059505939483643 train acc: 0.9375 test loss: 1.3156999349594116 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 631 train loss: 1.1030291318893433 train acc: 0.9416666626930237 test loss: 1.3235127925872803 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 632 train loss: 1.1069486141204834 train acc: 0.9395833611488342 test loss: 1.319090723991394 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 633 train loss: 1.1037226915359497 train acc: 0.9395833611488342 test loss: 1.305820107460022 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 634 train loss: 1.102105736732483 train acc: 0.9437500238418579 test loss: 1.3076589107513428 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 635 train loss: 1.1031229496002197 train acc: 0.9395833611488342 test loss: 1.328047275543213 best test loss: 1.295072317123413 test acc: 0.7083333134651184\n",
      "Epoch 636 train loss: 1.1036404371261597 train acc: 0.9395833611488342 test loss: 1.3263509273529053 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 637 train loss: 1.1031485795974731 train acc: 0.9416666626930237 test loss: 1.3271448612213135 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 638 train loss: 1.1039721965789795 train acc: 0.9395833611488342 test loss: 1.3088229894638062 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 639 train loss: 1.101651668548584 train acc: 0.9416666626930237 test loss: 1.3205243349075317 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 640 train loss: 1.1030536890029907 train acc: 0.9416666626930237 test loss: 1.3123422861099243 best test loss: 1.295072317123413 test acc: 0.7416666746139526\n",
      "Epoch 641 train loss: 1.1027909517288208 train acc: 0.9416666626930237 test loss: 1.3102869987487793 best test loss: 1.295072317123413 test acc: 0.7250000238418579\n",
      "Epoch 642 train loss: 1.1026860475540161 train acc: 0.9416666626930237 test loss: 1.3251646757125854 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 643 train loss: 1.1039454936981201 train acc: 0.9395833611488342 test loss: 1.2978976964950562 best test loss: 1.295072317123413 test acc: 0.7666666507720947\n",
      "Epoch 644 train loss: 1.103667974472046 train acc: 0.9395833611488342 test loss: 1.3194319009780884 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 645 train loss: 1.103915810585022 train acc: 0.9416666626930237 test loss: 1.3302215337753296 best test loss: 1.295072317123413 test acc: 0.7333333492279053\n",
      "Epoch 646 train loss: 1.1027876138687134 train acc: 0.9437500238418579 test loss: 1.3339438438415527 best test loss: 1.295072317123413 test acc: 0.7166666388511658\n",
      "Epoch 647 train loss: 1.10035240650177 train acc: 0.9458333253860474 test loss: 1.2913285493850708 best test loss: 1.2913285493850708 test acc: 0.7666666507720947\n",
      "Epoch 648 train loss: 1.1011792421340942 train acc: 0.9416666626930237 test loss: 1.332748293876648 best test loss: 1.2913285493850708 test acc: 0.7083333134651184\n",
      "Epoch 649 train loss: 1.1002593040466309 train acc: 0.9437500238418579 test loss: 1.2991279363632202 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 650 train loss: 1.1011677980422974 train acc: 0.9437500238418579 test loss: 1.3042558431625366 best test loss: 1.2913285493850708 test acc: 0.7250000238418579\n",
      "Epoch 651 train loss: 1.101098895072937 train acc: 0.9437500238418579 test loss: 1.316199779510498 best test loss: 1.2913285493850708 test acc: 0.7166666388511658\n",
      "Epoch 652 train loss: 1.1023627519607544 train acc: 0.9416666626930237 test loss: 1.3159290552139282 best test loss: 1.2913285493850708 test acc: 0.7250000238418579\n",
      "Epoch 653 train loss: 1.0992968082427979 train acc: 0.9458333253860474 test loss: 1.326017141342163 best test loss: 1.2913285493850708 test acc: 0.6916666626930237\n",
      "Epoch 654 train loss: 1.0995326042175293 train acc: 0.9458333253860474 test loss: 1.30693781375885 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 655 train loss: 1.10360586643219 train acc: 0.9437500238418579 test loss: 1.2923777103424072 best test loss: 1.2913285493850708 test acc: 0.7583333253860474\n",
      "Epoch 656 train loss: 1.1023749113082886 train acc: 0.9437500238418579 test loss: 1.3343456983566284 best test loss: 1.2913285493850708 test acc: 0.699999988079071\n",
      "Epoch 657 train loss: 1.0993382930755615 train acc: 0.9458333253860474 test loss: 1.3043879270553589 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 658 train loss: 1.102378487586975 train acc: 0.9416666626930237 test loss: 1.320517659187317 best test loss: 1.2913285493850708 test acc: 0.699999988079071\n",
      "Epoch 659 train loss: 1.0993002653121948 train acc: 0.9458333253860474 test loss: 1.3021321296691895 best test loss: 1.2913285493850708 test acc: 0.7250000238418579\n",
      "Epoch 660 train loss: 1.1009318828582764 train acc: 0.9437500238418579 test loss: 1.3296468257904053 best test loss: 1.2913285493850708 test acc: 0.7083333134651184\n",
      "Epoch 661 train loss: 1.099205493927002 train acc: 0.9458333253860474 test loss: 1.318362832069397 best test loss: 1.2913285493850708 test acc: 0.699999988079071\n",
      "Epoch 662 train loss: 1.0995694398880005 train acc: 0.9458333253860474 test loss: 1.3048768043518066 best test loss: 1.2913285493850708 test acc: 0.75\n",
      "Epoch 663 train loss: 1.1003966331481934 train acc: 0.9437500238418579 test loss: 1.3153883218765259 best test loss: 1.2913285493850708 test acc: 0.7250000238418579\n",
      "Epoch 664 train loss: 1.0995620489120483 train acc: 0.9458333253860474 test loss: 1.3376014232635498 best test loss: 1.2913285493850708 test acc: 0.675000011920929\n",
      "Epoch 665 train loss: 1.0987296104431152 train acc: 0.9458333253860474 test loss: 1.3008559942245483 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 666 train loss: 1.099306583404541 train acc: 0.9458333253860474 test loss: 1.3325971364974976 best test loss: 1.2913285493850708 test acc: 0.6916666626930237\n",
      "Epoch 667 train loss: 1.0988508462905884 train acc: 0.9458333253860474 test loss: 1.325333833694458 best test loss: 1.2913285493850708 test acc: 0.7250000238418579\n",
      "Epoch 668 train loss: 1.1016613245010376 train acc: 0.9458333253860474 test loss: 1.304889440536499 best test loss: 1.2913285493850708 test acc: 0.75\n",
      "Epoch 669 train loss: 1.0983918905258179 train acc: 0.9458333253860474 test loss: 1.3055644035339355 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 670 train loss: 1.0984798669815063 train acc: 0.9458333253860474 test loss: 1.3162041902542114 best test loss: 1.2913285493850708 test acc: 0.7166666388511658\n",
      "Epoch 671 train loss: 1.0979199409484863 train acc: 0.9458333253860474 test loss: 1.3106213808059692 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 672 train loss: 1.098523497581482 train acc: 0.9458333253860474 test loss: 1.3017631769180298 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 673 train loss: 1.097541093826294 train acc: 0.9458333253860474 test loss: 1.3216744661331177 best test loss: 1.2913285493850708 test acc: 0.7083333134651184\n",
      "Epoch 674 train loss: 1.0984992980957031 train acc: 0.9458333253860474 test loss: 1.3044031858444214 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 675 train loss: 1.0966135263442993 train acc: 0.9479166865348816 test loss: 1.3174710273742676 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 676 train loss: 1.097751259803772 train acc: 0.9479166865348816 test loss: 1.3094346523284912 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 677 train loss: 1.0966931581497192 train acc: 0.9479166865348816 test loss: 1.3127403259277344 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 678 train loss: 1.0987333059310913 train acc: 0.9458333253860474 test loss: 1.3125709295272827 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 679 train loss: 1.097618818283081 train acc: 0.9458333253860474 test loss: 1.3281188011169434 best test loss: 1.2913285493850708 test acc: 0.7083333134651184\n",
      "Epoch 680 train loss: 1.0963035821914673 train acc: 0.9479166865348816 test loss: 1.295160174369812 best test loss: 1.2913285493850708 test acc: 0.75\n",
      "Epoch 681 train loss: 1.0968670845031738 train acc: 0.9479166865348816 test loss: 1.3048344850540161 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 682 train loss: 1.0986970663070679 train acc: 0.9479166865348816 test loss: 1.3175429105758667 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 683 train loss: 1.0967025756835938 train acc: 0.9479166865348816 test loss: 1.3049194812774658 best test loss: 1.2913285493850708 test acc: 0.7416666746139526\n",
      "Epoch 684 train loss: 1.0962414741516113 train acc: 0.9479166865348816 test loss: 1.3054946660995483 best test loss: 1.2913285493850708 test acc: 0.75\n",
      "Epoch 685 train loss: 1.0983433723449707 train acc: 0.9458333253860474 test loss: 1.3045860528945923 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 686 train loss: 1.097684621810913 train acc: 0.9479166865348816 test loss: 1.3325563669204712 best test loss: 1.2913285493850708 test acc: 0.7083333134651184\n",
      "Epoch 687 train loss: 1.0967808961868286 train acc: 0.9479166865348816 test loss: 1.321881890296936 best test loss: 1.2913285493850708 test acc: 0.7166666388511658\n",
      "Epoch 688 train loss: 1.0961143970489502 train acc: 0.9479166865348816 test loss: 1.3110557794570923 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 689 train loss: 1.0961711406707764 train acc: 0.9479166865348816 test loss: 1.3060972690582275 best test loss: 1.2913285493850708 test acc: 0.7333333492279053\n",
      "Epoch 690 train loss: 1.096075177192688 train acc: 0.9479166865348816 test loss: 1.3182402849197388 best test loss: 1.2913285493850708 test acc: 0.7083333134651184\n",
      "Epoch 691 train loss: 1.0965420007705688 train acc: 0.9479166865348816 test loss: 1.3133798837661743 best test loss: 1.2913285493850708 test acc: 0.7250000238418579\n",
      "Epoch 692 train loss: 1.0965063571929932 train acc: 0.9479166865348816 test loss: 1.291008472442627 best test loss: 1.291008472442627 test acc: 0.7583333253860474\n",
      "Epoch 693 train loss: 1.0966626405715942 train acc: 0.9479166865348816 test loss: 1.3038161993026733 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 694 train loss: 1.0963492393493652 train acc: 0.9479166865348816 test loss: 1.3411338329315186 best test loss: 1.291008472442627 test acc: 0.6833333373069763\n",
      "Epoch 695 train loss: 1.0959688425064087 train acc: 0.9479166865348816 test loss: 1.3200076818466187 best test loss: 1.291008472442627 test acc: 0.7083333134651184\n",
      "Epoch 696 train loss: 1.0960389375686646 train acc: 0.9479166865348816 test loss: 1.3168407678604126 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 697 train loss: 1.0974918603897095 train acc: 0.9479166865348816 test loss: 1.3111464977264404 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 698 train loss: 1.0959324836730957 train acc: 0.9479166865348816 test loss: 1.3125989437103271 best test loss: 1.291008472442627 test acc: 0.7416666746139526\n",
      "Epoch 699 train loss: 1.096494436264038 train acc: 0.9479166865348816 test loss: 1.3064230680465698 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 700 train loss: 1.0969460010528564 train acc: 0.9458333253860474 test loss: 1.3059051036834717 best test loss: 1.291008472442627 test acc: 0.7250000238418579\n",
      "Epoch 701 train loss: 1.096886396408081 train acc: 0.9479166865348816 test loss: 1.3266538381576538 best test loss: 1.291008472442627 test acc: 0.7083333134651184\n",
      "Epoch 702 train loss: 1.0960885286331177 train acc: 0.9479166865348816 test loss: 1.3096991777420044 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 703 train loss: 1.0966565608978271 train acc: 0.9479166865348816 test loss: 1.3171849250793457 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 704 train loss: 1.0971556901931763 train acc: 0.9458333253860474 test loss: 1.319589376449585 best test loss: 1.291008472442627 test acc: 0.7166666388511658\n",
      "Epoch 705 train loss: 1.0970267057418823 train acc: 0.9479166865348816 test loss: 1.315773606300354 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 706 train loss: 1.0966060161590576 train acc: 0.9479166865348816 test loss: 1.3128294944763184 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 707 train loss: 1.0963505506515503 train acc: 0.9479166865348816 test loss: 1.2950057983398438 best test loss: 1.291008472442627 test acc: 0.7666666507720947\n",
      "Epoch 708 train loss: 1.095908284187317 train acc: 0.9479166865348816 test loss: 1.3143173456192017 best test loss: 1.291008472442627 test acc: 0.7083333134651184\n",
      "Epoch 709 train loss: 1.0965907573699951 train acc: 0.9479166865348816 test loss: 1.3104604482650757 best test loss: 1.291008472442627 test acc: 0.7250000238418579\n",
      "Epoch 710 train loss: 1.0969973802566528 train acc: 0.9479166865348816 test loss: 1.3092314004898071 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 711 train loss: 1.0964648723602295 train acc: 0.9479166865348816 test loss: 1.3034437894821167 best test loss: 1.291008472442627 test acc: 0.7250000238418579\n",
      "Epoch 712 train loss: 1.0973259210586548 train acc: 0.9479166865348816 test loss: 1.2998194694519043 best test loss: 1.291008472442627 test acc: 0.7583333253860474\n",
      "Epoch 713 train loss: 1.09811270236969 train acc: 0.9458333253860474 test loss: 1.3134536743164062 best test loss: 1.291008472442627 test acc: 0.7083333134651184\n",
      "Epoch 714 train loss: 1.0960655212402344 train acc: 0.9479166865348816 test loss: 1.3307914733886719 best test loss: 1.291008472442627 test acc: 0.7083333134651184\n",
      "Epoch 715 train loss: 1.0961971282958984 train acc: 0.9479166865348816 test loss: 1.3079577684402466 best test loss: 1.291008472442627 test acc: 0.7333333492279053\n",
      "Epoch 716 train loss: 1.0965484380722046 train acc: 0.9479166865348816 test loss: 1.2902050018310547 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 717 train loss: 1.0956472158432007 train acc: 0.9479166865348816 test loss: 1.3267436027526855 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 718 train loss: 1.0961649417877197 train acc: 0.9479166865348816 test loss: 1.3167377710342407 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 719 train loss: 1.0969066619873047 train acc: 0.9479166865348816 test loss: 1.3242809772491455 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 720 train loss: 1.0962402820587158 train acc: 0.9479166865348816 test loss: 1.3042418956756592 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 721 train loss: 1.0963786840438843 train acc: 0.9479166865348816 test loss: 1.3253538608551025 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 722 train loss: 1.0967038869857788 train acc: 0.9479166865348816 test loss: 1.3124715089797974 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 723 train loss: 1.0961472988128662 train acc: 0.9479166865348816 test loss: 1.3178257942199707 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 724 train loss: 1.095574975013733 train acc: 0.9479166865348816 test loss: 1.3057713508605957 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 725 train loss: 1.0962413549423218 train acc: 0.9479166865348816 test loss: 1.2927671670913696 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 726 train loss: 1.0964739322662354 train acc: 0.9479166865348816 test loss: 1.3132158517837524 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 727 train loss: 1.09605073928833 train acc: 0.9479166865348816 test loss: 1.3158129453659058 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 728 train loss: 1.0977951288223267 train acc: 0.9458333253860474 test loss: 1.3220869302749634 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 729 train loss: 1.096337914466858 train acc: 0.9479166865348816 test loss: 1.3059738874435425 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 730 train loss: 1.0964761972427368 train acc: 0.9479166865348816 test loss: 1.323477029800415 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 731 train loss: 1.09684157371521 train acc: 0.9479166865348816 test loss: 1.3083750009536743 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 732 train loss: 1.0968149900436401 train acc: 0.9479166865348816 test loss: 1.3126966953277588 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 733 train loss: 1.0962079763412476 train acc: 0.9479166865348816 test loss: 1.3155962228775024 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 734 train loss: 1.0958247184753418 train acc: 0.9479166865348816 test loss: 1.3104941844940186 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 735 train loss: 1.0971359014511108 train acc: 0.9479166865348816 test loss: 1.3125145435333252 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 736 train loss: 1.0965794324874878 train acc: 0.9479166865348816 test loss: 1.3153213262557983 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 737 train loss: 1.0961298942565918 train acc: 0.9479166865348816 test loss: 1.3310517072677612 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 738 train loss: 1.0976996421813965 train acc: 0.9458333253860474 test loss: 1.305661916732788 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 739 train loss: 1.0962198972702026 train acc: 0.9479166865348816 test loss: 1.313744068145752 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 740 train loss: 1.0958365201950073 train acc: 0.9479166865348816 test loss: 1.3248993158340454 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 741 train loss: 1.0957903861999512 train acc: 0.9479166865348816 test loss: 1.2989604473114014 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 742 train loss: 1.0961782932281494 train acc: 0.9479166865348816 test loss: 1.3110727071762085 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 743 train loss: 1.0962425470352173 train acc: 0.9479166865348816 test loss: 1.3102530241012573 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 744 train loss: 1.0958549976348877 train acc: 0.9479166865348816 test loss: 1.2926722764968872 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 745 train loss: 1.0957039594650269 train acc: 0.9479166865348816 test loss: 1.3168470859527588 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 746 train loss: 1.0956978797912598 train acc: 0.9479166865348816 test loss: 1.2945743799209595 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 747 train loss: 1.0962262153625488 train acc: 0.9479166865348816 test loss: 1.3030143976211548 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 748 train loss: 1.0954936742782593 train acc: 0.9479166865348816 test loss: 1.3093469142913818 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 749 train loss: 1.095434308052063 train acc: 0.9479166865348816 test loss: 1.3044013977050781 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 750 train loss: 1.0961440801620483 train acc: 0.9479166865348816 test loss: 1.2947646379470825 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 751 train loss: 1.0958491563796997 train acc: 0.9479166865348816 test loss: 1.3184990882873535 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 752 train loss: 1.0958057641983032 train acc: 0.9479166865348816 test loss: 1.3158535957336426 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 753 train loss: 1.0949422121047974 train acc: 0.9479166865348816 test loss: 1.3176013231277466 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 754 train loss: 1.094962477684021 train acc: 0.9479166865348816 test loss: 1.3201591968536377 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 755 train loss: 1.094892144203186 train acc: 0.9479166865348816 test loss: 1.314707636833191 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 756 train loss: 1.0946459770202637 train acc: 0.949999988079071 test loss: 1.3358901739120483 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 757 train loss: 1.0956013202667236 train acc: 0.9479166865348816 test loss: 1.317495584487915 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 758 train loss: 1.0942931175231934 train acc: 0.949999988079071 test loss: 1.3127731084823608 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 759 train loss: 1.0942171812057495 train acc: 0.949999988079071 test loss: 1.332573413848877 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 760 train loss: 1.0953043699264526 train acc: 0.9479166865348816 test loss: 1.3158583641052246 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 761 train loss: 1.094573736190796 train acc: 0.949999988079071 test loss: 1.3165955543518066 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 762 train loss: 1.0956395864486694 train acc: 0.9479166865348816 test loss: 1.3196712732315063 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 763 train loss: 1.0946341753005981 train acc: 0.949999988079071 test loss: 1.3184078931808472 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 764 train loss: 1.0937660932540894 train acc: 0.949999988079071 test loss: 1.326704502105713 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 765 train loss: 1.0939266681671143 train acc: 0.949999988079071 test loss: 1.3181655406951904 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 766 train loss: 1.095136284828186 train acc: 0.9479166865348816 test loss: 1.3342822790145874 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 767 train loss: 1.0942299365997314 train acc: 0.949999988079071 test loss: 1.3426463603973389 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 768 train loss: 1.0949019193649292 train acc: 0.949999988079071 test loss: 1.3133494853973389 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 769 train loss: 1.0942662954330444 train acc: 0.949999988079071 test loss: 1.319726586341858 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 770 train loss: 1.0940303802490234 train acc: 0.949999988079071 test loss: 1.302719235420227 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 771 train loss: 1.0938870906829834 train acc: 0.949999988079071 test loss: 1.2942222356796265 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 772 train loss: 1.0945450067520142 train acc: 0.949999988079071 test loss: 1.3243553638458252 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 773 train loss: 1.095120906829834 train acc: 0.9479166865348816 test loss: 1.3029948472976685 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 774 train loss: 1.094611406326294 train acc: 0.949999988079071 test loss: 1.3257453441619873 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 775 train loss: 1.0933603048324585 train acc: 0.949999988079071 test loss: 1.3251452445983887 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 776 train loss: 1.093988060951233 train acc: 0.949999988079071 test loss: 1.3219627141952515 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 777 train loss: 1.0939446687698364 train acc: 0.949999988079071 test loss: 1.2912319898605347 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 778 train loss: 1.0937771797180176 train acc: 0.949999988079071 test loss: 1.3399196863174438 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 779 train loss: 1.094315767288208 train acc: 0.949999988079071 test loss: 1.305492639541626 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 780 train loss: 1.0935925245285034 train acc: 0.949999988079071 test loss: 1.3137975931167603 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 781 train loss: 1.0938326120376587 train acc: 0.949999988079071 test loss: 1.329723596572876 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 782 train loss: 1.0943976640701294 train acc: 0.949999988079071 test loss: 1.3115075826644897 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 783 train loss: 1.0941884517669678 train acc: 0.949999988079071 test loss: 1.3265471458435059 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 784 train loss: 1.0937100648880005 train acc: 0.949999988079071 test loss: 1.3167650699615479 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 785 train loss: 1.0939030647277832 train acc: 0.949999988079071 test loss: 1.3160881996154785 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 786 train loss: 1.0961068868637085 train acc: 0.9479166865348816 test loss: 1.3083359003067017 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 787 train loss: 1.0942937135696411 train acc: 0.9479166865348816 test loss: 1.3141039609909058 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 788 train loss: 1.0936123132705688 train acc: 0.949999988079071 test loss: 1.2960389852523804 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 789 train loss: 1.0942333936691284 train acc: 0.949999988079071 test loss: 1.3070391416549683 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 790 train loss: 1.0938122272491455 train acc: 0.949999988079071 test loss: 1.3190761804580688 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 791 train loss: 1.0937631130218506 train acc: 0.949999988079071 test loss: 1.3283530473709106 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 792 train loss: 1.093605875968933 train acc: 0.949999988079071 test loss: 1.3144125938415527 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 793 train loss: 1.0945566892623901 train acc: 0.949999988079071 test loss: 1.3331577777862549 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 794 train loss: 1.0939444303512573 train acc: 0.949999988079071 test loss: 1.3386965990066528 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 795 train loss: 1.093727707862854 train acc: 0.949999988079071 test loss: 1.3178160190582275 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 796 train loss: 1.0939273834228516 train acc: 0.949999988079071 test loss: 1.3274749517440796 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 797 train loss: 1.0937201976776123 train acc: 0.949999988079071 test loss: 1.3450210094451904 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 798 train loss: 1.0936681032180786 train acc: 0.949999988079071 test loss: 1.3381706476211548 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 799 train loss: 1.0943273305892944 train acc: 0.949999988079071 test loss: 1.3264796733856201 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 800 train loss: 1.0946693420410156 train acc: 0.9479166865348816 test loss: 1.3091959953308105 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 801 train loss: 1.0940139293670654 train acc: 0.949999988079071 test loss: 1.3190770149230957 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 802 train loss: 1.0938712358474731 train acc: 0.949999988079071 test loss: 1.3335685729980469 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 803 train loss: 1.0937308073043823 train acc: 0.949999988079071 test loss: 1.3466942310333252 best test loss: 1.2902050018310547 test acc: 0.675000011920929\n",
      "Epoch 804 train loss: 1.093815565109253 train acc: 0.949999988079071 test loss: 1.3285776376724243 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 805 train loss: 1.0930066108703613 train acc: 0.949999988079071 test loss: 1.3385175466537476 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 806 train loss: 1.0938076972961426 train acc: 0.949999988079071 test loss: 1.3315712213516235 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 807 train loss: 1.0941082239151 train acc: 0.949999988079071 test loss: 1.3109556436538696 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 808 train loss: 1.0936610698699951 train acc: 0.949999988079071 test loss: 1.3028767108917236 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 809 train loss: 1.0936099290847778 train acc: 0.949999988079071 test loss: 1.3421971797943115 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 810 train loss: 1.0938762426376343 train acc: 0.949999988079071 test loss: 1.3111282587051392 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 811 train loss: 1.0935454368591309 train acc: 0.949999988079071 test loss: 1.3293007612228394 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 812 train loss: 1.0945111513137817 train acc: 0.9479166865348816 test loss: 1.3344783782958984 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 813 train loss: 1.0934921503067017 train acc: 0.949999988079071 test loss: 1.332878589630127 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 814 train loss: 1.093596339225769 train acc: 0.949999988079071 test loss: 1.3237847089767456 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 815 train loss: 1.0936332941055298 train acc: 0.949999988079071 test loss: 1.317919135093689 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 816 train loss: 1.093773365020752 train acc: 0.949999988079071 test loss: 1.3119277954101562 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 817 train loss: 1.0939794778823853 train acc: 0.949999988079071 test loss: 1.3130595684051514 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 818 train loss: 1.0938972234725952 train acc: 0.949999988079071 test loss: 1.3048672676086426 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 819 train loss: 1.0935920476913452 train acc: 0.949999988079071 test loss: 1.329478144645691 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 820 train loss: 1.0942463874816895 train acc: 0.949999988079071 test loss: 1.329899549484253 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 821 train loss: 1.093672275543213 train acc: 0.949999988079071 test loss: 1.3184278011322021 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 822 train loss: 1.0936626195907593 train acc: 0.949999988079071 test loss: 1.333763599395752 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 823 train loss: 1.0941429138183594 train acc: 0.949999988079071 test loss: 1.3355461359024048 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 824 train loss: 1.0941541194915771 train acc: 0.949999988079071 test loss: 1.3406023979187012 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 825 train loss: 1.0934314727783203 train acc: 0.949999988079071 test loss: 1.3274061679840088 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 826 train loss: 1.0934711694717407 train acc: 0.949999988079071 test loss: 1.3135205507278442 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 827 train loss: 1.092706322669983 train acc: 0.949999988079071 test loss: 1.3336181640625 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 828 train loss: 1.0930973291397095 train acc: 0.949999988079071 test loss: 1.3362042903900146 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 829 train loss: 1.0939253568649292 train acc: 0.949999988079071 test loss: 1.303096890449524 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 830 train loss: 1.0937186479568481 train acc: 0.949999988079071 test loss: 1.3230197429656982 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 831 train loss: 1.0933640003204346 train acc: 0.949999988079071 test loss: 1.3299044370651245 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 832 train loss: 1.0929443836212158 train acc: 0.9520833492279053 test loss: 1.317291498184204 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 833 train loss: 1.0931107997894287 train acc: 0.9520833492279053 test loss: 1.3362109661102295 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 834 train loss: 1.09259831905365 train acc: 0.9520833492279053 test loss: 1.309209942817688 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 835 train loss: 1.0944801568984985 train acc: 0.949999988079071 test loss: 1.31378173828125 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 836 train loss: 1.0935873985290527 train acc: 0.949999988079071 test loss: 1.3172917366027832 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 837 train loss: 1.0947740077972412 train acc: 0.949999988079071 test loss: 1.2984352111816406 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 838 train loss: 1.0938658714294434 train acc: 0.949999988079071 test loss: 1.3258014917373657 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 839 train loss: 1.0919528007507324 train acc: 0.9520833492279053 test loss: 1.3232537508010864 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 840 train loss: 1.0940676927566528 train acc: 0.9479166865348816 test loss: 1.322435736656189 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 841 train loss: 1.0923653841018677 train acc: 0.9520833492279053 test loss: 1.3294321298599243 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 842 train loss: 1.0930652618408203 train acc: 0.9520833492279053 test loss: 1.3387194871902466 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 843 train loss: 1.0940287113189697 train acc: 0.949999988079071 test loss: 1.3065637350082397 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 844 train loss: 1.0915030241012573 train acc: 0.9520833492279053 test loss: 1.3293254375457764 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 845 train loss: 1.0917187929153442 train acc: 0.9520833492279053 test loss: 1.3205417394638062 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 846 train loss: 1.091919183731079 train acc: 0.9520833492279053 test loss: 1.323851227760315 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 847 train loss: 1.0936405658721924 train acc: 0.949999988079071 test loss: 1.3425112962722778 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 848 train loss: 1.0937243700027466 train acc: 0.949999988079071 test loss: 1.3095955848693848 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 849 train loss: 1.0928014516830444 train acc: 0.949999988079071 test loss: 1.334898829460144 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 850 train loss: 1.09402334690094 train acc: 0.949999988079071 test loss: 1.314037799835205 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 851 train loss: 1.0928272008895874 train acc: 0.949999988079071 test loss: 1.3272509574890137 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 852 train loss: 1.0916484594345093 train acc: 0.9520833492279053 test loss: 1.3219596147537231 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 853 train loss: 1.0916634798049927 train acc: 0.9520833492279053 test loss: 1.312994122505188 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 854 train loss: 1.0922876596450806 train acc: 0.9520833492279053 test loss: 1.3042181730270386 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 855 train loss: 1.0925800800323486 train acc: 0.9520833492279053 test loss: 1.3235405683517456 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 856 train loss: 1.0939061641693115 train acc: 0.949999988079071 test loss: 1.323143720626831 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 857 train loss: 1.0915378332138062 train acc: 0.9520833492279053 test loss: 1.3421809673309326 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 858 train loss: 1.0917683839797974 train acc: 0.9520833492279053 test loss: 1.3029675483703613 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 859 train loss: 1.0914438962936401 train acc: 0.9520833492279053 test loss: 1.337647795677185 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 860 train loss: 1.0918222665786743 train acc: 0.9520833492279053 test loss: 1.3213346004486084 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 861 train loss: 1.0917835235595703 train acc: 0.9520833492279053 test loss: 1.3313813209533691 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 862 train loss: 1.092155933380127 train acc: 0.9520833492279053 test loss: 1.3220804929733276 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 863 train loss: 1.0923207998275757 train acc: 0.9520833492279053 test loss: 1.3279081583023071 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 864 train loss: 1.0928899049758911 train acc: 0.9520833492279053 test loss: 1.3437906503677368 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 865 train loss: 1.091943621635437 train acc: 0.9520833492279053 test loss: 1.305305004119873 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 866 train loss: 1.0915076732635498 train acc: 0.9520833492279053 test loss: 1.323340654373169 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 867 train loss: 1.091508388519287 train acc: 0.9520833492279053 test loss: 1.3163851499557495 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 868 train loss: 1.0914331674575806 train acc: 0.9520833492279053 test loss: 1.328323245048523 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 869 train loss: 1.0914913415908813 train acc: 0.9520833492279053 test loss: 1.3145204782485962 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 870 train loss: 1.0916603803634644 train acc: 0.9520833492279053 test loss: 1.3332394361495972 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 871 train loss: 1.0922791957855225 train acc: 0.9520833492279053 test loss: 1.3167694807052612 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 872 train loss: 1.0917531251907349 train acc: 0.9520833492279053 test loss: 1.323698878288269 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 873 train loss: 1.0916579961776733 train acc: 0.9520833492279053 test loss: 1.3056766986846924 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 874 train loss: 1.0916084051132202 train acc: 0.9520833492279053 test loss: 1.3206980228424072 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 875 train loss: 1.091560959815979 train acc: 0.9520833492279053 test loss: 1.3149338960647583 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 876 train loss: 1.0915974378585815 train acc: 0.9520833492279053 test loss: 1.3271318674087524 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 877 train loss: 1.0920761823654175 train acc: 0.9520833492279053 test loss: 1.3154394626617432 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 878 train loss: 1.0915426015853882 train acc: 0.9520833492279053 test loss: 1.3222870826721191 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 879 train loss: 1.09156334400177 train acc: 0.9520833492279053 test loss: 1.2951186895370483 best test loss: 1.2902050018310547 test acc: 0.7666666507720947\n",
      "Epoch 880 train loss: 1.091721773147583 train acc: 0.9520833492279053 test loss: 1.3215968608856201 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 881 train loss: 1.0939873456954956 train acc: 0.949999988079071 test loss: 1.3220174312591553 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 882 train loss: 1.0920499563217163 train acc: 0.9520833492279053 test loss: 1.3308480978012085 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 883 train loss: 1.0913197994232178 train acc: 0.9520833492279053 test loss: 1.3306044340133667 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 884 train loss: 1.0913664102554321 train acc: 0.9520833492279053 test loss: 1.3075554370880127 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 885 train loss: 1.0914355516433716 train acc: 0.9520833492279053 test loss: 1.340157151222229 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 886 train loss: 1.0926063060760498 train acc: 0.9520833492279053 test loss: 1.320024013519287 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 887 train loss: 1.0916507244110107 train acc: 0.9520833492279053 test loss: 1.3407460451126099 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 888 train loss: 1.091408133506775 train acc: 0.9520833492279053 test loss: 1.3129420280456543 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 889 train loss: 1.0931308269500732 train acc: 0.949999988079071 test loss: 1.3268824815750122 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 890 train loss: 1.091729998588562 train acc: 0.9520833492279053 test loss: 1.332391619682312 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 891 train loss: 1.0925368070602417 train acc: 0.9520833492279053 test loss: 1.3035070896148682 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 892 train loss: 1.0917229652404785 train acc: 0.9520833492279053 test loss: 1.3300819396972656 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 893 train loss: 1.0922057628631592 train acc: 0.9520833492279053 test loss: 1.326026439666748 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 894 train loss: 1.091407060623169 train acc: 0.9520833492279053 test loss: 1.3351171016693115 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 895 train loss: 1.0912115573883057 train acc: 0.9520833492279053 test loss: 1.3552244901657104 best test loss: 1.2902050018310547 test acc: 0.675000011920929\n",
      "Epoch 896 train loss: 1.0918899774551392 train acc: 0.9520833492279053 test loss: 1.331569790840149 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 897 train loss: 1.0912816524505615 train acc: 0.9520833492279053 test loss: 1.3346989154815674 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 898 train loss: 1.0912024974822998 train acc: 0.9520833492279053 test loss: 1.3272850513458252 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 899 train loss: 1.0918155908584595 train acc: 0.9520833492279053 test loss: 1.326917290687561 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 900 train loss: 1.0915768146514893 train acc: 0.9520833492279053 test loss: 1.30646812915802 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 901 train loss: 1.0910885334014893 train acc: 0.9520833492279053 test loss: 1.3437256813049316 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 902 train loss: 1.0915213823318481 train acc: 0.9520833492279053 test loss: 1.3249298334121704 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 903 train loss: 1.0912593603134155 train acc: 0.9520833492279053 test loss: 1.323928952217102 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 904 train loss: 1.093497633934021 train acc: 0.9479166865348816 test loss: 1.3155632019042969 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 905 train loss: 1.0928081274032593 train acc: 0.9520833492279053 test loss: 1.3140990734100342 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 906 train loss: 1.0915123224258423 train acc: 0.9520833492279053 test loss: 1.3161972761154175 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 907 train loss: 1.0912134647369385 train acc: 0.9520833492279053 test loss: 1.3411996364593506 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 908 train loss: 1.0913530588150024 train acc: 0.9520833492279053 test loss: 1.3062570095062256 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 909 train loss: 1.091354250907898 train acc: 0.9520833492279053 test loss: 1.322643756866455 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 910 train loss: 1.0914438962936401 train acc: 0.9520833492279053 test loss: 1.3321737051010132 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 911 train loss: 1.0921709537506104 train acc: 0.9520833492279053 test loss: 1.3263882398605347 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 912 train loss: 1.0917962789535522 train acc: 0.9520833492279053 test loss: 1.3305060863494873 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 913 train loss: 1.0914851427078247 train acc: 0.9520833492279053 test loss: 1.338181734085083 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 914 train loss: 1.091514229774475 train acc: 0.9520833492279053 test loss: 1.3262436389923096 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 915 train loss: 1.0914586782455444 train acc: 0.9520833492279053 test loss: 1.3082821369171143 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 916 train loss: 1.0915344953536987 train acc: 0.9520833492279053 test loss: 1.329960584640503 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 917 train loss: 1.0913399457931519 train acc: 0.9520833492279053 test loss: 1.334754228591919 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 918 train loss: 1.0911571979522705 train acc: 0.9520833492279053 test loss: 1.3475338220596313 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 919 train loss: 1.0911858081817627 train acc: 0.9520833492279053 test loss: 1.321329951286316 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 920 train loss: 1.091072916984558 train acc: 0.9520833492279053 test loss: 1.3196805715560913 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 921 train loss: 1.0913615226745605 train acc: 0.9520833492279053 test loss: 1.3182231187820435 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 922 train loss: 1.092314600944519 train acc: 0.949999988079071 test loss: 1.3231948614120483 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 923 train loss: 1.091283917427063 train acc: 0.9520833492279053 test loss: 1.3526675701141357 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 924 train loss: 1.091290831565857 train acc: 0.9520833492279053 test loss: 1.3371260166168213 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 925 train loss: 1.0911623239517212 train acc: 0.9520833492279053 test loss: 1.3525272607803345 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 926 train loss: 1.091460108757019 train acc: 0.9520833492279053 test loss: 1.3231914043426514 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 927 train loss: 1.0917818546295166 train acc: 0.9520833492279053 test loss: 1.3370473384857178 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 928 train loss: 1.091240644454956 train acc: 0.9520833492279053 test loss: 1.3452399969100952 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 929 train loss: 1.0913817882537842 train acc: 0.9520833492279053 test loss: 1.313198447227478 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 930 train loss: 1.0910913944244385 train acc: 0.9520833492279053 test loss: 1.3285315036773682 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 931 train loss: 1.0914967060089111 train acc: 0.9520833492279053 test loss: 1.3227753639221191 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 932 train loss: 1.0918385982513428 train acc: 0.9520833492279053 test loss: 1.3236113786697388 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 933 train loss: 1.0913994312286377 train acc: 0.9520833492279053 test loss: 1.3126872777938843 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 934 train loss: 1.0914126634597778 train acc: 0.9520833492279053 test loss: 1.3246318101882935 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 935 train loss: 1.0913147926330566 train acc: 0.9520833492279053 test loss: 1.325247883796692 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 936 train loss: 1.091227650642395 train acc: 0.9520833492279053 test loss: 1.329946756362915 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 937 train loss: 1.0913089513778687 train acc: 0.9520833492279053 test loss: 1.323855996131897 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 938 train loss: 1.0911368131637573 train acc: 0.9520833492279053 test loss: 1.339586615562439 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 939 train loss: 1.0911548137664795 train acc: 0.9520833492279053 test loss: 1.3197170495986938 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 940 train loss: 1.091199278831482 train acc: 0.9520833492279053 test loss: 1.3260031938552856 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 941 train loss: 1.0913127660751343 train acc: 0.9520833492279053 test loss: 1.3396475315093994 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 942 train loss: 1.0911563634872437 train acc: 0.9520833492279053 test loss: 1.3368350267410278 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 943 train loss: 1.0912516117095947 train acc: 0.9520833492279053 test loss: 1.3265010118484497 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 944 train loss: 1.0927345752716064 train acc: 0.949999988079071 test loss: 1.3121095895767212 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 945 train loss: 1.091275930404663 train acc: 0.9520833492279053 test loss: 1.3147295713424683 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 946 train loss: 1.0915067195892334 train acc: 0.9520833492279053 test loss: 1.3130592107772827 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 947 train loss: 1.0912036895751953 train acc: 0.9520833492279053 test loss: 1.325522780418396 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 948 train loss: 1.092155933380127 train acc: 0.9520833492279053 test loss: 1.3452399969100952 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 949 train loss: 1.0917725563049316 train acc: 0.9520833492279053 test loss: 1.3167378902435303 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 950 train loss: 1.0914607048034668 train acc: 0.9520833492279053 test loss: 1.3165392875671387 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 951 train loss: 1.0911651849746704 train acc: 0.9520833492279053 test loss: 1.332892894744873 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 952 train loss: 1.0910216569900513 train acc: 0.9520833492279053 test loss: 1.3391886949539185 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 953 train loss: 1.0921446084976196 train acc: 0.9520833492279053 test loss: 1.3111381530761719 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 954 train loss: 1.0912983417510986 train acc: 0.9520833492279053 test loss: 1.3329697847366333 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 955 train loss: 1.091357707977295 train acc: 0.9520833492279053 test loss: 1.316881775856018 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 956 train loss: 1.0914990901947021 train acc: 0.9520833492279053 test loss: 1.3252642154693604 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 957 train loss: 1.0918583869934082 train acc: 0.9520833492279053 test loss: 1.341927170753479 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 958 train loss: 1.0916078090667725 train acc: 0.9520833492279053 test loss: 1.3269299268722534 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 959 train loss: 1.091681718826294 train acc: 0.9520833492279053 test loss: 1.3331348896026611 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 960 train loss: 1.0912535190582275 train acc: 0.9520833492279053 test loss: 1.321602463722229 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 961 train loss: 1.0915242433547974 train acc: 0.9520833492279053 test loss: 1.3273574113845825 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 962 train loss: 1.0915272235870361 train acc: 0.9520833492279053 test loss: 1.3468948602676392 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 963 train loss: 1.0910305976867676 train acc: 0.9520833492279053 test loss: 1.3190966844558716 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 964 train loss: 1.0917414426803589 train acc: 0.9520833492279053 test loss: 1.325907826423645 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 965 train loss: 1.0911674499511719 train acc: 0.9520833492279053 test loss: 1.3461641073226929 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 966 train loss: 1.0915807485580444 train acc: 0.9520833492279053 test loss: 1.3432751893997192 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 967 train loss: 1.0914502143859863 train acc: 0.9520833492279053 test loss: 1.3094531297683716 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 968 train loss: 1.091513752937317 train acc: 0.9520833492279053 test loss: 1.3098021745681763 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 969 train loss: 1.0913546085357666 train acc: 0.9520833492279053 test loss: 1.324422001838684 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 970 train loss: 1.0913389921188354 train acc: 0.9520833492279053 test loss: 1.3176039457321167 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 971 train loss: 1.091270089149475 train acc: 0.9520833492279053 test loss: 1.3378636837005615 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 972 train loss: 1.0913547277450562 train acc: 0.9520833492279053 test loss: 1.3507100343704224 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 973 train loss: 1.0912177562713623 train acc: 0.9520833492279053 test loss: 1.3420794010162354 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 974 train loss: 1.0910547971725464 train acc: 0.9520833492279053 test loss: 1.333263874053955 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 975 train loss: 1.0912861824035645 train acc: 0.9520833492279053 test loss: 1.2908740043640137 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 976 train loss: 1.0927146673202515 train acc: 0.949999988079071 test loss: 1.3246033191680908 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 977 train loss: 1.0909308195114136 train acc: 0.9520833492279053 test loss: 1.3384116888046265 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 978 train loss: 1.0912096500396729 train acc: 0.9520833492279053 test loss: 1.3442317247390747 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 979 train loss: 1.0912060737609863 train acc: 0.9520833492279053 test loss: 1.3364821672439575 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 980 train loss: 1.0910404920578003 train acc: 0.9520833492279053 test loss: 1.3217308521270752 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 981 train loss: 1.0913187265396118 train acc: 0.9520833492279053 test loss: 1.3184177875518799 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 982 train loss: 1.0916411876678467 train acc: 0.9520833492279053 test loss: 1.3254293203353882 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 983 train loss: 1.091321587562561 train acc: 0.9520833492279053 test loss: 1.341884732246399 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 984 train loss: 1.0913960933685303 train acc: 0.9520833492279053 test loss: 1.3241671323776245 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 985 train loss: 1.0912140607833862 train acc: 0.9520833492279053 test loss: 1.3387434482574463 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 986 train loss: 1.0916582345962524 train acc: 0.9520833492279053 test loss: 1.3150670528411865 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 987 train loss: 1.0911195278167725 train acc: 0.9520833492279053 test loss: 1.3358343839645386 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 988 train loss: 1.0910524129867554 train acc: 0.9520833492279053 test loss: 1.3390724658966064 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 989 train loss: 1.0910438299179077 train acc: 0.9520833492279053 test loss: 1.3253651857376099 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 990 train loss: 1.0911341905593872 train acc: 0.9520833492279053 test loss: 1.3448361158370972 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 991 train loss: 1.0915414094924927 train acc: 0.9520833492279053 test loss: 1.3254300355911255 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 992 train loss: 1.09122896194458 train acc: 0.9520833492279053 test loss: 1.3197335004806519 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 993 train loss: 1.091254711151123 train acc: 0.9520833492279053 test loss: 1.3229748010635376 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 994 train loss: 1.0910587310791016 train acc: 0.9520833492279053 test loss: 1.3350024223327637 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 995 train loss: 1.0913406610488892 train acc: 0.9520833492279053 test loss: 1.321624994277954 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 996 train loss: 1.0911524295806885 train acc: 0.9520833492279053 test loss: 1.328426718711853 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 997 train loss: 1.0909552574157715 train acc: 0.9520833492279053 test loss: 1.304196834564209 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 998 train loss: 1.0915582180023193 train acc: 0.9520833492279053 test loss: 1.3389235734939575 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 999 train loss: 1.0912082195281982 train acc: 0.9520833492279053 test loss: 1.3148472309112549 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1000 train loss: 1.0911670923233032 train acc: 0.9520833492279053 test loss: 1.30794358253479 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1001 train loss: 1.09122896194458 train acc: 0.9520833492279053 test loss: 1.3266128301620483 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1002 train loss: 1.09114670753479 train acc: 0.9520833492279053 test loss: 1.3175307512283325 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1003 train loss: 1.091265082359314 train acc: 0.9520833492279053 test loss: 1.3177114725112915 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1004 train loss: 1.0912773609161377 train acc: 0.9520833492279053 test loss: 1.3310986757278442 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1005 train loss: 1.0911502838134766 train acc: 0.9520833492279053 test loss: 1.3153618574142456 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1006 train loss: 1.0910900831222534 train acc: 0.9520833492279053 test loss: 1.3355473279953003 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1007 train loss: 1.0911504030227661 train acc: 0.9520833492279053 test loss: 1.3216701745986938 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1008 train loss: 1.091145396232605 train acc: 0.9520833492279053 test loss: 1.3135703802108765 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1009 train loss: 1.0912281274795532 train acc: 0.9520833492279053 test loss: 1.311260461807251 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1010 train loss: 1.0931090116500854 train acc: 0.949999988079071 test loss: 1.3392401933670044 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1011 train loss: 1.091182827949524 train acc: 0.9520833492279053 test loss: 1.3070025444030762 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1012 train loss: 1.0911117792129517 train acc: 0.9520833492279053 test loss: 1.318934679031372 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1013 train loss: 1.0911335945129395 train acc: 0.9520833492279053 test loss: 1.3177154064178467 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1014 train loss: 1.0911427736282349 train acc: 0.9520833492279053 test loss: 1.3402752876281738 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1015 train loss: 1.0912799835205078 train acc: 0.9520833492279053 test loss: 1.3366144895553589 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1016 train loss: 1.0915011167526245 train acc: 0.9520833492279053 test loss: 1.3163567781448364 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1017 train loss: 1.091222882270813 train acc: 0.9520833492279053 test loss: 1.3155250549316406 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1018 train loss: 1.0909801721572876 train acc: 0.9520833492279053 test loss: 1.3243440389633179 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1019 train loss: 1.0913325548171997 train acc: 0.9520833492279053 test loss: 1.3203798532485962 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1020 train loss: 1.091347098350525 train acc: 0.9520833492279053 test loss: 1.3158706426620483 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1021 train loss: 1.091056227684021 train acc: 0.9520833492279053 test loss: 1.3324590921401978 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1022 train loss: 1.091025471687317 train acc: 0.9520833492279053 test loss: 1.3152823448181152 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1023 train loss: 1.0909254550933838 train acc: 0.9520833492279053 test loss: 1.3395495414733887 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1024 train loss: 1.0912365913391113 train acc: 0.9520833492279053 test loss: 1.3370516300201416 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1025 train loss: 1.091038465499878 train acc: 0.9520833492279053 test loss: 1.3516088724136353 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1026 train loss: 1.0912069082260132 train acc: 0.9520833492279053 test loss: 1.3403719663619995 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1027 train loss: 1.0911755561828613 train acc: 0.9520833492279053 test loss: 1.3318235874176025 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1028 train loss: 1.0909161567687988 train acc: 0.9520833492279053 test loss: 1.336188554763794 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1029 train loss: 1.0912948846817017 train acc: 0.9520833492279053 test loss: 1.3127424716949463 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1030 train loss: 1.0913865566253662 train acc: 0.9520833492279053 test loss: 1.3176300525665283 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1031 train loss: 1.0915075540542603 train acc: 0.9520833492279053 test loss: 1.3338655233383179 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1032 train loss: 1.0911197662353516 train acc: 0.9520833492279053 test loss: 1.3207606077194214 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1033 train loss: 1.0910903215408325 train acc: 0.9520833492279053 test loss: 1.3175069093704224 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1034 train loss: 1.091115117073059 train acc: 0.9520833492279053 test loss: 1.3123153448104858 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1035 train loss: 1.0912556648254395 train acc: 0.9520833492279053 test loss: 1.3302550315856934 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1036 train loss: 1.0910264253616333 train acc: 0.9520833492279053 test loss: 1.3166842460632324 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1037 train loss: 1.0911716222763062 train acc: 0.9520833492279053 test loss: 1.3329126834869385 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1038 train loss: 1.0908879041671753 train acc: 0.9520833492279053 test loss: 1.3221434354782104 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1039 train loss: 1.091385006904602 train acc: 0.9520833492279053 test loss: 1.3047734498977661 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1040 train loss: 1.0913056135177612 train acc: 0.9520833492279053 test loss: 1.3113958835601807 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1041 train loss: 1.0910046100616455 train acc: 0.9520833492279053 test loss: 1.32516610622406 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1042 train loss: 1.0911346673965454 train acc: 0.9520833492279053 test loss: 1.322724461555481 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1043 train loss: 1.0912585258483887 train acc: 0.9520833492279053 test loss: 1.3210762739181519 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1044 train loss: 1.0912396907806396 train acc: 0.9520833492279053 test loss: 1.3324342966079712 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1045 train loss: 1.0913406610488892 train acc: 0.9520833492279053 test loss: 1.3277629613876343 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1046 train loss: 1.0918344259262085 train acc: 0.9520833492279053 test loss: 1.3274409770965576 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1047 train loss: 1.0913201570510864 train acc: 0.9520833492279053 test loss: 1.3148704767227173 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1048 train loss: 1.0911517143249512 train acc: 0.9520833492279053 test loss: 1.321770191192627 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1049 train loss: 1.091294765472412 train acc: 0.9520833492279053 test loss: 1.328583002090454 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1050 train loss: 1.0913349390029907 train acc: 0.9520833492279053 test loss: 1.3002960681915283 best test loss: 1.2902050018310547 test acc: 0.7583333253860474\n",
      "Epoch 1051 train loss: 1.09142005443573 train acc: 0.9520833492279053 test loss: 1.330942153930664 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1052 train loss: 1.0910619497299194 train acc: 0.9520833492279053 test loss: 1.3158477544784546 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1053 train loss: 1.0911109447479248 train acc: 0.9520833492279053 test loss: 1.3333038091659546 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1054 train loss: 1.0910395383834839 train acc: 0.9520833492279053 test loss: 1.3068746328353882 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1055 train loss: 1.091138482093811 train acc: 0.9520833492279053 test loss: 1.3357056379318237 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1056 train loss: 1.0911725759506226 train acc: 0.9520833492279053 test loss: 1.3314248323440552 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1057 train loss: 1.0911091566085815 train acc: 0.9520833492279053 test loss: 1.3002967834472656 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1058 train loss: 1.0911657810211182 train acc: 0.9520833492279053 test loss: 1.3286644220352173 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1059 train loss: 1.0910426378250122 train acc: 0.9520833492279053 test loss: 1.3069015741348267 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1060 train loss: 1.0912901163101196 train acc: 0.9520833492279053 test loss: 1.3044774532318115 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1061 train loss: 1.0910817384719849 train acc: 0.9520833492279053 test loss: 1.3251067399978638 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1062 train loss: 1.091083288192749 train acc: 0.9520833492279053 test loss: 1.337157964706421 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 1063 train loss: 1.091003656387329 train acc: 0.9520833492279053 test loss: 1.310279369354248 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1064 train loss: 1.0913995504379272 train acc: 0.9520833492279053 test loss: 1.3219826221466064 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1065 train loss: 1.0912338495254517 train acc: 0.9520833492279053 test loss: 1.342392086982727 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1066 train loss: 1.0911028385162354 train acc: 0.9520833492279053 test loss: 1.3254884481430054 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1067 train loss: 1.0910472869873047 train acc: 0.9520833492279053 test loss: 1.3137460947036743 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1068 train loss: 1.0910391807556152 train acc: 0.9520833492279053 test loss: 1.3255006074905396 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1069 train loss: 1.0913043022155762 train acc: 0.9520833492279053 test loss: 1.3181397914886475 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1070 train loss: 1.0913406610488892 train acc: 0.9520833492279053 test loss: 1.3200126886367798 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1071 train loss: 1.0911211967468262 train acc: 0.9520833492279053 test loss: 1.3132970333099365 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1072 train loss: 1.091179609298706 train acc: 0.9520833492279053 test loss: 1.3151986598968506 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1073 train loss: 1.0909006595611572 train acc: 0.9520833492279053 test loss: 1.3238259553909302 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1074 train loss: 1.0910226106643677 train acc: 0.9520833492279053 test loss: 1.321056842803955 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1075 train loss: 1.091539740562439 train acc: 0.9520833492279053 test loss: 1.3122844696044922 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1076 train loss: 1.091068148612976 train acc: 0.9520833492279053 test loss: 1.3320107460021973 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1077 train loss: 1.091371774673462 train acc: 0.9520833492279053 test loss: 1.3174805641174316 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1078 train loss: 1.0910089015960693 train acc: 0.9520833492279053 test loss: 1.3285038471221924 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1079 train loss: 1.0912123918533325 train acc: 0.9520833492279053 test loss: 1.3006436824798584 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1080 train loss: 1.0912107229232788 train acc: 0.9520833492279053 test loss: 1.323556900024414 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1081 train loss: 1.0908398628234863 train acc: 0.9520833492279053 test loss: 1.3010673522949219 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1082 train loss: 1.0911612510681152 train acc: 0.9520833492279053 test loss: 1.313188076019287 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1083 train loss: 1.0915286540985107 train acc: 0.9520833492279053 test loss: 1.3442870378494263 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1084 train loss: 1.0912824869155884 train acc: 0.9520833492279053 test loss: 1.327824354171753 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1085 train loss: 1.0911216735839844 train acc: 0.9520833492279053 test loss: 1.3282115459442139 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1086 train loss: 1.0910592079162598 train acc: 0.9520833492279053 test loss: 1.3256282806396484 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1087 train loss: 1.090973138809204 train acc: 0.9520833492279053 test loss: 1.3015141487121582 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1088 train loss: 1.0911641120910645 train acc: 0.9520833492279053 test loss: 1.3137760162353516 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1089 train loss: 1.091262698173523 train acc: 0.9520833492279053 test loss: 1.3162705898284912 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1090 train loss: 1.0910744667053223 train acc: 0.9520833492279053 test loss: 1.3178560733795166 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1091 train loss: 1.0909063816070557 train acc: 0.9520833492279053 test loss: 1.3130946159362793 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1092 train loss: 1.0916615724563599 train acc: 0.9520833492279053 test loss: 1.322738766670227 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1093 train loss: 1.0911844968795776 train acc: 0.9520833492279053 test loss: 1.330819845199585 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1094 train loss: 1.0913995504379272 train acc: 0.9520833492279053 test loss: 1.3316280841827393 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1095 train loss: 1.0916916131973267 train acc: 0.9520833492279053 test loss: 1.3511073589324951 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1096 train loss: 1.0913317203521729 train acc: 0.9520833492279053 test loss: 1.3170870542526245 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1097 train loss: 1.0910017490386963 train acc: 0.9520833492279053 test loss: 1.324119210243225 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1098 train loss: 1.0910625457763672 train acc: 0.9520833492279053 test loss: 1.3347567319869995 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1099 train loss: 1.0909991264343262 train acc: 0.9520833492279053 test loss: 1.3268489837646484 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1100 train loss: 1.0913008451461792 train acc: 0.9520833492279053 test loss: 1.3110748529434204 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1101 train loss: 1.091295599937439 train acc: 0.9520833492279053 test loss: 1.3329187631607056 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1102 train loss: 1.0912353992462158 train acc: 0.9520833492279053 test loss: 1.3538861274719238 best test loss: 1.2902050018310547 test acc: 0.6833333373069763\n",
      "Epoch 1103 train loss: 1.0912548303604126 train acc: 0.9520833492279053 test loss: 1.3176963329315186 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1104 train loss: 1.0911059379577637 train acc: 0.9520833492279053 test loss: 1.32328200340271 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1105 train loss: 1.0912400484085083 train acc: 0.9520833492279053 test loss: 1.322576642036438 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1106 train loss: 1.0910625457763672 train acc: 0.9520833492279053 test loss: 1.3225213289260864 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1107 train loss: 1.0910066366195679 train acc: 0.9520833492279053 test loss: 1.3267532587051392 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1108 train loss: 1.091082215309143 train acc: 0.9520833492279053 test loss: 1.3259388208389282 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1109 train loss: 1.0909805297851562 train acc: 0.9520833492279053 test loss: 1.3380074501037598 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1110 train loss: 1.0912531614303589 train acc: 0.9520833492279053 test loss: 1.3432613611221313 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1111 train loss: 1.0911787748336792 train acc: 0.9520833492279053 test loss: 1.3159115314483643 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1112 train loss: 1.0914223194122314 train acc: 0.9520833492279053 test loss: 1.308658480644226 best test loss: 1.2902050018310547 test acc: 0.7416666746139526\n",
      "Epoch 1113 train loss: 1.0911219120025635 train acc: 0.9520833492279053 test loss: 1.3401315212249756 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1114 train loss: 1.0909587144851685 train acc: 0.9520833492279053 test loss: 1.30452299118042 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1115 train loss: 1.0910805463790894 train acc: 0.9520833492279053 test loss: 1.335825800895691 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1116 train loss: 1.0910431146621704 train acc: 0.9520833492279053 test loss: 1.3242837190628052 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1117 train loss: 1.0911178588867188 train acc: 0.9520833492279053 test loss: 1.3382833003997803 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1118 train loss: 1.091115117073059 train acc: 0.9520833492279053 test loss: 1.338324785232544 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1119 train loss: 1.091257929801941 train acc: 0.9520833492279053 test loss: 1.3224446773529053 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1120 train loss: 1.0913313627243042 train acc: 0.9520833492279053 test loss: 1.3364876508712769 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1121 train loss: 1.0911154747009277 train acc: 0.9520833492279053 test loss: 1.3406375646591187 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1122 train loss: 1.0911954641342163 train acc: 0.9520833492279053 test loss: 1.351859450340271 best test loss: 1.2902050018310547 test acc: 0.6916666626930237\n",
      "Epoch 1123 train loss: 1.0913444757461548 train acc: 0.9520833492279053 test loss: 1.3373833894729614 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1124 train loss: 1.091103434562683 train acc: 0.9520833492279053 test loss: 1.3451637029647827 best test loss: 1.2902050018310547 test acc: 0.7083333134651184\n",
      "Epoch 1125 train loss: 1.0910383462905884 train acc: 0.9520833492279053 test loss: 1.338276982307434 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1126 train loss: 1.0911147594451904 train acc: 0.9520833492279053 test loss: 1.3148043155670166 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1127 train loss: 1.0911717414855957 train acc: 0.9520833492279053 test loss: 1.3209210634231567 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1128 train loss: 1.0910608768463135 train acc: 0.9520833492279053 test loss: 1.3203186988830566 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1129 train loss: 1.0911563634872437 train acc: 0.9520833492279053 test loss: 1.3132941722869873 best test loss: 1.2902050018310547 test acc: 0.7333333492279053\n",
      "Epoch 1130 train loss: 1.0910104513168335 train acc: 0.9520833492279053 test loss: 1.3270549774169922 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1131 train loss: 1.0912508964538574 train acc: 0.9520833492279053 test loss: 1.3399393558502197 best test loss: 1.2902050018310547 test acc: 0.699999988079071\n",
      "Epoch 1132 train loss: 1.0912327766418457 train acc: 0.9520833492279053 test loss: 1.3275402784347534 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1133 train loss: 1.0911632776260376 train acc: 0.9520833492279053 test loss: 1.3266047239303589 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1134 train loss: 1.0910733938217163 train acc: 0.9520833492279053 test loss: 1.309182047843933 best test loss: 1.2902050018310547 test acc: 0.75\n",
      "Epoch 1135 train loss: 1.0906506776809692 train acc: 0.9520833492279053 test loss: 1.324406385421753 best test loss: 1.2902050018310547 test acc: 0.7166666388511658\n",
      "Epoch 1136 train loss: 1.0911864042282104 train acc: 0.9520833492279053 test loss: 1.3196697235107422 best test loss: 1.2902050018310547 test acc: 0.7250000238418579\n",
      "Epoch 1137 train loss: 1.0916205644607544 train acc: 0.9520833492279053 test loss: 1.2862135171890259 best test loss: 1.2862135171890259 test acc: 0.7749999761581421\n",
      "Epoch 1138 train loss: 1.0914251804351807 train acc: 0.9520833492279053 test loss: 1.3237571716308594 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1139 train loss: 1.0912755727767944 train acc: 0.9520833492279053 test loss: 1.3069993257522583 best test loss: 1.2862135171890259 test acc: 0.7583333253860474\n",
      "Epoch 1140 train loss: 1.0909111499786377 train acc: 0.9520833492279053 test loss: 1.3223819732666016 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1141 train loss: 1.0907620191574097 train acc: 0.9520833492279053 test loss: 1.32568359375 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1142 train loss: 1.090995192527771 train acc: 0.9520833492279053 test loss: 1.3310792446136475 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1143 train loss: 1.0911616086959839 train acc: 0.9520833492279053 test loss: 1.3223785161972046 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1144 train loss: 1.0914642810821533 train acc: 0.9520833492279053 test loss: 1.3376561403274536 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1145 train loss: 1.0913522243499756 train acc: 0.9520833492279053 test loss: 1.3534330129623413 best test loss: 1.2862135171890259 test acc: 0.6833333373069763\n",
      "Epoch 1146 train loss: 1.0911294221878052 train acc: 0.9520833492279053 test loss: 1.307542085647583 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1147 train loss: 1.0910471677780151 train acc: 0.9520833492279053 test loss: 1.3135281801223755 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1148 train loss: 1.0911945104599 train acc: 0.9520833492279053 test loss: 1.3183354139328003 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1149 train loss: 1.0907244682312012 train acc: 0.9520833492279053 test loss: 1.3365014791488647 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1150 train loss: 1.0912660360336304 train acc: 0.9520833492279053 test loss: 1.3311841487884521 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1151 train loss: 1.091158151626587 train acc: 0.9520833492279053 test loss: 1.3292319774627686 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1152 train loss: 1.091051697731018 train acc: 0.9520833492279053 test loss: 1.3011202812194824 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1153 train loss: 1.0911260843276978 train acc: 0.9520833492279053 test loss: 1.3362911939620972 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1154 train loss: 1.0911099910736084 train acc: 0.9520833492279053 test loss: 1.311230182647705 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1155 train loss: 1.0910016298294067 train acc: 0.9520833492279053 test loss: 1.3278357982635498 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1156 train loss: 1.0911295413970947 train acc: 0.9520833492279053 test loss: 1.2979819774627686 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1157 train loss: 1.091001272201538 train acc: 0.9520833492279053 test loss: 1.3243520259857178 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1158 train loss: 1.0912240743637085 train acc: 0.9520833492279053 test loss: 1.3172756433486938 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1159 train loss: 1.0910298824310303 train acc: 0.9520833492279053 test loss: 1.3315318822860718 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1160 train loss: 1.0909769535064697 train acc: 0.9520833492279053 test loss: 1.3275777101516724 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1161 train loss: 1.0910893678665161 train acc: 0.9520833492279053 test loss: 1.3111140727996826 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1162 train loss: 1.0910903215408325 train acc: 0.9520833492279053 test loss: 1.3232783079147339 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1163 train loss: 1.090859055519104 train acc: 0.9520833492279053 test loss: 1.320476770401001 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1164 train loss: 1.0913853645324707 train acc: 0.9520833492279053 test loss: 1.3202552795410156 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1165 train loss: 1.0912995338439941 train acc: 0.9520833492279053 test loss: 1.3096827268600464 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1166 train loss: 1.0911507606506348 train acc: 0.9520833492279053 test loss: 1.3316842317581177 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1167 train loss: 1.0911129713058472 train acc: 0.9520833492279053 test loss: 1.2988859415054321 best test loss: 1.2862135171890259 test acc: 0.7583333253860474\n",
      "Epoch 1168 train loss: 1.0910439491271973 train acc: 0.9520833492279053 test loss: 1.2947382926940918 best test loss: 1.2862135171890259 test acc: 0.7583333253860474\n",
      "Epoch 1169 train loss: 1.0911809206008911 train acc: 0.9520833492279053 test loss: 1.3199236392974854 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1170 train loss: 1.0910415649414062 train acc: 0.9520833492279053 test loss: 1.3110458850860596 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1171 train loss: 1.090798258781433 train acc: 0.9520833492279053 test loss: 1.341180443763733 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1172 train loss: 1.0911592245101929 train acc: 0.9520833492279053 test loss: 1.3310542106628418 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1173 train loss: 1.0908116102218628 train acc: 0.9520833492279053 test loss: 1.3127466440200806 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1174 train loss: 1.0910923480987549 train acc: 0.9520833492279053 test loss: 1.3326894044876099 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1175 train loss: 1.0913581848144531 train acc: 0.9520833492279053 test loss: 1.308932900428772 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1176 train loss: 1.0910767316818237 train acc: 0.9520833492279053 test loss: 1.3187123537063599 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1177 train loss: 1.0912182331085205 train acc: 0.9520833492279053 test loss: 1.3220704793930054 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1178 train loss: 1.090903401374817 train acc: 0.9520833492279053 test loss: 1.3401471376419067 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1179 train loss: 1.091763973236084 train acc: 0.9520833492279053 test loss: 1.3173344135284424 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1180 train loss: 1.091076135635376 train acc: 0.9520833492279053 test loss: 1.3220183849334717 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1181 train loss: 1.09084951877594 train acc: 0.9520833492279053 test loss: 1.3124983310699463 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1182 train loss: 1.0911682844161987 train acc: 0.9520833492279053 test loss: 1.3239327669143677 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1183 train loss: 1.0910089015960693 train acc: 0.9520833492279053 test loss: 1.3265039920806885 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1184 train loss: 1.0913878679275513 train acc: 0.9520833492279053 test loss: 1.3224378824234009 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1185 train loss: 1.0907543897628784 train acc: 0.9520833492279053 test loss: 1.3410032987594604 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1186 train loss: 1.0911659002304077 train acc: 0.9520833492279053 test loss: 1.3157801628112793 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1187 train loss: 1.090991735458374 train acc: 0.9520833492279053 test loss: 1.3382604122161865 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1188 train loss: 1.0911602973937988 train acc: 0.9520833492279053 test loss: 1.338901400566101 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1189 train loss: 1.0913727283477783 train acc: 0.9520833492279053 test loss: 1.331216812133789 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1190 train loss: 1.0910776853561401 train acc: 0.9520833492279053 test loss: 1.316715955734253 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1191 train loss: 1.091155767440796 train acc: 0.9520833492279053 test loss: 1.3224433660507202 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1192 train loss: 1.0909903049468994 train acc: 0.9520833492279053 test loss: 1.2985731363296509 best test loss: 1.2862135171890259 test acc: 0.7583333253860474\n",
      "Epoch 1193 train loss: 1.0915497541427612 train acc: 0.9520833492279053 test loss: 1.322346806526184 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1194 train loss: 1.0908894538879395 train acc: 0.9520833492279053 test loss: 1.3243299722671509 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1195 train loss: 1.0909173488616943 train acc: 0.9520833492279053 test loss: 1.3260644674301147 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1196 train loss: 1.0910694599151611 train acc: 0.9520833492279053 test loss: 1.3362644910812378 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1197 train loss: 1.0910470485687256 train acc: 0.9520833492279053 test loss: 1.3061686754226685 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1198 train loss: 1.0910314321517944 train acc: 0.9520833492279053 test loss: 1.3419380187988281 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1199 train loss: 1.0905667543411255 train acc: 0.9520833492279053 test loss: 1.327336072921753 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1200 train loss: 1.0910688638687134 train acc: 0.9520833492279053 test loss: 1.3285959959030151 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1201 train loss: 1.0911993980407715 train acc: 0.9520833492279053 test loss: 1.319743275642395 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1202 train loss: 1.0910896062850952 train acc: 0.9520833492279053 test loss: 1.3183953762054443 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1203 train loss: 1.0908946990966797 train acc: 0.9520833492279053 test loss: 1.3119990825653076 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1204 train loss: 1.0910143852233887 train acc: 0.9520833492279053 test loss: 1.3317567110061646 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1205 train loss: 1.0909571647644043 train acc: 0.9520833492279053 test loss: 1.323352336883545 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1206 train loss: 1.090998888015747 train acc: 0.9520833492279053 test loss: 1.3200466632843018 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1207 train loss: 1.0912491083145142 train acc: 0.9520833492279053 test loss: 1.3237323760986328 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1208 train loss: 1.0910710096359253 train acc: 0.9520833492279053 test loss: 1.3363707065582275 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1209 train loss: 1.0909010171890259 train acc: 0.9520833492279053 test loss: 1.313122272491455 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1210 train loss: 1.091814398765564 train acc: 0.9520833492279053 test loss: 1.3287047147750854 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1211 train loss: 1.090915322303772 train acc: 0.9520833492279053 test loss: 1.3216938972473145 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1212 train loss: 1.0910810232162476 train acc: 0.9520833492279053 test loss: 1.3144993782043457 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1213 train loss: 1.0908429622650146 train acc: 0.9520833492279053 test loss: 1.3301500082015991 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1214 train loss: 1.0910171270370483 train acc: 0.9520833492279053 test loss: 1.3347508907318115 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1215 train loss: 1.0917680263519287 train acc: 0.9520833492279053 test loss: 1.3405706882476807 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1216 train loss: 1.0911322832107544 train acc: 0.9520833492279053 test loss: 1.3464381694793701 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1217 train loss: 1.0912222862243652 train acc: 0.9520833492279053 test loss: 1.3129994869232178 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1218 train loss: 1.0910494327545166 train acc: 0.9520833492279053 test loss: 1.310739278793335 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1219 train loss: 1.0909807682037354 train acc: 0.9520833492279053 test loss: 1.3079657554626465 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1220 train loss: 1.091448426246643 train acc: 0.9520833492279053 test loss: 1.3161171674728394 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1221 train loss: 1.0909764766693115 train acc: 0.9520833492279053 test loss: 1.3342437744140625 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1222 train loss: 1.0909863710403442 train acc: 0.9520833492279053 test loss: 1.3196330070495605 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1223 train loss: 1.0913805961608887 train acc: 0.9520833492279053 test loss: 1.325069785118103 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1224 train loss: 1.0909042358398438 train acc: 0.9520833492279053 test loss: 1.3307100534439087 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1225 train loss: 1.0911625623703003 train acc: 0.9520833492279053 test loss: 1.3295738697052002 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1226 train loss: 1.0909253358840942 train acc: 0.9520833492279053 test loss: 1.3197863101959229 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1227 train loss: 1.0910401344299316 train acc: 0.9520833492279053 test loss: 1.317691683769226 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1228 train loss: 1.0914103984832764 train acc: 0.9520833492279053 test loss: 1.3373229503631592 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1229 train loss: 1.0910512208938599 train acc: 0.9520833492279053 test loss: 1.3106681108474731 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1230 train loss: 1.090988278388977 train acc: 0.9520833492279053 test loss: 1.3143781423568726 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1231 train loss: 1.0910204648971558 train acc: 0.9520833492279053 test loss: 1.3168203830718994 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1232 train loss: 1.0909454822540283 train acc: 0.9520833492279053 test loss: 1.336147665977478 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1233 train loss: 1.091156244277954 train acc: 0.9520833492279053 test loss: 1.319976806640625 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1234 train loss: 1.0915285348892212 train acc: 0.9520833492279053 test loss: 1.3163105249404907 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1235 train loss: 1.0910130739212036 train acc: 0.9520833492279053 test loss: 1.3286725282669067 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1236 train loss: 1.0909855365753174 train acc: 0.9520833492279053 test loss: 1.3344075679779053 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1237 train loss: 1.0909600257873535 train acc: 0.9520833492279053 test loss: 1.3344037532806396 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1238 train loss: 1.0910977125167847 train acc: 0.9520833492279053 test loss: 1.3301442861557007 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1239 train loss: 1.0910471677780151 train acc: 0.9520833492279053 test loss: 1.3329235315322876 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1240 train loss: 1.0910303592681885 train acc: 0.9520833492279053 test loss: 1.3088018894195557 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1241 train loss: 1.0909394025802612 train acc: 0.9520833492279053 test loss: 1.342990517616272 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1242 train loss: 1.0910364389419556 train acc: 0.9520833492279053 test loss: 1.325516939163208 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1243 train loss: 1.0911052227020264 train acc: 0.9520833492279053 test loss: 1.3214633464813232 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1244 train loss: 1.0913602113723755 train acc: 0.9520833492279053 test loss: 1.3290907144546509 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1245 train loss: 1.0907291173934937 train acc: 0.9520833492279053 test loss: 1.3194539546966553 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1246 train loss: 1.0910919904708862 train acc: 0.9520833492279053 test loss: 1.3333828449249268 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1247 train loss: 1.091038465499878 train acc: 0.9520833492279053 test loss: 1.3349261283874512 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1248 train loss: 1.0907965898513794 train acc: 0.9520833492279053 test loss: 1.3145616054534912 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1249 train loss: 1.091362476348877 train acc: 0.9520833492279053 test loss: 1.3306009769439697 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1250 train loss: 1.0910913944244385 train acc: 0.9520833492279053 test loss: 1.3177540302276611 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1251 train loss: 1.0912376642227173 train acc: 0.9520833492279053 test loss: 1.318780541419983 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1252 train loss: 1.0910296440124512 train acc: 0.9520833492279053 test loss: 1.3279274702072144 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1253 train loss: 1.091137409210205 train acc: 0.9520833492279053 test loss: 1.3346351385116577 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1254 train loss: 1.0911333560943604 train acc: 0.9520833492279053 test loss: 1.3147811889648438 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1255 train loss: 1.091065526008606 train acc: 0.9520833492279053 test loss: 1.3242679834365845 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1256 train loss: 1.0909379720687866 train acc: 0.9520833492279053 test loss: 1.3272795677185059 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1257 train loss: 1.0910314321517944 train acc: 0.9520833492279053 test loss: 1.3509851694107056 best test loss: 1.2862135171890259 test acc: 0.6833333373069763\n",
      "Epoch 1258 train loss: 1.0910738706588745 train acc: 0.9520833492279053 test loss: 1.3273314237594604 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1259 train loss: 1.0910605192184448 train acc: 0.9520833492279053 test loss: 1.3246803283691406 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1260 train loss: 1.0909996032714844 train acc: 0.9520833492279053 test loss: 1.3339099884033203 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1261 train loss: 1.090962529182434 train acc: 0.9520833492279053 test loss: 1.334160327911377 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1262 train loss: 1.0911154747009277 train acc: 0.9520833492279053 test loss: 1.3227996826171875 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1263 train loss: 1.090952754020691 train acc: 0.9520833492279053 test loss: 1.3336970806121826 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1264 train loss: 1.0908368825912476 train acc: 0.9520833492279053 test loss: 1.3171560764312744 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1265 train loss: 1.0909823179244995 train acc: 0.9520833492279053 test loss: 1.3380624055862427 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1266 train loss: 1.0909016132354736 train acc: 0.9520833492279053 test loss: 1.3365105390548706 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1267 train loss: 1.0913310050964355 train acc: 0.9520833492279053 test loss: 1.3272162675857544 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1268 train loss: 1.0908669233322144 train acc: 0.9520833492279053 test loss: 1.3355573415756226 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1269 train loss: 1.0910547971725464 train acc: 0.9520833492279053 test loss: 1.3193682432174683 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1270 train loss: 1.0911924839019775 train acc: 0.9520833492279053 test loss: 1.3383997678756714 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1271 train loss: 1.0912758111953735 train acc: 0.9520833492279053 test loss: 1.296735405921936 best test loss: 1.2862135171890259 test acc: 0.7583333253860474\n",
      "Epoch 1272 train loss: 1.0908584594726562 train acc: 0.9520833492279053 test loss: 1.326164960861206 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1273 train loss: 1.0911040306091309 train acc: 0.9520833492279053 test loss: 1.3438167572021484 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1274 train loss: 1.0910223722457886 train acc: 0.9520833492279053 test loss: 1.3233433961868286 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1275 train loss: 1.0909188985824585 train acc: 0.9520833492279053 test loss: 1.329969048500061 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1276 train loss: 1.0910475254058838 train acc: 0.9520833492279053 test loss: 1.3185198307037354 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1277 train loss: 1.0910543203353882 train acc: 0.9520833492279053 test loss: 1.3098231554031372 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1278 train loss: 1.090954303741455 train acc: 0.9520833492279053 test loss: 1.3222768306732178 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1279 train loss: 1.090986967086792 train acc: 0.9520833492279053 test loss: 1.2990249395370483 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1280 train loss: 1.090824007987976 train acc: 0.9520833492279053 test loss: 1.3004136085510254 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1281 train loss: 1.0910944938659668 train acc: 0.9520833492279053 test loss: 1.3417361974716187 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1282 train loss: 1.0909894704818726 train acc: 0.9520833492279053 test loss: 1.3202403783798218 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1283 train loss: 1.090833067893982 train acc: 0.9520833492279053 test loss: 1.325774908065796 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1284 train loss: 1.0912973880767822 train acc: 0.9520833492279053 test loss: 1.3291109800338745 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1285 train loss: 1.091166377067566 train acc: 0.9520833492279053 test loss: 1.3380367755889893 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1286 train loss: 1.0907866954803467 train acc: 0.9520833492279053 test loss: 1.3220820426940918 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1287 train loss: 1.0909384489059448 train acc: 0.9520833492279053 test loss: 1.3444072008132935 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1288 train loss: 1.0910600423812866 train acc: 0.9520833492279053 test loss: 1.3234213590621948 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1289 train loss: 1.0908339023590088 train acc: 0.9520833492279053 test loss: 1.3243663311004639 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1290 train loss: 1.091098427772522 train acc: 0.9520833492279053 test loss: 1.3296903371810913 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1291 train loss: 1.0910072326660156 train acc: 0.9520833492279053 test loss: 1.317898154258728 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1292 train loss: 1.0909647941589355 train acc: 0.9520833492279053 test loss: 1.3586218357086182 best test loss: 1.2862135171890259 test acc: 0.6833333373069763\n",
      "Epoch 1293 train loss: 1.091013789176941 train acc: 0.9520833492279053 test loss: 1.3326799869537354 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1294 train loss: 1.0912423133850098 train acc: 0.9520833492279053 test loss: 1.3275463581085205 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1295 train loss: 1.0907952785491943 train acc: 0.9520833492279053 test loss: 1.3304588794708252 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1296 train loss: 1.091103434562683 train acc: 0.9520833492279053 test loss: 1.3182309865951538 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1297 train loss: 1.091064691543579 train acc: 0.9520833492279053 test loss: 1.3270199298858643 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1298 train loss: 1.0910483598709106 train acc: 0.9520833492279053 test loss: 1.324563980102539 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1299 train loss: 1.0907093286514282 train acc: 0.9520833492279053 test loss: 1.3325313329696655 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1300 train loss: 1.091150164604187 train acc: 0.9520833492279053 test loss: 1.3566614389419556 best test loss: 1.2862135171890259 test acc: 0.6833333373069763\n",
      "Epoch 1301 train loss: 1.090914011001587 train acc: 0.9520833492279053 test loss: 1.3056252002716064 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1302 train loss: 1.091163158416748 train acc: 0.9520833492279053 test loss: 1.3431336879730225 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1303 train loss: 1.0912867784500122 train acc: 0.9520833492279053 test loss: 1.3210664987564087 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1304 train loss: 1.0909725427627563 train acc: 0.9520833492279053 test loss: 1.3455352783203125 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1305 train loss: 1.0911678075790405 train acc: 0.9520833492279053 test loss: 1.316373348236084 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1306 train loss: 1.0912463665008545 train acc: 0.9520833492279053 test loss: 1.3252981901168823 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1307 train loss: 1.0912474393844604 train acc: 0.9520833492279053 test loss: 1.3431713581085205 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1308 train loss: 1.0911295413970947 train acc: 0.9520833492279053 test loss: 1.300755262374878 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1309 train loss: 1.0911295413970947 train acc: 0.9520833492279053 test loss: 1.2993649244308472 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1310 train loss: 1.0909148454666138 train acc: 0.9520833492279053 test loss: 1.344068169593811 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1311 train loss: 1.0909531116485596 train acc: 0.9520833492279053 test loss: 1.3318424224853516 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1312 train loss: 1.09109365940094 train acc: 0.9520833492279053 test loss: 1.3137106895446777 best test loss: 1.2862135171890259 test acc: 0.75\n",
      "Epoch 1313 train loss: 1.0910754203796387 train acc: 0.9520833492279053 test loss: 1.331390619277954 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1314 train loss: 1.0909425020217896 train acc: 0.9520833492279053 test loss: 1.3232300281524658 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1315 train loss: 1.090988039970398 train acc: 0.9520833492279053 test loss: 1.336940050125122 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1316 train loss: 1.0910992622375488 train acc: 0.9520833492279053 test loss: 1.321445107460022 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1317 train loss: 1.091179370880127 train acc: 0.9520833492279053 test loss: 1.3375924825668335 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1318 train loss: 1.090904951095581 train acc: 0.9520833492279053 test loss: 1.3216427564620972 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1319 train loss: 1.0908869504928589 train acc: 0.9520833492279053 test loss: 1.321776032447815 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1320 train loss: 1.0910202264785767 train acc: 0.9520833492279053 test loss: 1.3253839015960693 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1321 train loss: 1.091302752494812 train acc: 0.9520833492279053 test loss: 1.3462059497833252 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1322 train loss: 1.090889573097229 train acc: 0.9520833492279053 test loss: 1.311991572380066 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1323 train loss: 1.0911651849746704 train acc: 0.9520833492279053 test loss: 1.332069993019104 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1324 train loss: 1.0911563634872437 train acc: 0.9520833492279053 test loss: 1.341532826423645 best test loss: 1.2862135171890259 test acc: 0.6916666626930237\n",
      "Epoch 1325 train loss: 1.0908539295196533 train acc: 0.9520833492279053 test loss: 1.3179248571395874 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1326 train loss: 1.0910297632217407 train acc: 0.9520833492279053 test loss: 1.327440619468689 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1327 train loss: 1.091140627861023 train acc: 0.9520833492279053 test loss: 1.3263143301010132 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1328 train loss: 1.0908522605895996 train acc: 0.9520833492279053 test loss: 1.3245288133621216 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1329 train loss: 1.0909837484359741 train acc: 0.9520833492279053 test loss: 1.318255066871643 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1330 train loss: 1.0910602807998657 train acc: 0.9520833492279053 test loss: 1.3086763620376587 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1331 train loss: 1.0911625623703003 train acc: 0.9520833492279053 test loss: 1.3118244409561157 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1332 train loss: 1.0909310579299927 train acc: 0.9520833492279053 test loss: 1.3220703601837158 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1333 train loss: 1.090839147567749 train acc: 0.9520833492279053 test loss: 1.3314546346664429 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1334 train loss: 1.0909756422042847 train acc: 0.9520833492279053 test loss: 1.3421560525894165 best test loss: 1.2862135171890259 test acc: 0.699999988079071\n",
      "Epoch 1335 train loss: 1.091156244277954 train acc: 0.9520833492279053 test loss: 1.3214695453643799 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1336 train loss: 1.091186285018921 train acc: 0.9520833492279053 test loss: 1.3217042684555054 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1337 train loss: 1.0910706520080566 train acc: 0.9520833492279053 test loss: 1.3117910623550415 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1338 train loss: 1.0909836292266846 train acc: 0.9520833492279053 test loss: 1.3335762023925781 best test loss: 1.2862135171890259 test acc: 0.7083333134651184\n",
      "Epoch 1339 train loss: 1.0911544561386108 train acc: 0.9520833492279053 test loss: 1.328930139541626 best test loss: 1.2862135171890259 test acc: 0.7166666388511658\n",
      "Epoch 1340 train loss: 1.090881586074829 train acc: 0.9520833492279053 test loss: 1.313812494277954 best test loss: 1.2862135171890259 test acc: 0.7333333492279053\n",
      "Epoch 1341 train loss: 1.0910276174545288 train acc: 0.9520833492279053 test loss: 1.3173950910568237 best test loss: 1.2862135171890259 test acc: 0.7250000238418579\n",
      "Epoch 1342 train loss: 1.0911142826080322 train acc: 0.9520833492279053 test loss: 1.3048819303512573 best test loss: 1.2862135171890259 test acc: 0.7416666746139526\n",
      "Epoch 1343 train loss: 1.0913221836090088 train acc: 0.9520833492279053 test loss: 1.283445954322815 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 1344 train loss: 1.090866208076477 train acc: 0.9520833492279053 test loss: 1.3327151536941528 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1345 train loss: 1.0910497903823853 train acc: 0.9520833492279053 test loss: 1.3185209035873413 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1346 train loss: 1.0909216403961182 train acc: 0.9520833492279053 test loss: 1.3427398204803467 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1347 train loss: 1.0913406610488892 train acc: 0.9520833492279053 test loss: 1.3324006795883179 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1348 train loss: 1.0910677909851074 train acc: 0.9520833492279053 test loss: 1.3249105215072632 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1349 train loss: 1.0907608270645142 train acc: 0.9520833492279053 test loss: 1.3462767601013184 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1350 train loss: 1.091130018234253 train acc: 0.9520833492279053 test loss: 1.300165057182312 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1351 train loss: 1.0912514925003052 train acc: 0.9520833492279053 test loss: 1.3444498777389526 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1352 train loss: 1.0906922817230225 train acc: 0.9520833492279053 test loss: 1.3223762512207031 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1353 train loss: 1.0912216901779175 train acc: 0.9520833492279053 test loss: 1.3173662424087524 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1354 train loss: 1.0907129049301147 train acc: 0.9520833492279053 test loss: 1.3335928916931152 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1355 train loss: 1.0910239219665527 train acc: 0.9520833492279053 test loss: 1.3011492490768433 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1356 train loss: 1.0910451412200928 train acc: 0.9520833492279053 test loss: 1.330901026725769 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1357 train loss: 1.0910812616348267 train acc: 0.9520833492279053 test loss: 1.33987557888031 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1358 train loss: 1.0912058353424072 train acc: 0.9520833492279053 test loss: 1.3183804750442505 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1359 train loss: 1.0917452573776245 train acc: 0.949999988079071 test loss: 1.3039005994796753 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1360 train loss: 1.0910452604293823 train acc: 0.9520833492279053 test loss: 1.3367037773132324 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1361 train loss: 1.090972661972046 train acc: 0.9520833492279053 test loss: 1.3355084657669067 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1362 train loss: 1.0909804105758667 train acc: 0.9520833492279053 test loss: 1.3163833618164062 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1363 train loss: 1.0911102294921875 train acc: 0.9520833492279053 test loss: 1.3402961492538452 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1364 train loss: 1.0908290147781372 train acc: 0.9520833492279053 test loss: 1.335713267326355 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1365 train loss: 1.0907996892929077 train acc: 0.9520833492279053 test loss: 1.3288840055465698 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1366 train loss: 1.0916882753372192 train acc: 0.9520833492279053 test loss: 1.3581187725067139 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 1367 train loss: 1.0910580158233643 train acc: 0.9520833492279053 test loss: 1.3138880729675293 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1368 train loss: 1.091023564338684 train acc: 0.9520833492279053 test loss: 1.3179413080215454 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1369 train loss: 1.0911768674850464 train acc: 0.9520833492279053 test loss: 1.3276371955871582 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1370 train loss: 1.0909894704818726 train acc: 0.9520833492279053 test loss: 1.3204468488693237 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1371 train loss: 1.0918269157409668 train acc: 0.9520833492279053 test loss: 1.3464443683624268 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1372 train loss: 1.0910499095916748 train acc: 0.9520833492279053 test loss: 1.3443233966827393 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1373 train loss: 1.0909680128097534 train acc: 0.9520833492279053 test loss: 1.3268316984176636 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1374 train loss: 1.0908488035202026 train acc: 0.9520833492279053 test loss: 1.3386789560317993 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1375 train loss: 1.091048002243042 train acc: 0.9520833492279053 test loss: 1.3255277872085571 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1376 train loss: 1.0909894704818726 train acc: 0.9520833492279053 test loss: 1.3175625801086426 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1377 train loss: 1.0908160209655762 train acc: 0.9520833492279053 test loss: 1.3181508779525757 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1378 train loss: 1.0910038948059082 train acc: 0.9520833492279053 test loss: 1.3294206857681274 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1379 train loss: 1.0909812450408936 train acc: 0.9520833492279053 test loss: 1.3323346376419067 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1380 train loss: 1.0909749269485474 train acc: 0.9520833492279053 test loss: 1.3452880382537842 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1381 train loss: 1.0910660028457642 train acc: 0.9520833492279053 test loss: 1.3169077634811401 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1382 train loss: 1.0909720659255981 train acc: 0.9520833492279053 test loss: 1.3308459520339966 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1383 train loss: 1.090932011604309 train acc: 0.9520833492279053 test loss: 1.3305615186691284 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1384 train loss: 1.0911097526550293 train acc: 0.9520833492279053 test loss: 1.3058853149414062 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1385 train loss: 1.0909876823425293 train acc: 0.9520833492279053 test loss: 1.3177871704101562 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1386 train loss: 1.0910499095916748 train acc: 0.9520833492279053 test loss: 1.330215334892273 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1387 train loss: 1.0909361839294434 train acc: 0.9520833492279053 test loss: 1.3283697366714478 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1388 train loss: 1.0910098552703857 train acc: 0.9520833492279053 test loss: 1.3337570428848267 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1389 train loss: 1.0910972356796265 train acc: 0.9520833492279053 test loss: 1.3416064977645874 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 1390 train loss: 1.091011881828308 train acc: 0.9520833492279053 test loss: 1.3370593786239624 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1391 train loss: 1.0911329984664917 train acc: 0.9520833492279053 test loss: 1.3069801330566406 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1392 train loss: 1.0910370349884033 train acc: 0.9520833492279053 test loss: 1.3161295652389526 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1393 train loss: 1.0909883975982666 train acc: 0.9520833492279053 test loss: 1.3340187072753906 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1394 train loss: 1.0904958248138428 train acc: 0.9520833492279053 test loss: 1.3402620553970337 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 1395 train loss: 1.090914011001587 train acc: 0.9520833492279053 test loss: 1.3225462436676025 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1396 train loss: 1.0912268161773682 train acc: 0.9520833492279053 test loss: 1.3267043828964233 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1397 train loss: 1.090965986251831 train acc: 0.9520833492279053 test loss: 1.3266292810440063 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1398 train loss: 1.0912836790084839 train acc: 0.9520833492279053 test loss: 1.333746075630188 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1399 train loss: 1.091115951538086 train acc: 0.9520833492279053 test loss: 1.3261438608169556 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1400 train loss: 1.091094970703125 train acc: 0.9520833492279053 test loss: 1.321146011352539 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1401 train loss: 1.0910861492156982 train acc: 0.9520833492279053 test loss: 1.315976858139038 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1402 train loss: 1.0911165475845337 train acc: 0.9520833492279053 test loss: 1.326309323310852 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1403 train loss: 1.0909558534622192 train acc: 0.9520833492279053 test loss: 1.337506651878357 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1404 train loss: 1.090915322303772 train acc: 0.9520833492279053 test loss: 1.3356367349624634 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1405 train loss: 1.0912814140319824 train acc: 0.9520833492279053 test loss: 1.3306471109390259 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1406 train loss: 1.090912103652954 train acc: 0.9520833492279053 test loss: 1.3189899921417236 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1407 train loss: 1.0895187854766846 train acc: 0.9541666507720947 test loss: 1.3333642482757568 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1408 train loss: 1.0911592245101929 train acc: 0.9520833492279053 test loss: 1.3372271060943604 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1409 train loss: 1.0904994010925293 train acc: 0.9520833492279053 test loss: 1.3235344886779785 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1410 train loss: 1.0908876657485962 train acc: 0.9520833492279053 test loss: 1.308914303779602 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1411 train loss: 1.0911805629730225 train acc: 0.9520833492279053 test loss: 1.3111332654953003 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1412 train loss: 1.0900224447250366 train acc: 0.9541666507720947 test loss: 1.3252201080322266 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1413 train loss: 1.090537428855896 train acc: 0.9520833492279053 test loss: 1.3164056539535522 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1414 train loss: 1.0896594524383545 train acc: 0.9541666507720947 test loss: 1.2916356325149536 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1415 train loss: 1.0890580415725708 train acc: 0.9541666507720947 test loss: 1.3116695880889893 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1416 train loss: 1.0890552997589111 train acc: 0.9541666507720947 test loss: 1.32633376121521 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1417 train loss: 1.089047908782959 train acc: 0.9541666507720947 test loss: 1.3089427947998047 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1418 train loss: 1.0888670682907104 train acc: 0.9541666507720947 test loss: 1.3038928508758545 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1419 train loss: 1.088958740234375 train acc: 0.9541666507720947 test loss: 1.2963757514953613 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1420 train loss: 1.0897901058197021 train acc: 0.9541666507720947 test loss: 1.3359614610671997 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1421 train loss: 1.0891749858856201 train acc: 0.9541666507720947 test loss: 1.3045920133590698 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1422 train loss: 1.0890898704528809 train acc: 0.9541666507720947 test loss: 1.3269875049591064 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1423 train loss: 1.0891293287277222 train acc: 0.9541666507720947 test loss: 1.3089096546173096 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1424 train loss: 1.0892709493637085 train acc: 0.9541666507720947 test loss: 1.3137303590774536 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1425 train loss: 1.0889619588851929 train acc: 0.9541666507720947 test loss: 1.3061480522155762 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1426 train loss: 1.0889239311218262 train acc: 0.9541666507720947 test loss: 1.324809193611145 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1427 train loss: 1.089025616645813 train acc: 0.9541666507720947 test loss: 1.2969393730163574 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1428 train loss: 1.0888900756835938 train acc: 0.9541666507720947 test loss: 1.3370686769485474 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 1429 train loss: 1.088794231414795 train acc: 0.9541666507720947 test loss: 1.3204985857009888 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1430 train loss: 1.0887178182601929 train acc: 0.9541666507720947 test loss: 1.3242876529693604 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1431 train loss: 1.0890053510665894 train acc: 0.9541666507720947 test loss: 1.3300201892852783 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1432 train loss: 1.0888134241104126 train acc: 0.9541666507720947 test loss: 1.3140068054199219 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1433 train loss: 1.0890995264053345 train acc: 0.9541666507720947 test loss: 1.3142343759536743 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1434 train loss: 1.08892822265625 train acc: 0.9541666507720947 test loss: 1.3067262172698975 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1435 train loss: 1.0890079736709595 train acc: 0.9541666507720947 test loss: 1.3000048398971558 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1436 train loss: 1.08916175365448 train acc: 0.9541666507720947 test loss: 1.3022887706756592 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1437 train loss: 1.0888081789016724 train acc: 0.9541666507720947 test loss: 1.3279571533203125 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1438 train loss: 1.0891262292861938 train acc: 0.9541666507720947 test loss: 1.3046547174453735 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1439 train loss: 1.0889240503311157 train acc: 0.9541666507720947 test loss: 1.3058876991271973 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1440 train loss: 1.0890716314315796 train acc: 0.9541666507720947 test loss: 1.3193435668945312 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1441 train loss: 1.088996410369873 train acc: 0.9541666507720947 test loss: 1.2995344400405884 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1442 train loss: 1.08890700340271 train acc: 0.9541666507720947 test loss: 1.3051546812057495 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1443 train loss: 1.088772177696228 train acc: 0.9541666507720947 test loss: 1.319163203239441 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1444 train loss: 1.0891481637954712 train acc: 0.9541666507720947 test loss: 1.3041926622390747 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1445 train loss: 1.089060664176941 train acc: 0.9541666507720947 test loss: 1.3073687553405762 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1446 train loss: 1.0889159440994263 train acc: 0.9541666507720947 test loss: 1.3155039548873901 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1447 train loss: 1.0889753103256226 train acc: 0.9541666507720947 test loss: 1.327379584312439 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1448 train loss: 1.08868408203125 train acc: 0.9541666507720947 test loss: 1.2975366115570068 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1449 train loss: 1.0891013145446777 train acc: 0.9541666507720947 test loss: 1.3269246816635132 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1450 train loss: 1.0890265703201294 train acc: 0.9541666507720947 test loss: 1.31551992893219 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1451 train loss: 1.0887922048568726 train acc: 0.9541666507720947 test loss: 1.3177218437194824 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1452 train loss: 1.088818073272705 train acc: 0.9541666507720947 test loss: 1.3198655843734741 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1453 train loss: 1.0888665914535522 train acc: 0.9541666507720947 test loss: 1.2938498258590698 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1454 train loss: 1.0890568494796753 train acc: 0.9541666507720947 test loss: 1.319027066230774 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1455 train loss: 1.089045763015747 train acc: 0.9541666507720947 test loss: 1.3263722658157349 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1456 train loss: 1.0891668796539307 train acc: 0.9541666507720947 test loss: 1.3267585039138794 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1457 train loss: 1.0890977382659912 train acc: 0.9541666507720947 test loss: 1.306870698928833 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1458 train loss: 1.088539719581604 train acc: 0.9541666507720947 test loss: 1.3172391653060913 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1459 train loss: 1.088879108428955 train acc: 0.9541666507720947 test loss: 1.2914042472839355 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1460 train loss: 1.0894461870193481 train acc: 0.9541666507720947 test loss: 1.307030439376831 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1461 train loss: 1.0889190435409546 train acc: 0.9541666507720947 test loss: 1.3239716291427612 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1462 train loss: 1.0889928340911865 train acc: 0.9541666507720947 test loss: 1.312875509262085 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1463 train loss: 1.088731050491333 train acc: 0.9541666507720947 test loss: 1.3197662830352783 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1464 train loss: 1.0887291431427002 train acc: 0.9541666507720947 test loss: 1.3117104768753052 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1465 train loss: 1.0888270139694214 train acc: 0.9541666507720947 test loss: 1.3028842210769653 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1466 train loss: 1.0887010097503662 train acc: 0.9541666507720947 test loss: 1.301831841468811 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1467 train loss: 1.0889248847961426 train acc: 0.9541666507720947 test loss: 1.331599473953247 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1468 train loss: 1.0891146659851074 train acc: 0.9541666507720947 test loss: 1.3239120244979858 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1469 train loss: 1.0891474485397339 train acc: 0.9541666507720947 test loss: 1.3132203817367554 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1470 train loss: 1.088982343673706 train acc: 0.9541666507720947 test loss: 1.3133352994918823 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1471 train loss: 1.0889495611190796 train acc: 0.9541666507720947 test loss: 1.3079968690872192 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1472 train loss: 1.0889804363250732 train acc: 0.9541666507720947 test loss: 1.3082176446914673 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1473 train loss: 1.0888606309890747 train acc: 0.9541666507720947 test loss: 1.300484538078308 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1474 train loss: 1.0888442993164062 train acc: 0.9541666507720947 test loss: 1.3042182922363281 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1475 train loss: 1.0888344049453735 train acc: 0.9541666507720947 test loss: 1.30662202835083 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1476 train loss: 1.088977575302124 train acc: 0.9541666507720947 test loss: 1.3171895742416382 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1477 train loss: 1.0886696577072144 train acc: 0.9541666507720947 test loss: 1.314813256263733 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1478 train loss: 1.0890865325927734 train acc: 0.9541666507720947 test loss: 1.310799479484558 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1479 train loss: 1.0888136625289917 train acc: 0.9541666507720947 test loss: 1.327907919883728 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1480 train loss: 1.0890119075775146 train acc: 0.9541666507720947 test loss: 1.3072153329849243 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1481 train loss: 1.0890214443206787 train acc: 0.9541666507720947 test loss: 1.3092108964920044 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1482 train loss: 1.0888444185256958 train acc: 0.9541666507720947 test loss: 1.3145259618759155 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1483 train loss: 1.0890634059906006 train acc: 0.9541666507720947 test loss: 1.3030375242233276 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1484 train loss: 1.0889850854873657 train acc: 0.9541666507720947 test loss: 1.3042794466018677 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1485 train loss: 1.0889967679977417 train acc: 0.9541666507720947 test loss: 1.300087809562683 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1486 train loss: 1.0887619256973267 train acc: 0.9541666507720947 test loss: 1.3026655912399292 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1487 train loss: 1.0888012647628784 train acc: 0.9541666507720947 test loss: 1.3063197135925293 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1488 train loss: 1.0889309644699097 train acc: 0.9541666507720947 test loss: 1.3168343305587769 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1489 train loss: 1.0889923572540283 train acc: 0.9541666507720947 test loss: 1.2977478504180908 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1490 train loss: 1.08909273147583 train acc: 0.9541666507720947 test loss: 1.3068102598190308 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1491 train loss: 1.0888664722442627 train acc: 0.9541666507720947 test loss: 1.3189365863800049 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1492 train loss: 1.0889071226119995 train acc: 0.9541666507720947 test loss: 1.3069193363189697 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1493 train loss: 1.0887854099273682 train acc: 0.9541666507720947 test loss: 1.3134119510650635 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1494 train loss: 1.0887051820755005 train acc: 0.9541666507720947 test loss: 1.3204137086868286 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1495 train loss: 1.0888499021530151 train acc: 0.9541666507720947 test loss: 1.3088691234588623 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1496 train loss: 1.0890763998031616 train acc: 0.9541666507720947 test loss: 1.3139526844024658 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1497 train loss: 1.0890945196151733 train acc: 0.9541666507720947 test loss: 1.3071651458740234 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1498 train loss: 1.089037299156189 train acc: 0.9541666507720947 test loss: 1.3106999397277832 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1499 train loss: 1.088851809501648 train acc: 0.9541666507720947 test loss: 1.3174878358840942 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1500 train loss: 1.0890341997146606 train acc: 0.9541666507720947 test loss: 1.315124750137329 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1501 train loss: 1.0889464616775513 train acc: 0.9541666507720947 test loss: 1.3061554431915283 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1502 train loss: 1.089114785194397 train acc: 0.9541666507720947 test loss: 1.3253569602966309 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1503 train loss: 1.0887402296066284 train acc: 0.9541666507720947 test loss: 1.3117948770523071 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1504 train loss: 1.0889092683792114 train acc: 0.9541666507720947 test loss: 1.30168879032135 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1505 train loss: 1.0887625217437744 train acc: 0.9541666507720947 test loss: 1.3185694217681885 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1506 train loss: 1.0890361070632935 train acc: 0.9541666507720947 test loss: 1.335903286933899 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1507 train loss: 1.0888108015060425 train acc: 0.9541666507720947 test loss: 1.3100193738937378 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1508 train loss: 1.0886969566345215 train acc: 0.9541666507720947 test loss: 1.3149389028549194 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1509 train loss: 1.088786005973816 train acc: 0.9541666507720947 test loss: 1.3220351934432983 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1510 train loss: 1.0888516902923584 train acc: 0.9541666507720947 test loss: 1.3131424188613892 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1511 train loss: 1.0888956785202026 train acc: 0.9541666507720947 test loss: 1.3227595090866089 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1512 train loss: 1.088997721672058 train acc: 0.9541666507720947 test loss: 1.3174432516098022 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1513 train loss: 1.088963270187378 train acc: 0.9541666507720947 test loss: 1.3278616666793823 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1514 train loss: 1.0886521339416504 train acc: 0.9541666507720947 test loss: 1.30787992477417 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1515 train loss: 1.0888909101486206 train acc: 0.9541666507720947 test loss: 1.310562252998352 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1516 train loss: 1.0886040925979614 train acc: 0.9541666507720947 test loss: 1.3268417119979858 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1517 train loss: 1.0888288021087646 train acc: 0.9541666507720947 test loss: 1.3116416931152344 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1518 train loss: 1.0887442827224731 train acc: 0.9541666507720947 test loss: 1.3248051404953003 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1519 train loss: 1.0889873504638672 train acc: 0.9541666507720947 test loss: 1.29884934425354 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 1520 train loss: 1.0888220071792603 train acc: 0.9541666507720947 test loss: 1.3169118165969849 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1521 train loss: 1.0890145301818848 train acc: 0.9541666507720947 test loss: 1.3041552305221558 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1522 train loss: 1.0904375314712524 train acc: 0.9520833492279053 test loss: 1.3226358890533447 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1523 train loss: 1.0888011455535889 train acc: 0.9541666507720947 test loss: 1.3172250986099243 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1524 train loss: 1.0889017581939697 train acc: 0.9541666507720947 test loss: 1.3349032402038574 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1525 train loss: 1.088830828666687 train acc: 0.9541666507720947 test loss: 1.3208107948303223 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1526 train loss: 1.0888936519622803 train acc: 0.9541666507720947 test loss: 1.311339020729065 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1527 train loss: 1.0890462398529053 train acc: 0.9541666507720947 test loss: 1.3106706142425537 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1528 train loss: 1.0889453887939453 train acc: 0.9541666507720947 test loss: 1.2979316711425781 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1529 train loss: 1.0888423919677734 train acc: 0.9541666507720947 test loss: 1.2996701002120972 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1530 train loss: 1.0887755155563354 train acc: 0.9541666507720947 test loss: 1.3112578392028809 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1531 train loss: 1.0888954401016235 train acc: 0.9541666507720947 test loss: 1.295820951461792 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1532 train loss: 1.0888749361038208 train acc: 0.9541666507720947 test loss: 1.3031901121139526 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1533 train loss: 1.0890558958053589 train acc: 0.9541666507720947 test loss: 1.3067129850387573 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1534 train loss: 1.0888208150863647 train acc: 0.9541666507720947 test loss: 1.2966631650924683 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1535 train loss: 1.0889593362808228 train acc: 0.9541666507720947 test loss: 1.3192559480667114 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1536 train loss: 1.0887789726257324 train acc: 0.9541666507720947 test loss: 1.3122525215148926 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1537 train loss: 1.0888174772262573 train acc: 0.9541666507720947 test loss: 1.3139146566390991 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1538 train loss: 1.0891709327697754 train acc: 0.9541666507720947 test loss: 1.2977938652038574 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1539 train loss: 1.0889358520507812 train acc: 0.9541666507720947 test loss: 1.3236416578292847 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1540 train loss: 1.0889919996261597 train acc: 0.9541666507720947 test loss: 1.3201255798339844 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1541 train loss: 1.0890235900878906 train acc: 0.9541666507720947 test loss: 1.3052585124969482 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1542 train loss: 1.0889251232147217 train acc: 0.9541666507720947 test loss: 1.32631516456604 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1543 train loss: 1.0891227722167969 train acc: 0.9541666507720947 test loss: 1.2969110012054443 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1544 train loss: 1.0891878604888916 train acc: 0.9541666507720947 test loss: 1.2935775518417358 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1545 train loss: 1.088788628578186 train acc: 0.9541666507720947 test loss: 1.310064673423767 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1546 train loss: 1.0889091491699219 train acc: 0.9541666507720947 test loss: 1.3133059740066528 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1547 train loss: 1.0889918804168701 train acc: 0.9541666507720947 test loss: 1.3105435371398926 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1548 train loss: 1.0888396501541138 train acc: 0.9541666507720947 test loss: 1.30184006690979 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1549 train loss: 1.0887972116470337 train acc: 0.9541666507720947 test loss: 1.3188767433166504 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1550 train loss: 1.0889146327972412 train acc: 0.9541666507720947 test loss: 1.3127801418304443 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1551 train loss: 1.08905029296875 train acc: 0.9541666507720947 test loss: 1.3179622888565063 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1552 train loss: 1.0887917280197144 train acc: 0.9541666507720947 test loss: 1.3136039972305298 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1553 train loss: 1.0887938737869263 train acc: 0.9541666507720947 test loss: 1.3327997922897339 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1554 train loss: 1.0887049436569214 train acc: 0.9541666507720947 test loss: 1.3158342838287354 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1555 train loss: 1.0888253450393677 train acc: 0.9541666507720947 test loss: 1.3070303201675415 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1556 train loss: 1.088828444480896 train acc: 0.9541666507720947 test loss: 1.312847375869751 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1557 train loss: 1.0888831615447998 train acc: 0.9541666507720947 test loss: 1.3203843832015991 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1558 train loss: 1.0889155864715576 train acc: 0.9541666507720947 test loss: 1.3218460083007812 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1559 train loss: 1.0886064767837524 train acc: 0.9541666507720947 test loss: 1.2950536012649536 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1560 train loss: 1.0887446403503418 train acc: 0.9541666507720947 test loss: 1.3251311779022217 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1561 train loss: 1.088744878768921 train acc: 0.9541666507720947 test loss: 1.3101155757904053 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1562 train loss: 1.088958501815796 train acc: 0.9541666507720947 test loss: 1.3078786134719849 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1563 train loss: 1.0888779163360596 train acc: 0.9541666507720947 test loss: 1.3241924047470093 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1564 train loss: 1.0888320207595825 train acc: 0.9541666507720947 test loss: 1.3036326169967651 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1565 train loss: 1.0890203714370728 train acc: 0.9541666507720947 test loss: 1.3125288486480713 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1566 train loss: 1.088910698890686 train acc: 0.9541666507720947 test loss: 1.3167717456817627 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1567 train loss: 1.0890774726867676 train acc: 0.9541666507720947 test loss: 1.302269697189331 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1568 train loss: 1.088912844657898 train acc: 0.9541666507720947 test loss: 1.328444004058838 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1569 train loss: 1.088884949684143 train acc: 0.9541666507720947 test loss: 1.3182182312011719 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1570 train loss: 1.0887254476547241 train acc: 0.9541666507720947 test loss: 1.3149176836013794 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1571 train loss: 1.0886777639389038 train acc: 0.9541666507720947 test loss: 1.3116681575775146 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1572 train loss: 1.0890341997146606 train acc: 0.9541666507720947 test loss: 1.317015528678894 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1573 train loss: 1.0889908075332642 train acc: 0.9541666507720947 test loss: 1.291448950767517 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1574 train loss: 1.0889066457748413 train acc: 0.9541666507720947 test loss: 1.3030060529708862 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1575 train loss: 1.088945984840393 train acc: 0.9541666507720947 test loss: 1.311403751373291 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1576 train loss: 1.089003324508667 train acc: 0.9541666507720947 test loss: 1.3111892938613892 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1577 train loss: 1.0888649225234985 train acc: 0.9541666507720947 test loss: 1.3172743320465088 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1578 train loss: 1.0888384580612183 train acc: 0.9541666507720947 test loss: 1.2952865362167358 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1579 train loss: 1.0889661312103271 train acc: 0.9541666507720947 test loss: 1.3363564014434814 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1580 train loss: 1.0888198614120483 train acc: 0.9541666507720947 test loss: 1.3126095533370972 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1581 train loss: 1.0889204740524292 train acc: 0.9541666507720947 test loss: 1.3044860363006592 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1582 train loss: 1.0890649557113647 train acc: 0.9541666507720947 test loss: 1.3046989440917969 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1583 train loss: 1.0889142751693726 train acc: 0.9541666507720947 test loss: 1.2929075956344604 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1584 train loss: 1.0887585878372192 train acc: 0.9541666507720947 test loss: 1.2964203357696533 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1585 train loss: 1.0889424085617065 train acc: 0.9541666507720947 test loss: 1.314488172531128 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1586 train loss: 1.0889396667480469 train acc: 0.9541666507720947 test loss: 1.3284705877304077 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1587 train loss: 1.0889017581939697 train acc: 0.9541666507720947 test loss: 1.3239938020706177 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1588 train loss: 1.0888437032699585 train acc: 0.9541666507720947 test loss: 1.3017972707748413 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1589 train loss: 1.0889365673065186 train acc: 0.9541666507720947 test loss: 1.3263986110687256 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1590 train loss: 1.088873267173767 train acc: 0.9541666507720947 test loss: 1.3101996183395386 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1591 train loss: 1.0889796018600464 train acc: 0.9541666507720947 test loss: 1.3509422540664673 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 1592 train loss: 1.0886625051498413 train acc: 0.9541666507720947 test loss: 1.3102116584777832 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1593 train loss: 1.088842749595642 train acc: 0.9541666507720947 test loss: 1.3011459112167358 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1594 train loss: 1.0887458324432373 train acc: 0.9541666507720947 test loss: 1.307753324508667 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1595 train loss: 1.0889545679092407 train acc: 0.9541666507720947 test loss: 1.308970332145691 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1596 train loss: 1.088926076889038 train acc: 0.9541666507720947 test loss: 1.3015414476394653 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1597 train loss: 1.0887380838394165 train acc: 0.9541666507720947 test loss: 1.2975308895111084 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1598 train loss: 1.0887912511825562 train acc: 0.9541666507720947 test loss: 1.3095194101333618 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1599 train loss: 1.0889301300048828 train acc: 0.9541666507720947 test loss: 1.2924302816390991 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 1600 train loss: 1.088977336883545 train acc: 0.9541666507720947 test loss: 1.3253121376037598 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1601 train loss: 1.0890392065048218 train acc: 0.9541666507720947 test loss: 1.3021823167800903 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1602 train loss: 1.088494062423706 train acc: 0.9541666507720947 test loss: 1.303895354270935 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1603 train loss: 1.0890991687774658 train acc: 0.9541666507720947 test loss: 1.3031309843063354 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1604 train loss: 1.0890400409698486 train acc: 0.9541666507720947 test loss: 1.303198218345642 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1605 train loss: 1.0887326002120972 train acc: 0.9541666507720947 test loss: 1.312565565109253 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1606 train loss: 1.0889006853103638 train acc: 0.9541666507720947 test loss: 1.3034751415252686 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1607 train loss: 1.088976263999939 train acc: 0.9541666507720947 test loss: 1.294482707977295 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1608 train loss: 1.0889410972595215 train acc: 0.9541666507720947 test loss: 1.3065992593765259 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1609 train loss: 1.0889742374420166 train acc: 0.9541666507720947 test loss: 1.3043688535690308 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1610 train loss: 1.0889136791229248 train acc: 0.9541666507720947 test loss: 1.2875399589538574 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1611 train loss: 1.0889416933059692 train acc: 0.9541666507720947 test loss: 1.326319932937622 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1612 train loss: 1.088927149772644 train acc: 0.9541666507720947 test loss: 1.3253133296966553 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1613 train loss: 1.0898096561431885 train acc: 0.9541666507720947 test loss: 1.3166027069091797 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1614 train loss: 1.0888519287109375 train acc: 0.9541666507720947 test loss: 1.3238391876220703 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1615 train loss: 1.089681625366211 train acc: 0.9541666507720947 test loss: 1.3173073530197144 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1616 train loss: 1.0887633562088013 train acc: 0.9541666507720947 test loss: 1.3169522285461426 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1617 train loss: 1.088930368423462 train acc: 0.9541666507720947 test loss: 1.3098065853118896 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1618 train loss: 1.0888826847076416 train acc: 0.9541666507720947 test loss: 1.2997955083847046 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1619 train loss: 1.0889993906021118 train acc: 0.9541666507720947 test loss: 1.295835256576538 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1620 train loss: 1.089047908782959 train acc: 0.9541666507720947 test loss: 1.3152599334716797 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1621 train loss: 1.0889579057693481 train acc: 0.9541666507720947 test loss: 1.298640489578247 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1622 train loss: 1.0890276432037354 train acc: 0.9541666507720947 test loss: 1.3215203285217285 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1623 train loss: 1.089163899421692 train acc: 0.9541666507720947 test loss: 1.3346730470657349 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1624 train loss: 1.0890233516693115 train acc: 0.9541666507720947 test loss: 1.3230441808700562 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1625 train loss: 1.089028239250183 train acc: 0.9541666507720947 test loss: 1.3122159242630005 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1626 train loss: 1.0887677669525146 train acc: 0.9541666507720947 test loss: 1.318153738975525 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1627 train loss: 1.0891032218933105 train acc: 0.9541666507720947 test loss: 1.3244681358337402 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1628 train loss: 1.088868498802185 train acc: 0.9541666507720947 test loss: 1.3004449605941772 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1629 train loss: 1.0887587070465088 train acc: 0.9541666507720947 test loss: 1.3149487972259521 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1630 train loss: 1.0888553857803345 train acc: 0.9541666507720947 test loss: 1.3315675258636475 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1631 train loss: 1.0885318517684937 train acc: 0.9541666507720947 test loss: 1.323819637298584 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1632 train loss: 1.0888029336929321 train acc: 0.9541666507720947 test loss: 1.2959538698196411 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1633 train loss: 1.088866949081421 train acc: 0.9541666507720947 test loss: 1.334271788597107 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1634 train loss: 1.0890263319015503 train acc: 0.9541666507720947 test loss: 1.3088676929473877 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1635 train loss: 1.088884711265564 train acc: 0.9541666507720947 test loss: 1.3144992589950562 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1636 train loss: 1.088905692100525 train acc: 0.9541666507720947 test loss: 1.310874342918396 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1637 train loss: 1.0889666080474854 train acc: 0.9541666507720947 test loss: 1.325965404510498 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1638 train loss: 1.0889418125152588 train acc: 0.9541666507720947 test loss: 1.3094226121902466 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1639 train loss: 1.0890592336654663 train acc: 0.9541666507720947 test loss: 1.3229618072509766 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1640 train loss: 1.088757038116455 train acc: 0.9541666507720947 test loss: 1.3286559581756592 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1641 train loss: 1.0888890027999878 train acc: 0.9541666507720947 test loss: 1.314552664756775 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1642 train loss: 1.0889347791671753 train acc: 0.9541666507720947 test loss: 1.3295429944992065 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1643 train loss: 1.0888980627059937 train acc: 0.9541666507720947 test loss: 1.3355180025100708 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1644 train loss: 1.0888890027999878 train acc: 0.9541666507720947 test loss: 1.3130929470062256 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1645 train loss: 1.0888479948043823 train acc: 0.9541666507720947 test loss: 1.3118072748184204 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1646 train loss: 1.0888882875442505 train acc: 0.9541666507720947 test loss: 1.313591718673706 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1647 train loss: 1.0889142751693726 train acc: 0.9541666507720947 test loss: 1.3112075328826904 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1648 train loss: 1.0889217853546143 train acc: 0.9541666507720947 test loss: 1.309306025505066 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1649 train loss: 1.0887359380722046 train acc: 0.9541666507720947 test loss: 1.3207147121429443 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1650 train loss: 1.0889757871627808 train acc: 0.9541666507720947 test loss: 1.3085414171218872 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1651 train loss: 1.0890976190567017 train acc: 0.9541666507720947 test loss: 1.312147617340088 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1652 train loss: 1.088806390762329 train acc: 0.9541666507720947 test loss: 1.3140556812286377 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1653 train loss: 1.0889838933944702 train acc: 0.9541666507720947 test loss: 1.2930089235305786 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1654 train loss: 1.0888781547546387 train acc: 0.9541666507720947 test loss: 1.2989007234573364 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1655 train loss: 1.0888733863830566 train acc: 0.9541666507720947 test loss: 1.3269795179367065 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1656 train loss: 1.0887376070022583 train acc: 0.9541666507720947 test loss: 1.3235303163528442 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1657 train loss: 1.0889586210250854 train acc: 0.9541666507720947 test loss: 1.3260834217071533 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1658 train loss: 1.0889966487884521 train acc: 0.9541666507720947 test loss: 1.312317132949829 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1659 train loss: 1.088773250579834 train acc: 0.9541666507720947 test loss: 1.3263511657714844 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1660 train loss: 1.0889790058135986 train acc: 0.9541666507720947 test loss: 1.3103969097137451 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1661 train loss: 1.0889934301376343 train acc: 0.9541666507720947 test loss: 1.2972164154052734 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1662 train loss: 1.0888863801956177 train acc: 0.9541666507720947 test loss: 1.3091892004013062 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1663 train loss: 1.0889225006103516 train acc: 0.9541666507720947 test loss: 1.3319660425186157 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1664 train loss: 1.089900255203247 train acc: 0.9541666507720947 test loss: 1.2998343706130981 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1665 train loss: 1.0889943838119507 train acc: 0.9541666507720947 test loss: 1.3201359510421753 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1666 train loss: 1.0888335704803467 train acc: 0.9541666507720947 test loss: 1.3151155710220337 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1667 train loss: 1.0887353420257568 train acc: 0.9541666507720947 test loss: 1.3244659900665283 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1668 train loss: 1.0889989137649536 train acc: 0.9541666507720947 test loss: 1.328029751777649 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1669 train loss: 1.0887672901153564 train acc: 0.9541666507720947 test loss: 1.3048169612884521 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1670 train loss: 1.0889862775802612 train acc: 0.9541666507720947 test loss: 1.3082635402679443 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1671 train loss: 1.0889594554901123 train acc: 0.9541666507720947 test loss: 1.3341984748840332 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1672 train loss: 1.0888372659683228 train acc: 0.9541666507720947 test loss: 1.3151413202285767 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1673 train loss: 1.0888718366622925 train acc: 0.9541666507720947 test loss: 1.3315446376800537 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1674 train loss: 1.0889439582824707 train acc: 0.9541666507720947 test loss: 1.3057777881622314 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1675 train loss: 1.0885850191116333 train acc: 0.9541666507720947 test loss: 1.305213212966919 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1676 train loss: 1.0890134572982788 train acc: 0.9541666507720947 test loss: 1.335874080657959 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1677 train loss: 1.0888397693634033 train acc: 0.9541666507720947 test loss: 1.3074164390563965 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1678 train loss: 1.0888564586639404 train acc: 0.9541666507720947 test loss: 1.3126856088638306 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1679 train loss: 1.0888559818267822 train acc: 0.9541666507720947 test loss: 1.2979532480239868 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1680 train loss: 1.08872389793396 train acc: 0.9541666507720947 test loss: 1.3050720691680908 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1681 train loss: 1.088917851448059 train acc: 0.9541666507720947 test loss: 1.304709792137146 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1682 train loss: 1.0886865854263306 train acc: 0.9541666507720947 test loss: 1.3128430843353271 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1683 train loss: 1.0886108875274658 train acc: 0.9541666507720947 test loss: 1.3299161195755005 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1684 train loss: 1.0889509916305542 train acc: 0.9541666507720947 test loss: 1.3066827058792114 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1685 train loss: 1.0888862609863281 train acc: 0.9541666507720947 test loss: 1.3252071142196655 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1686 train loss: 1.0887482166290283 train acc: 0.9541666507720947 test loss: 1.3031666278839111 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1687 train loss: 1.0888550281524658 train acc: 0.9541666507720947 test loss: 1.3218892812728882 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1688 train loss: 1.088823676109314 train acc: 0.9541666507720947 test loss: 1.3176394701004028 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1689 train loss: 1.088790774345398 train acc: 0.9541666507720947 test loss: 1.3100370168685913 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1690 train loss: 1.0886820554733276 train acc: 0.9541666507720947 test loss: 1.3277137279510498 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1691 train loss: 1.0887812376022339 train acc: 0.9541666507720947 test loss: 1.3099024295806885 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1692 train loss: 1.0890077352523804 train acc: 0.9541666507720947 test loss: 1.3023861646652222 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1693 train loss: 1.08870530128479 train acc: 0.9541666507720947 test loss: 1.3200865983963013 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1694 train loss: 1.0886560678482056 train acc: 0.9541666507720947 test loss: 1.3154394626617432 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1695 train loss: 1.0887808799743652 train acc: 0.9541666507720947 test loss: 1.3387551307678223 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1696 train loss: 1.089048981666565 train acc: 0.9541666507720947 test loss: 1.3130435943603516 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1697 train loss: 1.088991403579712 train acc: 0.9541666507720947 test loss: 1.3398219347000122 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1698 train loss: 1.088849663734436 train acc: 0.9541666507720947 test loss: 1.3200846910476685 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1699 train loss: 1.088560700416565 train acc: 0.9541666507720947 test loss: 1.3269684314727783 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1700 train loss: 1.0887266397476196 train acc: 0.9541666507720947 test loss: 1.3342286348342896 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1701 train loss: 1.0889551639556885 train acc: 0.9541666507720947 test loss: 1.330348253250122 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1702 train loss: 1.0890753269195557 train acc: 0.9541666507720947 test loss: 1.3191016912460327 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1703 train loss: 1.0888437032699585 train acc: 0.9541666507720947 test loss: 1.317547082901001 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1704 train loss: 1.0887757539749146 train acc: 0.9541666507720947 test loss: 1.3255932331085205 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1705 train loss: 1.0890333652496338 train acc: 0.9541666507720947 test loss: 1.3284657001495361 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1706 train loss: 1.0888363122940063 train acc: 0.9541666507720947 test loss: 1.323974609375 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1707 train loss: 1.0890790224075317 train acc: 0.9541666507720947 test loss: 1.327768087387085 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1708 train loss: 1.0890005826950073 train acc: 0.9541666507720947 test loss: 1.3150355815887451 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1709 train loss: 1.0888049602508545 train acc: 0.9541666507720947 test loss: 1.3201980590820312 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1710 train loss: 1.089229702949524 train acc: 0.9541666507720947 test loss: 1.3180012702941895 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1711 train loss: 1.0890392065048218 train acc: 0.9541666507720947 test loss: 1.3315613269805908 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1712 train loss: 1.088860034942627 train acc: 0.9541666507720947 test loss: 1.3156421184539795 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1713 train loss: 1.0890376567840576 train acc: 0.9541666507720947 test loss: 1.3227843046188354 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1714 train loss: 1.0890971422195435 train acc: 0.9541666507720947 test loss: 1.308944582939148 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1715 train loss: 1.088815450668335 train acc: 0.9541666507720947 test loss: 1.3362137079238892 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1716 train loss: 1.089038610458374 train acc: 0.9541666507720947 test loss: 1.2905499935150146 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1717 train loss: 1.088788390159607 train acc: 0.9541666507720947 test loss: 1.3494702577590942 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1718 train loss: 1.0885815620422363 train acc: 0.9541666507720947 test loss: 1.3111463785171509 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1719 train loss: 1.0888131856918335 train acc: 0.9541666507720947 test loss: 1.3185501098632812 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1720 train loss: 1.0886753797531128 train acc: 0.9541666507720947 test loss: 1.321533441543579 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1721 train loss: 1.089585542678833 train acc: 0.9541666507720947 test loss: 1.308897614479065 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1722 train loss: 1.088847041130066 train acc: 0.9541666507720947 test loss: 1.3110936880111694 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1723 train loss: 1.0888324975967407 train acc: 0.9541666507720947 test loss: 1.3127362728118896 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1724 train loss: 1.089003086090088 train acc: 0.9541666507720947 test loss: 1.3132203817367554 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1725 train loss: 1.0887067317962646 train acc: 0.9541666507720947 test loss: 1.313363790512085 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1726 train loss: 1.0886977910995483 train acc: 0.9541666507720947 test loss: 1.29616117477417 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1727 train loss: 1.08903968334198 train acc: 0.9541666507720947 test loss: 1.3088403940200806 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1728 train loss: 1.0890406370162964 train acc: 0.9541666507720947 test loss: 1.308050513267517 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1729 train loss: 1.088824987411499 train acc: 0.9541666507720947 test loss: 1.3201817274093628 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1730 train loss: 1.0888731479644775 train acc: 0.9541666507720947 test loss: 1.3271769285202026 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1731 train loss: 1.0888805389404297 train acc: 0.9541666507720947 test loss: 1.3508981466293335 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 1732 train loss: 1.0891860723495483 train acc: 0.9541666507720947 test loss: 1.3198812007904053 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1733 train loss: 1.089110016822815 train acc: 0.9541666507720947 test loss: 1.3324555158615112 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1734 train loss: 1.0887954235076904 train acc: 0.9541666507720947 test loss: 1.3027161359786987 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1735 train loss: 1.088890790939331 train acc: 0.9541666507720947 test loss: 1.3067156076431274 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1736 train loss: 1.0888444185256958 train acc: 0.9541666507720947 test loss: 1.3031878471374512 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1737 train loss: 1.0889328718185425 train acc: 0.9541666507720947 test loss: 1.3269413709640503 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1738 train loss: 1.088862657546997 train acc: 0.9541666507720947 test loss: 1.3141497373580933 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1739 train loss: 1.0888214111328125 train acc: 0.9541666507720947 test loss: 1.3077926635742188 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1740 train loss: 1.0888938903808594 train acc: 0.9541666507720947 test loss: 1.3204803466796875 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1741 train loss: 1.0888454914093018 train acc: 0.9541666507720947 test loss: 1.3128783702850342 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1742 train loss: 1.0887001752853394 train acc: 0.9541666507720947 test loss: 1.3359116315841675 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1743 train loss: 1.0886868238449097 train acc: 0.9541666507720947 test loss: 1.3214746713638306 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1744 train loss: 1.089100956916809 train acc: 0.9541666507720947 test loss: 1.3254247903823853 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1745 train loss: 1.0887296199798584 train acc: 0.9541666507720947 test loss: 1.3092272281646729 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1746 train loss: 1.0888129472732544 train acc: 0.9541666507720947 test loss: 1.2985297441482544 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1747 train loss: 1.088835597038269 train acc: 0.9541666507720947 test loss: 1.3338204622268677 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1748 train loss: 1.0888363122940063 train acc: 0.9541666507720947 test loss: 1.3217003345489502 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1749 train loss: 1.0888408422470093 train acc: 0.9541666507720947 test loss: 1.2995779514312744 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1750 train loss: 1.0887229442596436 train acc: 0.9541666507720947 test loss: 1.3096258640289307 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1751 train loss: 1.088816523551941 train acc: 0.9541666507720947 test loss: 1.3206740617752075 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1752 train loss: 1.0887669324874878 train acc: 0.9541666507720947 test loss: 1.3184047937393188 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1753 train loss: 1.0888806581497192 train acc: 0.9541666507720947 test loss: 1.33035409450531 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1754 train loss: 1.0889467000961304 train acc: 0.9541666507720947 test loss: 1.3340734243392944 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1755 train loss: 1.0888993740081787 train acc: 0.9541666507720947 test loss: 1.3005229234695435 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1756 train loss: 1.0887874364852905 train acc: 0.9541666507720947 test loss: 1.3154023885726929 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1757 train loss: 1.0889008045196533 train acc: 0.9541666507720947 test loss: 1.3397551774978638 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1758 train loss: 1.0889003276824951 train acc: 0.9541666507720947 test loss: 1.3156380653381348 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1759 train loss: 1.0888689756393433 train acc: 0.9541666507720947 test loss: 1.3172030448913574 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1760 train loss: 1.088735580444336 train acc: 0.9541666507720947 test loss: 1.3429181575775146 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1761 train loss: 1.0888887643814087 train acc: 0.9541666507720947 test loss: 1.3165496587753296 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1762 train loss: 1.0888327360153198 train acc: 0.9541666507720947 test loss: 1.3476581573486328 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 1763 train loss: 1.0889514684677124 train acc: 0.9541666507720947 test loss: 1.3280409574508667 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1764 train loss: 1.0888707637786865 train acc: 0.9541666507720947 test loss: 1.2863069772720337 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1765 train loss: 1.088663935661316 train acc: 0.9541666507720947 test loss: 1.3477977514266968 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1766 train loss: 1.0889707803726196 train acc: 0.9541666507720947 test loss: 1.3225133419036865 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1767 train loss: 1.0889084339141846 train acc: 0.9541666507720947 test loss: 1.310138463973999 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1768 train loss: 1.0888983011245728 train acc: 0.9541666507720947 test loss: 1.3305619955062866 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1769 train loss: 1.0888475179672241 train acc: 0.9541666507720947 test loss: 1.3279378414154053 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1770 train loss: 1.08901047706604 train acc: 0.9541666507720947 test loss: 1.3181484937667847 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1771 train loss: 1.0888108015060425 train acc: 0.9541666507720947 test loss: 1.3027039766311646 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1772 train loss: 1.0888056755065918 train acc: 0.9541666507720947 test loss: 1.300707459449768 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1773 train loss: 1.0888270139694214 train acc: 0.9541666507720947 test loss: 1.318855881690979 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1774 train loss: 1.0887848138809204 train acc: 0.9541666507720947 test loss: 1.316301941871643 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1775 train loss: 1.0889016389846802 train acc: 0.9541666507720947 test loss: 1.313891887664795 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1776 train loss: 1.0887107849121094 train acc: 0.9541666507720947 test loss: 1.3147169351577759 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1777 train loss: 1.0889980792999268 train acc: 0.9541666507720947 test loss: 1.3111554384231567 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1778 train loss: 1.0885289907455444 train acc: 0.9541666507720947 test loss: 1.3026583194732666 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1779 train loss: 1.0888255834579468 train acc: 0.9541666507720947 test loss: 1.3139632940292358 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1780 train loss: 1.0888850688934326 train acc: 0.9541666507720947 test loss: 1.3171731233596802 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1781 train loss: 1.0888335704803467 train acc: 0.9541666507720947 test loss: 1.3184109926223755 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1782 train loss: 1.0887256860733032 train acc: 0.9541666507720947 test loss: 1.3123670816421509 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1783 train loss: 1.088720679283142 train acc: 0.9541666507720947 test loss: 1.333288550376892 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1784 train loss: 1.088989496231079 train acc: 0.9541666507720947 test loss: 1.3214157819747925 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1785 train loss: 1.0889846086502075 train acc: 0.9541666507720947 test loss: 1.3139927387237549 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1786 train loss: 1.0888575315475464 train acc: 0.9541666507720947 test loss: 1.3257237672805786 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1787 train loss: 1.08893620967865 train acc: 0.9541666507720947 test loss: 1.3169772624969482 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1788 train loss: 1.0888901948928833 train acc: 0.9541666507720947 test loss: 1.3209235668182373 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1789 train loss: 1.0890216827392578 train acc: 0.9541666507720947 test loss: 1.318983793258667 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1790 train loss: 1.0887436866760254 train acc: 0.9541666507720947 test loss: 1.2875900268554688 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1791 train loss: 1.088893175125122 train acc: 0.9541666507720947 test loss: 1.3195528984069824 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1792 train loss: 1.0888285636901855 train acc: 0.9541666507720947 test loss: 1.322760820388794 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1793 train loss: 1.0887939929962158 train acc: 0.9541666507720947 test loss: 1.3078808784484863 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1794 train loss: 1.0888631343841553 train acc: 0.9541666507720947 test loss: 1.3220326900482178 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1795 train loss: 1.0889534950256348 train acc: 0.9541666507720947 test loss: 1.3160828351974487 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1796 train loss: 1.088918685913086 train acc: 0.9541666507720947 test loss: 1.3177874088287354 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1797 train loss: 1.0890209674835205 train acc: 0.9541666507720947 test loss: 1.320465326309204 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1798 train loss: 1.0887235403060913 train acc: 0.9541666507720947 test loss: 1.3055565357208252 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1799 train loss: 1.0889434814453125 train acc: 0.9541666507720947 test loss: 1.3291131258010864 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1800 train loss: 1.0889129638671875 train acc: 0.9541666507720947 test loss: 1.3115315437316895 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1801 train loss: 1.0888569355010986 train acc: 0.9541666507720947 test loss: 1.298944115638733 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1802 train loss: 1.088813066482544 train acc: 0.9541666507720947 test loss: 1.3366904258728027 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1803 train loss: 1.0887176990509033 train acc: 0.9541666507720947 test loss: 1.3341529369354248 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1804 train loss: 1.0888248682022095 train acc: 0.9541666507720947 test loss: 1.317469835281372 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1805 train loss: 1.088807225227356 train acc: 0.9541666507720947 test loss: 1.3167062997817993 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1806 train loss: 1.08878493309021 train acc: 0.9541666507720947 test loss: 1.3257299661636353 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1807 train loss: 1.0887311697006226 train acc: 0.9541666507720947 test loss: 1.3216779232025146 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1808 train loss: 1.089016318321228 train acc: 0.9541666507720947 test loss: 1.325251817703247 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1809 train loss: 1.088800072669983 train acc: 0.9541666507720947 test loss: 1.3170562982559204 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1810 train loss: 1.088891863822937 train acc: 0.9541666507720947 test loss: 1.30279541015625 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1811 train loss: 1.0887434482574463 train acc: 0.9541666507720947 test loss: 1.321494221687317 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1812 train loss: 1.0887067317962646 train acc: 0.9541666507720947 test loss: 1.3184102773666382 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1813 train loss: 1.088977336883545 train acc: 0.9541666507720947 test loss: 1.3264219760894775 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1814 train loss: 1.0886586904525757 train acc: 0.9541666507720947 test loss: 1.315763235092163 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1815 train loss: 1.0886715650558472 train acc: 0.9541666507720947 test loss: 1.3118079900741577 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1816 train loss: 1.088714361190796 train acc: 0.9541666507720947 test loss: 1.3212108612060547 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1817 train loss: 1.088858962059021 train acc: 0.9541666507720947 test loss: 1.3216826915740967 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1818 train loss: 1.0887573957443237 train acc: 0.9541666507720947 test loss: 1.3027114868164062 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1819 train loss: 1.0889511108398438 train acc: 0.9541666507720947 test loss: 1.3249499797821045 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1820 train loss: 1.0886940956115723 train acc: 0.9541666507720947 test loss: 1.3234740495681763 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1821 train loss: 1.088901162147522 train acc: 0.9541666507720947 test loss: 1.3254737854003906 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1822 train loss: 1.0885952711105347 train acc: 0.9541666507720947 test loss: 1.3279820680618286 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1823 train loss: 1.088815689086914 train acc: 0.9541666507720947 test loss: 1.3147759437561035 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1824 train loss: 1.0886304378509521 train acc: 0.9541666507720947 test loss: 1.3140559196472168 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1825 train loss: 1.0888272523880005 train acc: 0.9541666507720947 test loss: 1.3146098852157593 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1826 train loss: 1.089045763015747 train acc: 0.9541666507720947 test loss: 1.3085721731185913 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1827 train loss: 1.088939905166626 train acc: 0.9541666507720947 test loss: 1.3310065269470215 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1828 train loss: 1.0890895128250122 train acc: 0.9541666507720947 test loss: 1.3145192861557007 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1829 train loss: 1.0887525081634521 train acc: 0.9541666507720947 test loss: 1.3082858324050903 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1830 train loss: 1.0889474153518677 train acc: 0.9541666507720947 test loss: 1.3166999816894531 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1831 train loss: 1.0890601873397827 train acc: 0.9541666507720947 test loss: 1.3189775943756104 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1832 train loss: 1.0888645648956299 train acc: 0.9541666507720947 test loss: 1.3031947612762451 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1833 train loss: 1.0886740684509277 train acc: 0.9541666507720947 test loss: 1.3149961233139038 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1834 train loss: 1.0887343883514404 train acc: 0.9541666507720947 test loss: 1.3085919618606567 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1835 train loss: 1.0888926982879639 train acc: 0.9541666507720947 test loss: 1.3268135786056519 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1836 train loss: 1.0890973806381226 train acc: 0.9541666507720947 test loss: 1.3233469724655151 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1837 train loss: 1.0887783765792847 train acc: 0.9541666507720947 test loss: 1.3370846509933472 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1838 train loss: 1.0888770818710327 train acc: 0.9541666507720947 test loss: 1.3340661525726318 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1839 train loss: 1.0887418985366821 train acc: 0.9541666507720947 test loss: 1.3163492679595947 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1840 train loss: 1.0890485048294067 train acc: 0.9541666507720947 test loss: 1.315846562385559 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1841 train loss: 1.0890116691589355 train acc: 0.9541666507720947 test loss: 1.3237768411636353 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1842 train loss: 1.0890761613845825 train acc: 0.9541666507720947 test loss: 1.2961723804473877 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1843 train loss: 1.0887563228607178 train acc: 0.9541666507720947 test loss: 1.304362177848816 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1844 train loss: 1.0888888835906982 train acc: 0.9541666507720947 test loss: 1.3020193576812744 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1845 train loss: 1.0889670848846436 train acc: 0.9541666507720947 test loss: 1.3151507377624512 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1846 train loss: 1.0891772508621216 train acc: 0.9541666507720947 test loss: 1.3212138414382935 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1847 train loss: 1.0888043642044067 train acc: 0.9541666507720947 test loss: 1.334781289100647 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1848 train loss: 1.0885685682296753 train acc: 0.9541666507720947 test loss: 1.3288171291351318 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1849 train loss: 1.0889947414398193 train acc: 0.9541666507720947 test loss: 1.3389993906021118 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1850 train loss: 1.0888824462890625 train acc: 0.9541666507720947 test loss: 1.3214370012283325 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1851 train loss: 1.0889981985092163 train acc: 0.9541666507720947 test loss: 1.324798583984375 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1852 train loss: 1.088966965675354 train acc: 0.9541666507720947 test loss: 1.3211572170257568 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1853 train loss: 1.0889278650283813 train acc: 0.9541666507720947 test loss: 1.3052687644958496 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1854 train loss: 1.0891026258468628 train acc: 0.9541666507720947 test loss: 1.3225188255310059 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1855 train loss: 1.088919997215271 train acc: 0.9541666507720947 test loss: 1.3296058177947998 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1856 train loss: 1.0887449979782104 train acc: 0.9541666507720947 test loss: 1.310042142868042 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1857 train loss: 1.0887778997421265 train acc: 0.9541666507720947 test loss: 1.3037185668945312 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1858 train loss: 1.0889533758163452 train acc: 0.9541666507720947 test loss: 1.2966179847717285 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1859 train loss: 1.0889233350753784 train acc: 0.9541666507720947 test loss: 1.3155899047851562 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1860 train loss: 1.0888183116912842 train acc: 0.9541666507720947 test loss: 1.3249671459197998 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1861 train loss: 1.0889219045639038 train acc: 0.9541666507720947 test loss: 1.3186591863632202 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1862 train loss: 1.0889396667480469 train acc: 0.9541666507720947 test loss: 1.3171765804290771 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1863 train loss: 1.0890648365020752 train acc: 0.9541666507720947 test loss: 1.308835744857788 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1864 train loss: 1.0889407396316528 train acc: 0.9541666507720947 test loss: 1.2930203676223755 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1865 train loss: 1.0887802839279175 train acc: 0.9541666507720947 test loss: 1.3271253108978271 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1866 train loss: 1.0887700319290161 train acc: 0.9541666507720947 test loss: 1.3161073923110962 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1867 train loss: 1.0889942646026611 train acc: 0.9541666507720947 test loss: 1.3150403499603271 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1868 train loss: 1.0889484882354736 train acc: 0.9541666507720947 test loss: 1.3241655826568604 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1869 train loss: 1.0890071392059326 train acc: 0.9541666507720947 test loss: 1.317260503768921 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1870 train loss: 1.0887426137924194 train acc: 0.9541666507720947 test loss: 1.303848147392273 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1871 train loss: 1.088694453239441 train acc: 0.9541666507720947 test loss: 1.2890928983688354 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1872 train loss: 1.088939905166626 train acc: 0.9541666507720947 test loss: 1.3089438676834106 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1873 train loss: 1.088626503944397 train acc: 0.9541666507720947 test loss: 1.3061960935592651 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1874 train loss: 1.0888593196868896 train acc: 0.9541666507720947 test loss: 1.3234869241714478 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1875 train loss: 1.0889559984207153 train acc: 0.9541666507720947 test loss: 1.330145001411438 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1876 train loss: 1.0887298583984375 train acc: 0.9541666507720947 test loss: 1.2915796041488647 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1877 train loss: 1.0888731479644775 train acc: 0.9541666507720947 test loss: 1.326647400856018 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1878 train loss: 1.0889228582382202 train acc: 0.9541666507720947 test loss: 1.3131637573242188 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1879 train loss: 1.0888196229934692 train acc: 0.9541666507720947 test loss: 1.3142026662826538 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1880 train loss: 1.088964581489563 train acc: 0.9541666507720947 test loss: 1.3490694761276245 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 1881 train loss: 1.0887757539749146 train acc: 0.9541666507720947 test loss: 1.3212635517120361 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1882 train loss: 1.088991641998291 train acc: 0.9541666507720947 test loss: 1.3241244554519653 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1883 train loss: 1.0890158414840698 train acc: 0.9541666507720947 test loss: 1.315488576889038 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1884 train loss: 1.0890228748321533 train acc: 0.9541666507720947 test loss: 1.308663010597229 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1885 train loss: 1.0889185667037964 train acc: 0.9541666507720947 test loss: 1.3238749504089355 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1886 train loss: 1.0888786315917969 train acc: 0.9541666507720947 test loss: 1.3277512788772583 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1887 train loss: 1.088828682899475 train acc: 0.9541666507720947 test loss: 1.312117338180542 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1888 train loss: 1.0888221263885498 train acc: 0.9541666507720947 test loss: 1.3149104118347168 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1889 train loss: 1.0888569355010986 train acc: 0.9541666507720947 test loss: 1.322035312652588 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1890 train loss: 1.088904857635498 train acc: 0.9541666507720947 test loss: 1.325122356414795 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1891 train loss: 1.0888683795928955 train acc: 0.9541666507720947 test loss: 1.3267433643341064 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1892 train loss: 1.0889428853988647 train acc: 0.9541666507720947 test loss: 1.3299208879470825 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1893 train loss: 1.088793396949768 train acc: 0.9541666507720947 test loss: 1.3087290525436401 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1894 train loss: 1.0888898372650146 train acc: 0.9541666507720947 test loss: 1.3146262168884277 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1895 train loss: 1.0887593030929565 train acc: 0.9541666507720947 test loss: 1.3144962787628174 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1896 train loss: 1.0889943838119507 train acc: 0.9541666507720947 test loss: 1.318325400352478 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1897 train loss: 1.0888879299163818 train acc: 0.9541666507720947 test loss: 1.3055188655853271 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1898 train loss: 1.0889184474945068 train acc: 0.9541666507720947 test loss: 1.3209760189056396 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1899 train loss: 1.0890744924545288 train acc: 0.9541666507720947 test loss: 1.3271671533584595 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1900 train loss: 1.0887598991394043 train acc: 0.9541666507720947 test loss: 1.2983899116516113 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1901 train loss: 1.0888828039169312 train acc: 0.9541666507720947 test loss: 1.3320399522781372 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1902 train loss: 1.0889666080474854 train acc: 0.9541666507720947 test loss: 1.3315056562423706 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1903 train loss: 1.0891207456588745 train acc: 0.9541666507720947 test loss: 1.2986701726913452 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 1904 train loss: 1.0890278816223145 train acc: 0.9541666507720947 test loss: 1.3063799142837524 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1905 train loss: 1.089375615119934 train acc: 0.9541666507720947 test loss: 1.3080193996429443 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1906 train loss: 1.0887870788574219 train acc: 0.9541666507720947 test loss: 1.3076858520507812 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1907 train loss: 1.0889567136764526 train acc: 0.9541666507720947 test loss: 1.3326992988586426 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1908 train loss: 1.088809609413147 train acc: 0.9541666507720947 test loss: 1.322077989578247 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1909 train loss: 1.0889726877212524 train acc: 0.9541666507720947 test loss: 1.3215242624282837 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1910 train loss: 1.088965654373169 train acc: 0.9541666507720947 test loss: 1.3062371015548706 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1911 train loss: 1.0887850522994995 train acc: 0.9541666507720947 test loss: 1.3201677799224854 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1912 train loss: 1.088891625404358 train acc: 0.9541666507720947 test loss: 1.3256049156188965 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1913 train loss: 1.0891153812408447 train acc: 0.9541666507720947 test loss: 1.3291655778884888 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1914 train loss: 1.0887484550476074 train acc: 0.9541666507720947 test loss: 1.3288905620574951 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1915 train loss: 1.088875651359558 train acc: 0.9541666507720947 test loss: 1.2853052616119385 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 1916 train loss: 1.0887742042541504 train acc: 0.9541666507720947 test loss: 1.3315669298171997 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1917 train loss: 1.0887330770492554 train acc: 0.9541666507720947 test loss: 1.3167569637298584 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1918 train loss: 1.0890419483184814 train acc: 0.9541666507720947 test loss: 1.2951436042785645 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1919 train loss: 1.088875651359558 train acc: 0.9541666507720947 test loss: 1.3174822330474854 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1920 train loss: 1.0887411832809448 train acc: 0.9541666507720947 test loss: 1.3180676698684692 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1921 train loss: 1.089173674583435 train acc: 0.9541666507720947 test loss: 1.3093310594558716 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1922 train loss: 1.0887352228164673 train acc: 0.9541666507720947 test loss: 1.3049485683441162 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1923 train loss: 1.0887432098388672 train acc: 0.9541666507720947 test loss: 1.3301522731781006 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1924 train loss: 1.0889935493469238 train acc: 0.9541666507720947 test loss: 1.3002873659133911 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1925 train loss: 1.0887480974197388 train acc: 0.9541666507720947 test loss: 1.3403135538101196 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1926 train loss: 1.0889438390731812 train acc: 0.9541666507720947 test loss: 1.3128730058670044 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1927 train loss: 1.088781714439392 train acc: 0.9541666507720947 test loss: 1.3314974308013916 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1928 train loss: 1.0887832641601562 train acc: 0.9541666507720947 test loss: 1.321040391921997 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1929 train loss: 1.088875651359558 train acc: 0.9541666507720947 test loss: 1.3122047185897827 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1930 train loss: 1.0885703563690186 train acc: 0.9541666507720947 test loss: 1.3156912326812744 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1931 train loss: 1.088678240776062 train acc: 0.9541666507720947 test loss: 1.3201650381088257 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1932 train loss: 1.088808298110962 train acc: 0.9541666507720947 test loss: 1.3077952861785889 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1933 train loss: 1.0888751745224 train acc: 0.9541666507720947 test loss: 1.3119847774505615 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1934 train loss: 1.08881676197052 train acc: 0.9541666507720947 test loss: 1.3088833093643188 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1935 train loss: 1.0888937711715698 train acc: 0.9541666507720947 test loss: 1.2942579984664917 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1936 train loss: 1.0887807607650757 train acc: 0.9541666507720947 test loss: 1.3264691829681396 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1937 train loss: 1.0887187719345093 train acc: 0.9541666507720947 test loss: 1.3019689321517944 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1938 train loss: 1.0887421369552612 train acc: 0.9541666507720947 test loss: 1.3035295009613037 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1939 train loss: 1.0888551473617554 train acc: 0.9541666507720947 test loss: 1.2991468906402588 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1940 train loss: 1.088772177696228 train acc: 0.9541666507720947 test loss: 1.3134706020355225 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1941 train loss: 1.0886095762252808 train acc: 0.9541666507720947 test loss: 1.3039406538009644 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1942 train loss: 1.0887954235076904 train acc: 0.9541666507720947 test loss: 1.316127896308899 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1943 train loss: 1.088794231414795 train acc: 0.9541666507720947 test loss: 1.333661675453186 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1944 train loss: 1.088931918144226 train acc: 0.9541666507720947 test loss: 1.3032901287078857 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1945 train loss: 1.088861346244812 train acc: 0.9541666507720947 test loss: 1.3068188428878784 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1946 train loss: 1.0887634754180908 train acc: 0.9541666507720947 test loss: 1.309894323348999 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1947 train loss: 1.0885683298110962 train acc: 0.9541666507720947 test loss: 1.3104814291000366 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1948 train loss: 1.0889664888381958 train acc: 0.9541666507720947 test loss: 1.3207813501358032 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1949 train loss: 1.0887188911437988 train acc: 0.9541666507720947 test loss: 1.310278296470642 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1950 train loss: 1.0889586210250854 train acc: 0.9541666507720947 test loss: 1.3253110647201538 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1951 train loss: 1.0888259410858154 train acc: 0.9541666507720947 test loss: 1.3341313600540161 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1952 train loss: 1.0888707637786865 train acc: 0.9541666507720947 test loss: 1.3319721221923828 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 1953 train loss: 1.0888707637786865 train acc: 0.9541666507720947 test loss: 1.3193732500076294 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1954 train loss: 1.0889605283737183 train acc: 0.9541666507720947 test loss: 1.311780571937561 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1955 train loss: 1.0889633893966675 train acc: 0.9541666507720947 test loss: 1.3153401613235474 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1956 train loss: 1.088743805885315 train acc: 0.9541666507720947 test loss: 1.3150360584259033 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1957 train loss: 1.0888361930847168 train acc: 0.9541666507720947 test loss: 1.3378065824508667 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1958 train loss: 1.0886683464050293 train acc: 0.9541666507720947 test loss: 1.2993344068527222 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1959 train loss: 1.0887153148651123 train acc: 0.9541666507720947 test loss: 1.2988263368606567 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1960 train loss: 1.0886279344558716 train acc: 0.9541666507720947 test loss: 1.3257611989974976 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1961 train loss: 1.088745355606079 train acc: 0.9541666507720947 test loss: 1.3174642324447632 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1962 train loss: 1.0887227058410645 train acc: 0.9541666507720947 test loss: 1.3211009502410889 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1963 train loss: 1.0888344049453735 train acc: 0.9541666507720947 test loss: 1.333373785018921 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1964 train loss: 1.0889405012130737 train acc: 0.9541666507720947 test loss: 1.3188796043395996 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1965 train loss: 1.0891146659851074 train acc: 0.9541666507720947 test loss: 1.320127010345459 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1966 train loss: 1.0888819694519043 train acc: 0.9541666507720947 test loss: 1.3122408390045166 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1967 train loss: 1.0887925624847412 train acc: 0.9541666507720947 test loss: 1.288134217262268 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 1968 train loss: 1.0889078378677368 train acc: 0.9541666507720947 test loss: 1.3264470100402832 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1969 train loss: 1.0888950824737549 train acc: 0.9541666507720947 test loss: 1.309577226638794 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1970 train loss: 1.088524341583252 train acc: 0.9541666507720947 test loss: 1.3252352476119995 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1971 train loss: 1.0889760255813599 train acc: 0.9541666507720947 test loss: 1.3126193284988403 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1972 train loss: 1.0890477895736694 train acc: 0.9541666507720947 test loss: 1.3103045225143433 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1973 train loss: 1.0887036323547363 train acc: 0.9541666507720947 test loss: 1.3042755126953125 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1974 train loss: 1.0888421535491943 train acc: 0.9541666507720947 test loss: 1.3140391111373901 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1975 train loss: 1.08884596824646 train acc: 0.9541666507720947 test loss: 1.336816430091858 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1976 train loss: 1.0886996984481812 train acc: 0.9541666507720947 test loss: 1.3152260780334473 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1977 train loss: 1.0888887643814087 train acc: 0.9541666507720947 test loss: 1.3057197332382202 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1978 train loss: 1.088869571685791 train acc: 0.9541666507720947 test loss: 1.3289142847061157 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 1979 train loss: 1.0888664722442627 train acc: 0.9541666507720947 test loss: 1.3245196342468262 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1980 train loss: 1.0890470743179321 train acc: 0.9541666507720947 test loss: 1.3201122283935547 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1981 train loss: 1.0891393423080444 train acc: 0.9541666507720947 test loss: 1.3177047967910767 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1982 train loss: 1.0890132188796997 train acc: 0.9541666507720947 test loss: 1.3379653692245483 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1983 train loss: 1.0886495113372803 train acc: 0.9541666507720947 test loss: 1.3272602558135986 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 1984 train loss: 1.0887815952301025 train acc: 0.9541666507720947 test loss: 1.314125657081604 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1985 train loss: 1.0886552333831787 train acc: 0.9541666507720947 test loss: 1.3285259008407593 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1986 train loss: 1.088777780532837 train acc: 0.9541666507720947 test loss: 1.3060263395309448 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1987 train loss: 1.0886064767837524 train acc: 0.9541666507720947 test loss: 1.3186581134796143 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1988 train loss: 1.0889544486999512 train acc: 0.9541666507720947 test loss: 1.3123301267623901 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1989 train loss: 1.0886919498443604 train acc: 0.9541666507720947 test loss: 1.3217650651931763 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1990 train loss: 1.0888170003890991 train acc: 0.9541666507720947 test loss: 1.3201874494552612 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1991 train loss: 1.0888111591339111 train acc: 0.9541666507720947 test loss: 1.327933669090271 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1992 train loss: 1.0887374877929688 train acc: 0.9541666507720947 test loss: 1.3039613962173462 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 1993 train loss: 1.088877558708191 train acc: 0.9541666507720947 test loss: 1.3206374645233154 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1994 train loss: 1.0888115167617798 train acc: 0.9541666507720947 test loss: 1.3087564706802368 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1995 train loss: 1.088722825050354 train acc: 0.9541666507720947 test loss: 1.316228985786438 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 1996 train loss: 1.0887863636016846 train acc: 0.9541666507720947 test loss: 1.3157376050949097 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 1997 train loss: 1.0887110233306885 train acc: 0.9541666507720947 test loss: 1.3317986726760864 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 1998 train loss: 1.088854432106018 train acc: 0.9541666507720947 test loss: 1.3040745258331299 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 1999 train loss: 1.0886015892028809 train acc: 0.9541666507720947 test loss: 1.3192028999328613 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2000 train loss: 1.088736891746521 train acc: 0.9541666507720947 test loss: 1.3328664302825928 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2001 train loss: 1.0888336896896362 train acc: 0.9541666507720947 test loss: 1.3194996118545532 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2002 train loss: 1.0888742208480835 train acc: 0.9541666507720947 test loss: 1.3284739255905151 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2003 train loss: 1.0890525579452515 train acc: 0.9541666507720947 test loss: 1.3109184503555298 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2004 train loss: 1.0886317491531372 train acc: 0.9541666507720947 test loss: 1.3186146020889282 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2005 train loss: 1.0885831117630005 train acc: 0.9541666507720947 test loss: 1.3151545524597168 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2006 train loss: 1.0887395143508911 train acc: 0.9541666507720947 test loss: 1.3108651638031006 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2007 train loss: 1.0889145135879517 train acc: 0.9541666507720947 test loss: 1.3240565061569214 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2008 train loss: 1.0886726379394531 train acc: 0.9541666507720947 test loss: 1.307257890701294 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2009 train loss: 1.0888407230377197 train acc: 0.9541666507720947 test loss: 1.3228763341903687 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2010 train loss: 1.0886363983154297 train acc: 0.9541666507720947 test loss: 1.3246678113937378 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2011 train loss: 1.0888543128967285 train acc: 0.9541666507720947 test loss: 1.321452260017395 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2012 train loss: 1.0889356136322021 train acc: 0.9541666507720947 test loss: 1.3070605993270874 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2013 train loss: 1.0887166261672974 train acc: 0.9541666507720947 test loss: 1.325507640838623 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2014 train loss: 1.0886435508728027 train acc: 0.9541666507720947 test loss: 1.3270857334136963 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2015 train loss: 1.088928461074829 train acc: 0.9541666507720947 test loss: 1.3270941972732544 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2016 train loss: 1.0888360738754272 train acc: 0.9541666507720947 test loss: 1.3095664978027344 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2017 train loss: 1.0887316465377808 train acc: 0.9541666507720947 test loss: 1.3106987476348877 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2018 train loss: 1.088956356048584 train acc: 0.9541666507720947 test loss: 1.3296713829040527 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2019 train loss: 1.0887471437454224 train acc: 0.9541666507720947 test loss: 1.3346748352050781 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2020 train loss: 1.088944911956787 train acc: 0.9541666507720947 test loss: 1.3276690244674683 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2021 train loss: 1.0889792442321777 train acc: 0.9541666507720947 test loss: 1.3102185726165771 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2022 train loss: 1.0890034437179565 train acc: 0.9541666507720947 test loss: 1.3367841243743896 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2023 train loss: 1.0886722803115845 train acc: 0.9541666507720947 test loss: 1.3210171461105347 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2024 train loss: 1.0888900756835938 train acc: 0.9541666507720947 test loss: 1.3188083171844482 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2025 train loss: 1.088735818862915 train acc: 0.9541666507720947 test loss: 1.3205103874206543 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2026 train loss: 1.0887176990509033 train acc: 0.9541666507720947 test loss: 1.3462166786193848 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2027 train loss: 1.088721513748169 train acc: 0.9541666507720947 test loss: 1.3135409355163574 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2028 train loss: 1.0888863801956177 train acc: 0.9541666507720947 test loss: 1.3287264108657837 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2029 train loss: 1.0890061855316162 train acc: 0.9541666507720947 test loss: 1.3148454427719116 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2030 train loss: 1.0889780521392822 train acc: 0.9541666507720947 test loss: 1.3069934844970703 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2031 train loss: 1.0887161493301392 train acc: 0.9541666507720947 test loss: 1.3093043565750122 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2032 train loss: 1.0888882875442505 train acc: 0.9541666507720947 test loss: 1.3297793865203857 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2033 train loss: 1.0887738466262817 train acc: 0.9541666507720947 test loss: 1.324325680732727 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2034 train loss: 1.0887045860290527 train acc: 0.9541666507720947 test loss: 1.3134633302688599 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2035 train loss: 1.088950514793396 train acc: 0.9541666507720947 test loss: 1.31342351436615 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2036 train loss: 1.088754415512085 train acc: 0.9541666507720947 test loss: 1.3458709716796875 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2037 train loss: 1.0886085033416748 train acc: 0.9541666507720947 test loss: 1.3394426107406616 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2038 train loss: 1.0890651941299438 train acc: 0.9541666507720947 test loss: 1.3136481046676636 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2039 train loss: 1.0888068675994873 train acc: 0.9541666507720947 test loss: 1.336832880973816 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2040 train loss: 1.0888696908950806 train acc: 0.9541666507720947 test loss: 1.3427577018737793 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2041 train loss: 1.0888233184814453 train acc: 0.9541666507720947 test loss: 1.3156739473342896 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2042 train loss: 1.0887531042099 train acc: 0.9541666507720947 test loss: 1.328274130821228 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2043 train loss: 1.0888988971710205 train acc: 0.9541666507720947 test loss: 1.3270001411437988 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2044 train loss: 1.088834524154663 train acc: 0.9541666507720947 test loss: 1.3340426683425903 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2045 train loss: 1.088744878768921 train acc: 0.9541666507720947 test loss: 1.3175126314163208 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2046 train loss: 1.0887693166732788 train acc: 0.9541666507720947 test loss: 1.3113948106765747 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2047 train loss: 1.0890753269195557 train acc: 0.9541666507720947 test loss: 1.3276432752609253 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2048 train loss: 1.0888789892196655 train acc: 0.9541666507720947 test loss: 1.3090765476226807 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2049 train loss: 1.0888310670852661 train acc: 0.9541666507720947 test loss: 1.3236066102981567 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2050 train loss: 1.0888099670410156 train acc: 0.9541666507720947 test loss: 1.3082332611083984 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2051 train loss: 1.088895320892334 train acc: 0.9541666507720947 test loss: 1.31236732006073 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2052 train loss: 1.0890085697174072 train acc: 0.9541666507720947 test loss: 1.319257378578186 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2053 train loss: 1.0889687538146973 train acc: 0.9541666507720947 test loss: 1.3170472383499146 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2054 train loss: 1.0888248682022095 train acc: 0.9541666507720947 test loss: 1.3211544752120972 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2055 train loss: 1.0890952348709106 train acc: 0.9541666507720947 test loss: 1.3230646848678589 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2056 train loss: 1.0888636112213135 train acc: 0.9541666507720947 test loss: 1.3230830430984497 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2057 train loss: 1.0887420177459717 train acc: 0.9541666507720947 test loss: 1.3079302310943604 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2058 train loss: 1.0886293649673462 train acc: 0.9541666507720947 test loss: 1.3147785663604736 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2059 train loss: 1.0887764692306519 train acc: 0.9541666507720947 test loss: 1.3114210367202759 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2060 train loss: 1.0888667106628418 train acc: 0.9541666507720947 test loss: 1.328529953956604 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2061 train loss: 1.0887762308120728 train acc: 0.9541666507720947 test loss: 1.3088361024856567 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2062 train loss: 1.0886951684951782 train acc: 0.9541666507720947 test loss: 1.2883034944534302 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 2063 train loss: 1.0889924764633179 train acc: 0.9541666507720947 test loss: 1.3290326595306396 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2064 train loss: 1.0887027978897095 train acc: 0.9541666507720947 test loss: 1.326791524887085 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2065 train loss: 1.0888876914978027 train acc: 0.9541666507720947 test loss: 1.3260823488235474 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2066 train loss: 1.0886822938919067 train acc: 0.9541666507720947 test loss: 1.322319507598877 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2067 train loss: 1.088751196861267 train acc: 0.9541666507720947 test loss: 1.3453962802886963 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2068 train loss: 1.0888440608978271 train acc: 0.9541666507720947 test loss: 1.3155124187469482 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2069 train loss: 1.0888447761535645 train acc: 0.9541666507720947 test loss: 1.3099478483200073 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2070 train loss: 1.0888316631317139 train acc: 0.9541666507720947 test loss: 1.3029062747955322 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2071 train loss: 1.088771104812622 train acc: 0.9541666507720947 test loss: 1.3046928644180298 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2072 train loss: 1.0889238119125366 train acc: 0.9541666507720947 test loss: 1.3185855150222778 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2073 train loss: 1.088638186454773 train acc: 0.9541666507720947 test loss: 1.3215266466140747 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2074 train loss: 1.0887070894241333 train acc: 0.9541666507720947 test loss: 1.307565450668335 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2075 train loss: 1.088809609413147 train acc: 0.9541666507720947 test loss: 1.3124001026153564 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2076 train loss: 1.0887998342514038 train acc: 0.9541666507720947 test loss: 1.3199812173843384 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2077 train loss: 1.0888559818267822 train acc: 0.9541666507720947 test loss: 1.3286921977996826 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2078 train loss: 1.0887584686279297 train acc: 0.9541666507720947 test loss: 1.3049850463867188 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2079 train loss: 1.0886577367782593 train acc: 0.9541666507720947 test loss: 1.2974194288253784 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2080 train loss: 1.0886858701705933 train acc: 0.9541666507720947 test loss: 1.316464900970459 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2081 train loss: 1.0887399911880493 train acc: 0.9541666507720947 test loss: 1.3158546686172485 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2082 train loss: 1.0889580249786377 train acc: 0.9541666507720947 test loss: 1.3128018379211426 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2083 train loss: 1.0889892578125 train acc: 0.9541666507720947 test loss: 1.3092005252838135 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2084 train loss: 1.0889028310775757 train acc: 0.9541666507720947 test loss: 1.318581461906433 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2085 train loss: 1.0889503955841064 train acc: 0.9541666507720947 test loss: 1.302255392074585 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2086 train loss: 1.0889487266540527 train acc: 0.9541666507720947 test loss: 1.329420804977417 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2087 train loss: 1.0889419317245483 train acc: 0.9541666507720947 test loss: 1.3167169094085693 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2088 train loss: 1.088779330253601 train acc: 0.9541666507720947 test loss: 1.3118189573287964 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2089 train loss: 1.0887477397918701 train acc: 0.9541666507720947 test loss: 1.3101879358291626 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2090 train loss: 1.088817834854126 train acc: 0.9541666507720947 test loss: 1.3150888681411743 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2091 train loss: 1.0887848138809204 train acc: 0.9541666507720947 test loss: 1.3210928440093994 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2092 train loss: 1.0886895656585693 train acc: 0.9541666507720947 test loss: 1.3196507692337036 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2093 train loss: 1.088739275932312 train acc: 0.9541666507720947 test loss: 1.3080557584762573 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2094 train loss: 1.0888291597366333 train acc: 0.9541666507720947 test loss: 1.320568323135376 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2095 train loss: 1.0890800952911377 train acc: 0.9541666507720947 test loss: 1.3078128099441528 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2096 train loss: 1.0888413190841675 train acc: 0.9541666507720947 test loss: 1.3051488399505615 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2097 train loss: 1.0886558294296265 train acc: 0.9541666507720947 test loss: 1.3337867259979248 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2098 train loss: 1.0888129472732544 train acc: 0.9541666507720947 test loss: 1.3311837911605835 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2099 train loss: 1.0886096954345703 train acc: 0.9541666507720947 test loss: 1.3037726879119873 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2100 train loss: 1.0889517068862915 train acc: 0.9541666507720947 test loss: 1.308511734008789 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2101 train loss: 1.0887538194656372 train acc: 0.9541666507720947 test loss: 1.312242865562439 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2102 train loss: 1.0887315273284912 train acc: 0.9541666507720947 test loss: 1.3016704320907593 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2103 train loss: 1.0889419317245483 train acc: 0.9541666507720947 test loss: 1.3197396993637085 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2104 train loss: 1.0886138677597046 train acc: 0.9541666507720947 test loss: 1.305724859237671 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2105 train loss: 1.0888408422470093 train acc: 0.9541666507720947 test loss: 1.3273423910140991 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2106 train loss: 1.0888272523880005 train acc: 0.9541666507720947 test loss: 1.3165745735168457 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2107 train loss: 1.0888710021972656 train acc: 0.9541666507720947 test loss: 1.3166636228561401 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2108 train loss: 1.0886801481246948 train acc: 0.9541666507720947 test loss: 1.3232429027557373 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2109 train loss: 1.088847279548645 train acc: 0.9541666507720947 test loss: 1.3087584972381592 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2110 train loss: 1.0889135599136353 train acc: 0.9541666507720947 test loss: 1.3096784353256226 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2111 train loss: 1.0887361764907837 train acc: 0.9541666507720947 test loss: 1.3067594766616821 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2112 train loss: 1.0887125730514526 train acc: 0.9541666507720947 test loss: 1.297980785369873 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2113 train loss: 1.0886722803115845 train acc: 0.9541666507720947 test loss: 1.315102219581604 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2114 train loss: 1.0887380838394165 train acc: 0.9541666507720947 test loss: 1.323117971420288 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2115 train loss: 1.0888559818267822 train acc: 0.9541666507720947 test loss: 1.333425760269165 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2116 train loss: 1.0886045694351196 train acc: 0.9541666507720947 test loss: 1.3187134265899658 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2117 train loss: 1.0887812376022339 train acc: 0.9541666507720947 test loss: 1.321146845817566 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2118 train loss: 1.0887808799743652 train acc: 0.9541666507720947 test loss: 1.320925235748291 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2119 train loss: 1.0886415243148804 train acc: 0.9541666507720947 test loss: 1.3322961330413818 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2120 train loss: 1.08878755569458 train acc: 0.9541666507720947 test loss: 1.3122031688690186 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2121 train loss: 1.0886093378067017 train acc: 0.9541666507720947 test loss: 1.3214997053146362 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2122 train loss: 1.0887396335601807 train acc: 0.9541666507720947 test loss: 1.3200348615646362 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2123 train loss: 1.088879942893982 train acc: 0.9541666507720947 test loss: 1.3147295713424683 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2124 train loss: 1.0887947082519531 train acc: 0.9541666507720947 test loss: 1.3243088722229004 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2125 train loss: 1.0888649225234985 train acc: 0.9541666507720947 test loss: 1.3247262239456177 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2126 train loss: 1.0887386798858643 train acc: 0.9541666507720947 test loss: 1.2986606359481812 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2127 train loss: 1.0888288021087646 train acc: 0.9541666507720947 test loss: 1.3194400072097778 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2128 train loss: 1.0888009071350098 train acc: 0.9541666507720947 test loss: 1.3001792430877686 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2129 train loss: 1.088883876800537 train acc: 0.9541666507720947 test loss: 1.319466471672058 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2130 train loss: 1.0889843702316284 train acc: 0.9541666507720947 test loss: 1.297649621963501 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2131 train loss: 1.0889077186584473 train acc: 0.9541666507720947 test loss: 1.3138744831085205 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2132 train loss: 1.0889376401901245 train acc: 0.9541666507720947 test loss: 1.3328590393066406 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2133 train loss: 1.0888618230819702 train acc: 0.9541666507720947 test loss: 1.3244175910949707 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2134 train loss: 1.0890017747879028 train acc: 0.9541666507720947 test loss: 1.309810757637024 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2135 train loss: 1.0887759923934937 train acc: 0.9541666507720947 test loss: 1.3151887655258179 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2136 train loss: 1.0888878107070923 train acc: 0.9541666507720947 test loss: 1.323493242263794 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2137 train loss: 1.088798999786377 train acc: 0.9541666507720947 test loss: 1.3123481273651123 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2138 train loss: 1.0886788368225098 train acc: 0.9541666507720947 test loss: 1.3225922584533691 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2139 train loss: 1.0886340141296387 train acc: 0.9541666507720947 test loss: 1.3066668510437012 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2140 train loss: 1.0888216495513916 train acc: 0.9541666507720947 test loss: 1.3314862251281738 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2141 train loss: 1.0888206958770752 train acc: 0.9541666507720947 test loss: 1.321958303451538 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2142 train loss: 1.0887713432312012 train acc: 0.9541666507720947 test loss: 1.321932077407837 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2143 train loss: 1.0887418985366821 train acc: 0.9541666507720947 test loss: 1.311827301979065 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2144 train loss: 1.0888458490371704 train acc: 0.9541666507720947 test loss: 1.3082667589187622 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2145 train loss: 1.0885059833526611 train acc: 0.9541666507720947 test loss: 1.316540241241455 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2146 train loss: 1.0888941287994385 train acc: 0.9541666507720947 test loss: 1.3296927213668823 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2147 train loss: 1.0890713930130005 train acc: 0.9541666507720947 test loss: 1.312512755393982 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2148 train loss: 1.088903784751892 train acc: 0.9541666507720947 test loss: 1.3238736391067505 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2149 train loss: 1.0887128114700317 train acc: 0.9541666507720947 test loss: 1.3140121698379517 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2150 train loss: 1.0887128114700317 train acc: 0.9541666507720947 test loss: 1.3177638053894043 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2151 train loss: 1.0888921022415161 train acc: 0.9541666507720947 test loss: 1.319205403327942 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2152 train loss: 1.0890541076660156 train acc: 0.9541666507720947 test loss: 1.3261874914169312 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2153 train loss: 1.0888975858688354 train acc: 0.9541666507720947 test loss: 1.3192468881607056 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2154 train loss: 1.0890980958938599 train acc: 0.9541666507720947 test loss: 1.3440964221954346 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2155 train loss: 1.0887337923049927 train acc: 0.9541666507720947 test loss: 1.3255258798599243 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2156 train loss: 1.0887439250946045 train acc: 0.9541666507720947 test loss: 1.308669924736023 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2157 train loss: 1.0889028310775757 train acc: 0.9541666507720947 test loss: 1.3249008655548096 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2158 train loss: 1.0890916585922241 train acc: 0.9541666507720947 test loss: 1.330755591392517 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2159 train loss: 1.0888216495513916 train acc: 0.9541666507720947 test loss: 1.306042194366455 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2160 train loss: 1.0888186693191528 train acc: 0.9541666507720947 test loss: 1.332033395767212 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2161 train loss: 1.0889297723770142 train acc: 0.9541666507720947 test loss: 1.302229642868042 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2162 train loss: 1.088998794555664 train acc: 0.9541666507720947 test loss: 1.316857099533081 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2163 train loss: 1.088741660118103 train acc: 0.9541666507720947 test loss: 1.2980128526687622 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2164 train loss: 1.0886788368225098 train acc: 0.9541666507720947 test loss: 1.3163710832595825 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2165 train loss: 1.088727593421936 train acc: 0.9541666507720947 test loss: 1.3235516548156738 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2166 train loss: 1.0885676145553589 train acc: 0.9541666507720947 test loss: 1.3091262578964233 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2167 train loss: 1.0886908769607544 train acc: 0.9541666507720947 test loss: 1.3157944679260254 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2168 train loss: 1.0887247323989868 train acc: 0.9541666507720947 test loss: 1.3148140907287598 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2169 train loss: 1.0887705087661743 train acc: 0.9541666507720947 test loss: 1.3204407691955566 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2170 train loss: 1.088742733001709 train acc: 0.9541666507720947 test loss: 1.3122962713241577 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2171 train loss: 1.0889242887496948 train acc: 0.9541666507720947 test loss: 1.2969372272491455 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2172 train loss: 1.08875572681427 train acc: 0.9541666507720947 test loss: 1.311975359916687 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2173 train loss: 1.088718056678772 train acc: 0.9541666507720947 test loss: 1.3076432943344116 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2174 train loss: 1.0887573957443237 train acc: 0.9541666507720947 test loss: 1.300650954246521 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2175 train loss: 1.088861346244812 train acc: 0.9541666507720947 test loss: 1.3034898042678833 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2176 train loss: 1.088852047920227 train acc: 0.9541666507720947 test loss: 1.3068485260009766 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2177 train loss: 1.0888705253601074 train acc: 0.9541666507720947 test loss: 1.3136616945266724 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2178 train loss: 1.0887031555175781 train acc: 0.9541666507720947 test loss: 1.310538649559021 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2179 train loss: 1.0886967182159424 train acc: 0.9541666507720947 test loss: 1.3078515529632568 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2180 train loss: 1.0889323949813843 train acc: 0.9541666507720947 test loss: 1.3013921976089478 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2181 train loss: 1.0887806415557861 train acc: 0.9541666507720947 test loss: 1.3100746870040894 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2182 train loss: 1.0887037515640259 train acc: 0.9541666507720947 test loss: 1.2972975969314575 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 2183 train loss: 1.0888830423355103 train acc: 0.9541666507720947 test loss: 1.3090851306915283 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2184 train loss: 1.0888828039169312 train acc: 0.9541666507720947 test loss: 1.3168388605117798 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2185 train loss: 1.0888553857803345 train acc: 0.9541666507720947 test loss: 1.3388093709945679 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2186 train loss: 1.0896079540252686 train acc: 0.9520833492279053 test loss: 1.3159016370773315 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2187 train loss: 1.088801622390747 train acc: 0.9541666507720947 test loss: 1.3174080848693848 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2188 train loss: 1.0888243913650513 train acc: 0.9541666507720947 test loss: 1.3189945220947266 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2189 train loss: 1.0888230800628662 train acc: 0.9541666507720947 test loss: 1.3327748775482178 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2190 train loss: 1.088881254196167 train acc: 0.9541666507720947 test loss: 1.3224400281906128 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2191 train loss: 1.0886433124542236 train acc: 0.9541666507720947 test loss: 1.3349251747131348 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2192 train loss: 1.0887936353683472 train acc: 0.9541666507720947 test loss: 1.3124750852584839 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2193 train loss: 1.0888497829437256 train acc: 0.9541666507720947 test loss: 1.3227180242538452 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2194 train loss: 1.0889008045196533 train acc: 0.9541666507720947 test loss: 1.3394389152526855 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2195 train loss: 1.0886932611465454 train acc: 0.9541666507720947 test loss: 1.3142896890640259 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2196 train loss: 1.0886876583099365 train acc: 0.9541666507720947 test loss: 1.3377556800842285 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2197 train loss: 1.0888010263442993 train acc: 0.9541666507720947 test loss: 1.3108364343643188 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2198 train loss: 1.088760495185852 train acc: 0.9541666507720947 test loss: 1.325734257698059 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2199 train loss: 1.0889960527420044 train acc: 0.9541666507720947 test loss: 1.3096039295196533 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2200 train loss: 1.0887138843536377 train acc: 0.9541666507720947 test loss: 1.3136999607086182 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2201 train loss: 1.0888497829437256 train acc: 0.9541666507720947 test loss: 1.3135393857955933 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2202 train loss: 1.088716745376587 train acc: 0.9541666507720947 test loss: 1.3291077613830566 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2203 train loss: 1.0888144969940186 train acc: 0.9541666507720947 test loss: 1.3292356729507446 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2204 train loss: 1.0886310338974 train acc: 0.9541666507720947 test loss: 1.2983571290969849 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2205 train loss: 1.0886982679367065 train acc: 0.9541666507720947 test loss: 1.320035457611084 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2206 train loss: 1.0888211727142334 train acc: 0.9541666507720947 test loss: 1.3189151287078857 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2207 train loss: 1.0888984203338623 train acc: 0.9541666507720947 test loss: 1.3372199535369873 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2208 train loss: 1.088726282119751 train acc: 0.9541666507720947 test loss: 1.322263479232788 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2209 train loss: 1.088878870010376 train acc: 0.9541666507720947 test loss: 1.3056540489196777 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2210 train loss: 1.0888673067092896 train acc: 0.9541666507720947 test loss: 1.3218097686767578 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2211 train loss: 1.088687777519226 train acc: 0.9541666507720947 test loss: 1.3288408517837524 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2212 train loss: 1.0886986255645752 train acc: 0.9541666507720947 test loss: 1.3161780834197998 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2213 train loss: 1.0889523029327393 train acc: 0.9541666507720947 test loss: 1.327020287513733 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2214 train loss: 1.0888500213623047 train acc: 0.9541666507720947 test loss: 1.3140391111373901 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2215 train loss: 1.0889151096343994 train acc: 0.9541666507720947 test loss: 1.3247780799865723 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2216 train loss: 1.0887465476989746 train acc: 0.9541666507720947 test loss: 1.3359410762786865 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2217 train loss: 1.0889244079589844 train acc: 0.9541666507720947 test loss: 1.3057841062545776 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2218 train loss: 1.0888015031814575 train acc: 0.9541666507720947 test loss: 1.3328028917312622 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2219 train loss: 1.0886304378509521 train acc: 0.9541666507720947 test loss: 1.331153392791748 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2220 train loss: 1.0885586738586426 train acc: 0.9541666507720947 test loss: 1.313860535621643 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2221 train loss: 1.0889389514923096 train acc: 0.9541666507720947 test loss: 1.3166162967681885 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2222 train loss: 1.0887244939804077 train acc: 0.9541666507720947 test loss: 1.3192278146743774 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2223 train loss: 1.088869333267212 train acc: 0.9541666507720947 test loss: 1.3337620496749878 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2224 train loss: 1.0888168811798096 train acc: 0.9541666507720947 test loss: 1.3164507150650024 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2225 train loss: 1.0885632038116455 train acc: 0.9541666507720947 test loss: 1.31413733959198 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2226 train loss: 1.0888258218765259 train acc: 0.9541666507720947 test loss: 1.3143516778945923 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2227 train loss: 1.0889456272125244 train acc: 0.9541666507720947 test loss: 1.3037978410720825 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2228 train loss: 1.0889447927474976 train acc: 0.9541666507720947 test loss: 1.3207448720932007 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2229 train loss: 1.0889555215835571 train acc: 0.9541666507720947 test loss: 1.2947826385498047 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2230 train loss: 1.0889708995819092 train acc: 0.9541666507720947 test loss: 1.3301109075546265 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2231 train loss: 1.088541030883789 train acc: 0.9541666507720947 test loss: 1.3127400875091553 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2232 train loss: 1.0887935161590576 train acc: 0.9541666507720947 test loss: 1.329380750656128 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2233 train loss: 1.08888840675354 train acc: 0.9541666507720947 test loss: 1.3044941425323486 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2234 train loss: 1.0888140201568604 train acc: 0.9541666507720947 test loss: 1.3222492933273315 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2235 train loss: 1.0888625383377075 train acc: 0.9541666507720947 test loss: 1.3186153173446655 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2236 train loss: 1.0889308452606201 train acc: 0.9541666507720947 test loss: 1.3053364753723145 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2237 train loss: 1.088639736175537 train acc: 0.9541666507720947 test loss: 1.3315989971160889 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2238 train loss: 1.0886985063552856 train acc: 0.9541666507720947 test loss: 1.3159480094909668 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2239 train loss: 1.0889105796813965 train acc: 0.9541666507720947 test loss: 1.3224499225616455 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2240 train loss: 1.0892720222473145 train acc: 0.9541666507720947 test loss: 1.3170738220214844 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2241 train loss: 1.0888651609420776 train acc: 0.9541666507720947 test loss: 1.3188127279281616 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2242 train loss: 1.0885958671569824 train acc: 0.9541666507720947 test loss: 1.3164292573928833 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2243 train loss: 1.0885009765625 train acc: 0.9541666507720947 test loss: 1.3303734064102173 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2244 train loss: 1.08888578414917 train acc: 0.9541666507720947 test loss: 1.3078420162200928 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2245 train loss: 1.0888742208480835 train acc: 0.9541666507720947 test loss: 1.3227636814117432 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2246 train loss: 1.0888422727584839 train acc: 0.9541666507720947 test loss: 1.2997727394104004 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2247 train loss: 1.0888879299163818 train acc: 0.9541666507720947 test loss: 1.302794098854065 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2248 train loss: 1.0888723134994507 train acc: 0.9541666507720947 test loss: 1.304590106010437 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2249 train loss: 1.0889251232147217 train acc: 0.9541666507720947 test loss: 1.3211013078689575 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2250 train loss: 1.0887713432312012 train acc: 0.9541666507720947 test loss: 1.3307698965072632 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2251 train loss: 1.0889312028884888 train acc: 0.9541666507720947 test loss: 1.3079546689987183 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2252 train loss: 1.0888407230377197 train acc: 0.9541666507720947 test loss: 1.3171961307525635 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2253 train loss: 1.0888513326644897 train acc: 0.9541666507720947 test loss: 1.3190582990646362 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2254 train loss: 1.0886012315750122 train acc: 0.9541666507720947 test loss: 1.3327083587646484 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2255 train loss: 1.0884815454483032 train acc: 0.9541666507720947 test loss: 1.3097094297409058 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2256 train loss: 1.0886256694793701 train acc: 0.9541666507720947 test loss: 1.3046718835830688 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2257 train loss: 1.0886727571487427 train acc: 0.9541666507720947 test loss: 1.3053220510482788 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2258 train loss: 1.088658094406128 train acc: 0.9541666507720947 test loss: 1.302364706993103 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2259 train loss: 1.0886378288269043 train acc: 0.9541666507720947 test loss: 1.311524748802185 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2260 train loss: 1.0888158082962036 train acc: 0.9541666507720947 test loss: 1.3147265911102295 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2261 train loss: 1.0887696743011475 train acc: 0.9541666507720947 test loss: 1.3243014812469482 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2262 train loss: 1.0888665914535522 train acc: 0.9541666507720947 test loss: 1.3298001289367676 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2263 train loss: 1.0886855125427246 train acc: 0.9541666507720947 test loss: 1.3295321464538574 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2264 train loss: 1.0889126062393188 train acc: 0.9541666507720947 test loss: 1.3217206001281738 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2265 train loss: 1.0887353420257568 train acc: 0.9541666507720947 test loss: 1.3305928707122803 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2266 train loss: 1.0888216495513916 train acc: 0.9541666507720947 test loss: 1.3124396800994873 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2267 train loss: 1.0887963771820068 train acc: 0.9541666507720947 test loss: 1.3210517168045044 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2268 train loss: 1.0886038541793823 train acc: 0.9541666507720947 test loss: 1.3152397871017456 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2269 train loss: 1.0886800289154053 train acc: 0.9541666507720947 test loss: 1.304864525794983 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2270 train loss: 1.0887000560760498 train acc: 0.9541666507720947 test loss: 1.3116365671157837 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2271 train loss: 1.0887296199798584 train acc: 0.9541666507720947 test loss: 1.3071941137313843 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2272 train loss: 1.0889396667480469 train acc: 0.9541666507720947 test loss: 1.3089230060577393 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2273 train loss: 1.0889126062393188 train acc: 0.9541666507720947 test loss: 1.3160806894302368 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2274 train loss: 1.0888025760650635 train acc: 0.9541666507720947 test loss: 1.3278157711029053 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2275 train loss: 1.0888316631317139 train acc: 0.9541666507720947 test loss: 1.3119508028030396 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2276 train loss: 1.0886974334716797 train acc: 0.9541666507720947 test loss: 1.340245008468628 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2277 train loss: 1.088745355606079 train acc: 0.9541666507720947 test loss: 1.3034741878509521 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2278 train loss: 1.0890040397644043 train acc: 0.9541666507720947 test loss: 1.3284175395965576 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2279 train loss: 1.0887696743011475 train acc: 0.9541666507720947 test loss: 1.3012808561325073 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2280 train loss: 1.0888203382492065 train acc: 0.9541666507720947 test loss: 1.3197062015533447 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2281 train loss: 1.089061975479126 train acc: 0.9541666507720947 test loss: 1.3275656700134277 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2282 train loss: 1.088742733001709 train acc: 0.9541666507720947 test loss: 1.3056374788284302 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2283 train loss: 1.0887969732284546 train acc: 0.9541666507720947 test loss: 1.3113560676574707 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2284 train loss: 1.0886286497116089 train acc: 0.9541666507720947 test loss: 1.3152974843978882 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2285 train loss: 1.0887351036071777 train acc: 0.9541666507720947 test loss: 1.3083196878433228 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2286 train loss: 1.0888683795928955 train acc: 0.9541666507720947 test loss: 1.3083298206329346 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2287 train loss: 1.0888614654541016 train acc: 0.9541666507720947 test loss: 1.3085637092590332 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2288 train loss: 1.0885597467422485 train acc: 0.9541666507720947 test loss: 1.2951017618179321 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2289 train loss: 1.0888183116912842 train acc: 0.9541666507720947 test loss: 1.3119862079620361 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2290 train loss: 1.088814377784729 train acc: 0.9541666507720947 test loss: 1.3275500535964966 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2291 train loss: 1.0888856649398804 train acc: 0.9541666507720947 test loss: 1.3102781772613525 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2292 train loss: 1.088613748550415 train acc: 0.9541666507720947 test loss: 1.3061517477035522 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2293 train loss: 1.088446021080017 train acc: 0.9541666507720947 test loss: 1.3128302097320557 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2294 train loss: 1.0890414714813232 train acc: 0.9541666507720947 test loss: 1.3176288604736328 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2295 train loss: 1.0888314247131348 train acc: 0.9541666507720947 test loss: 1.3238613605499268 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2296 train loss: 1.0887421369552612 train acc: 0.9541666507720947 test loss: 1.303715467453003 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2297 train loss: 1.0887432098388672 train acc: 0.9541666507720947 test loss: 1.3204196691513062 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2298 train loss: 1.0887929201126099 train acc: 0.9541666507720947 test loss: 1.300572156906128 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2299 train loss: 1.0886268615722656 train acc: 0.9541666507720947 test loss: 1.3354650735855103 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2300 train loss: 1.0887786149978638 train acc: 0.9541666507720947 test loss: 1.303093671798706 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2301 train loss: 1.088643193244934 train acc: 0.9541666507720947 test loss: 1.329527735710144 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2302 train loss: 1.0887947082519531 train acc: 0.9541666507720947 test loss: 1.3276578187942505 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2303 train loss: 1.0887402296066284 train acc: 0.9541666507720947 test loss: 1.3161967992782593 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2304 train loss: 1.0887240171432495 train acc: 0.9541666507720947 test loss: 1.3200491666793823 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2305 train loss: 1.0886762142181396 train acc: 0.9541666507720947 test loss: 1.3212336301803589 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2306 train loss: 1.088785171508789 train acc: 0.9541666507720947 test loss: 1.3001809120178223 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2307 train loss: 1.0887879133224487 train acc: 0.9541666507720947 test loss: 1.3236887454986572 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2308 train loss: 1.0888563394546509 train acc: 0.9541666507720947 test loss: 1.3138504028320312 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2309 train loss: 1.0887283086776733 train acc: 0.9541666507720947 test loss: 1.3106560707092285 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2310 train loss: 1.0890223979949951 train acc: 0.9541666507720947 test loss: 1.3244751691818237 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2311 train loss: 1.0888116359710693 train acc: 0.9541666507720947 test loss: 1.3166099786758423 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2312 train loss: 1.0888761281967163 train acc: 0.9541666507720947 test loss: 1.325099229812622 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2313 train loss: 1.0887905359268188 train acc: 0.9541666507720947 test loss: 1.3190220594406128 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2314 train loss: 1.0886800289154053 train acc: 0.9541666507720947 test loss: 1.3239768743515015 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2315 train loss: 1.0886088609695435 train acc: 0.9541666507720947 test loss: 1.3236355781555176 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2316 train loss: 1.0888535976409912 train acc: 0.9541666507720947 test loss: 1.3093019723892212 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2317 train loss: 1.088804006576538 train acc: 0.9541666507720947 test loss: 1.3196338415145874 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2318 train loss: 1.0886523723602295 train acc: 0.9541666507720947 test loss: 1.3085750341415405 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2319 train loss: 1.0886712074279785 train acc: 0.9541666507720947 test loss: 1.326617956161499 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2320 train loss: 1.0888631343841553 train acc: 0.9541666507720947 test loss: 1.3110471963882446 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2321 train loss: 1.0887705087661743 train acc: 0.9541666507720947 test loss: 1.3283350467681885 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2322 train loss: 1.0885285139083862 train acc: 0.9541666507720947 test loss: 1.3234667778015137 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2323 train loss: 1.0887006521224976 train acc: 0.9541666507720947 test loss: 1.3247523307800293 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2324 train loss: 1.089274287223816 train acc: 0.9541666507720947 test loss: 1.3203186988830566 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2325 train loss: 1.0887712240219116 train acc: 0.9541666507720947 test loss: 1.306512475013733 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2326 train loss: 1.0891469717025757 train acc: 0.9541666507720947 test loss: 1.3208422660827637 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2327 train loss: 1.0887749195098877 train acc: 0.9541666507720947 test loss: 1.3194992542266846 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2328 train loss: 1.088826298713684 train acc: 0.9541666507720947 test loss: 1.3414998054504395 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2329 train loss: 1.0887908935546875 train acc: 0.9541666507720947 test loss: 1.3075096607208252 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2330 train loss: 1.0886969566345215 train acc: 0.9541666507720947 test loss: 1.3237894773483276 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2331 train loss: 1.0887089967727661 train acc: 0.9541666507720947 test loss: 1.3259655237197876 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2332 train loss: 1.0887534618377686 train acc: 0.9541666507720947 test loss: 1.333577275276184 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2333 train loss: 1.0887558460235596 train acc: 0.9541666507720947 test loss: 1.3260623216629028 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2334 train loss: 1.0888779163360596 train acc: 0.9541666507720947 test loss: 1.3284896612167358 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2335 train loss: 1.0890464782714844 train acc: 0.9541666507720947 test loss: 1.3279197216033936 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2336 train loss: 1.0885640382766724 train acc: 0.9541666507720947 test loss: 1.342632532119751 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2337 train loss: 1.088830590248108 train acc: 0.9541666507720947 test loss: 1.324053168296814 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2338 train loss: 1.0889465808868408 train acc: 0.9541666507720947 test loss: 1.3182467222213745 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2339 train loss: 1.0887694358825684 train acc: 0.9541666507720947 test loss: 1.3213223218917847 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2340 train loss: 1.088836669921875 train acc: 0.9541666507720947 test loss: 1.3328075408935547 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2341 train loss: 1.0887227058410645 train acc: 0.9541666507720947 test loss: 1.3245255947113037 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2342 train loss: 1.088804841041565 train acc: 0.9541666507720947 test loss: 1.3131802082061768 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2343 train loss: 1.0887571573257446 train acc: 0.9541666507720947 test loss: 1.3221714496612549 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2344 train loss: 1.0888056755065918 train acc: 0.9541666507720947 test loss: 1.322036623954773 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2345 train loss: 1.0888450145721436 train acc: 0.9541666507720947 test loss: 1.2970187664031982 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2346 train loss: 1.0895768404006958 train acc: 0.9541666507720947 test loss: 1.3160399198532104 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2347 train loss: 1.0889091491699219 train acc: 0.9541666507720947 test loss: 1.3441025018692017 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2348 train loss: 1.0886733531951904 train acc: 0.9541666507720947 test loss: 1.328123688697815 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2349 train loss: 1.088679552078247 train acc: 0.9541666507720947 test loss: 1.3225029706954956 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2350 train loss: 1.0887418985366821 train acc: 0.9541666507720947 test loss: 1.3080629110336304 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2351 train loss: 1.0890562534332275 train acc: 0.9541666507720947 test loss: 1.3173201084136963 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2352 train loss: 1.0889954566955566 train acc: 0.9541666507720947 test loss: 1.3023093938827515 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2353 train loss: 1.0888524055480957 train acc: 0.9541666507720947 test loss: 1.3379734754562378 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2354 train loss: 1.0887888669967651 train acc: 0.9541666507720947 test loss: 1.313352108001709 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2355 train loss: 1.0888283252716064 train acc: 0.9541666507720947 test loss: 1.3317166566848755 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2356 train loss: 1.0886565446853638 train acc: 0.9541666507720947 test loss: 1.3273301124572754 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2357 train loss: 1.0888172388076782 train acc: 0.9541666507720947 test loss: 1.3429453372955322 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2358 train loss: 1.0888245105743408 train acc: 0.9541666507720947 test loss: 1.3332699537277222 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2359 train loss: 1.0888924598693848 train acc: 0.9541666507720947 test loss: 1.3188782930374146 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2360 train loss: 1.0886956453323364 train acc: 0.9541666507720947 test loss: 1.333140254020691 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2361 train loss: 1.0887802839279175 train acc: 0.9541666507720947 test loss: 1.3200911283493042 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2362 train loss: 1.0887531042099 train acc: 0.9541666507720947 test loss: 1.3271081447601318 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2363 train loss: 1.0888168811798096 train acc: 0.9541666507720947 test loss: 1.326650857925415 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2364 train loss: 1.0888495445251465 train acc: 0.9541666507720947 test loss: 1.3146030902862549 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2365 train loss: 1.0886510610580444 train acc: 0.9541666507720947 test loss: 1.3252805471420288 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2366 train loss: 1.0887552499771118 train acc: 0.9541666507720947 test loss: 1.3182222843170166 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2367 train loss: 1.0884076356887817 train acc: 0.9541666507720947 test loss: 1.297607183456421 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2368 train loss: 1.088993787765503 train acc: 0.9541666507720947 test loss: 1.3120453357696533 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2369 train loss: 1.0888581275939941 train acc: 0.9541666507720947 test loss: 1.3397282361984253 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2370 train loss: 1.0887409448623657 train acc: 0.9541666507720947 test loss: 1.322546124458313 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2371 train loss: 1.0887341499328613 train acc: 0.9541666507720947 test loss: 1.3320915699005127 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2372 train loss: 1.0885859727859497 train acc: 0.9541666507720947 test loss: 1.3301546573638916 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2373 train loss: 1.0889381170272827 train acc: 0.9541666507720947 test loss: 1.3320565223693848 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2374 train loss: 1.088707447052002 train acc: 0.9541666507720947 test loss: 1.3170266151428223 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2375 train loss: 1.0887930393218994 train acc: 0.9541666507720947 test loss: 1.3231033086776733 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2376 train loss: 1.0889321565628052 train acc: 0.9541666507720947 test loss: 1.301856279373169 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2377 train loss: 1.0888303518295288 train acc: 0.9541666507720947 test loss: 1.3046492338180542 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2378 train loss: 1.088983416557312 train acc: 0.9541666507720947 test loss: 1.3118706941604614 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2379 train loss: 1.0887895822525024 train acc: 0.9541666507720947 test loss: 1.321738839149475 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2380 train loss: 1.088737964630127 train acc: 0.9541666507720947 test loss: 1.3201134204864502 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2381 train loss: 1.0887500047683716 train acc: 0.9541666507720947 test loss: 1.333752989768982 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2382 train loss: 1.0887538194656372 train acc: 0.9541666507720947 test loss: 1.3282219171524048 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2383 train loss: 1.088866949081421 train acc: 0.9541666507720947 test loss: 1.3164598941802979 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2384 train loss: 1.088821291923523 train acc: 0.9541666507720947 test loss: 1.3199801445007324 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2385 train loss: 1.0887924432754517 train acc: 0.9541666507720947 test loss: 1.3230164051055908 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2386 train loss: 1.0889149904251099 train acc: 0.9541666507720947 test loss: 1.3116799592971802 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2387 train loss: 1.0889687538146973 train acc: 0.9541666507720947 test loss: 1.3193172216415405 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2388 train loss: 1.088602900505066 train acc: 0.9541666507720947 test loss: 1.3312382698059082 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2389 train loss: 1.0889633893966675 train acc: 0.9541666507720947 test loss: 1.3067227602005005 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2390 train loss: 1.0889313220977783 train acc: 0.9541666507720947 test loss: 1.322493076324463 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2391 train loss: 1.088915228843689 train acc: 0.9541666507720947 test loss: 1.3306165933609009 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2392 train loss: 1.0887702703475952 train acc: 0.9541666507720947 test loss: 1.3477853536605835 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2393 train loss: 1.0888932943344116 train acc: 0.9541666507720947 test loss: 1.296438455581665 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2394 train loss: 1.0885272026062012 train acc: 0.9541666507720947 test loss: 1.3161698579788208 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2395 train loss: 1.088828444480896 train acc: 0.9541666507720947 test loss: 1.332083821296692 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2396 train loss: 1.088672399520874 train acc: 0.9541666507720947 test loss: 1.325290322303772 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2397 train loss: 1.0886479616165161 train acc: 0.9541666507720947 test loss: 1.2960047721862793 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 2398 train loss: 1.0887163877487183 train acc: 0.9541666507720947 test loss: 1.3373119831085205 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2399 train loss: 1.0887798070907593 train acc: 0.9541666507720947 test loss: 1.3201590776443481 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2400 train loss: 1.0886794328689575 train acc: 0.9541666507720947 test loss: 1.3175036907196045 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2401 train loss: 1.0890694856643677 train acc: 0.9541666507720947 test loss: 1.3208448886871338 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2402 train loss: 1.0886235237121582 train acc: 0.9541666507720947 test loss: 1.3265488147735596 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2403 train loss: 1.088767409324646 train acc: 0.9541666507720947 test loss: 1.335869312286377 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2404 train loss: 1.0887118577957153 train acc: 0.9541666507720947 test loss: 1.3236106634140015 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2405 train loss: 1.0887157917022705 train acc: 0.9541666507720947 test loss: 1.3260884284973145 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2406 train loss: 1.0888718366622925 train acc: 0.9541666507720947 test loss: 1.303916573524475 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2407 train loss: 1.0887646675109863 train acc: 0.9541666507720947 test loss: 1.3220278024673462 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2408 train loss: 1.0888205766677856 train acc: 0.9541666507720947 test loss: 1.3024916648864746 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2409 train loss: 1.0888221263885498 train acc: 0.9541666507720947 test loss: 1.3235498666763306 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2410 train loss: 1.0887234210968018 train acc: 0.9541666507720947 test loss: 1.3300518989562988 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2411 train loss: 1.0888311862945557 train acc: 0.9541666507720947 test loss: 1.3265042304992676 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2412 train loss: 1.088678002357483 train acc: 0.9541666507720947 test loss: 1.3155231475830078 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2413 train loss: 1.0886626243591309 train acc: 0.9541666507720947 test loss: 1.30793035030365 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2414 train loss: 1.0888609886169434 train acc: 0.9541666507720947 test loss: 1.3324412107467651 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2415 train loss: 1.0889235734939575 train acc: 0.9541666507720947 test loss: 1.3247870206832886 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2416 train loss: 1.0888112783432007 train acc: 0.9541666507720947 test loss: 1.3148564100265503 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2417 train loss: 1.0887513160705566 train acc: 0.9541666507720947 test loss: 1.3246618509292603 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2418 train loss: 1.0885906219482422 train acc: 0.9541666507720947 test loss: 1.3205935955047607 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2419 train loss: 1.08867609500885 train acc: 0.9541666507720947 test loss: 1.3281781673431396 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2420 train loss: 1.088670253753662 train acc: 0.9541666507720947 test loss: 1.3360326290130615 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2421 train loss: 1.0888123512268066 train acc: 0.9541666507720947 test loss: 1.3224451541900635 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2422 train loss: 1.0890166759490967 train acc: 0.9541666507720947 test loss: 1.305890679359436 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2423 train loss: 1.088785171508789 train acc: 0.9541666507720947 test loss: 1.3335347175598145 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2424 train loss: 1.088632583618164 train acc: 0.9541666507720947 test loss: 1.317214846611023 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2425 train loss: 1.0889519453048706 train acc: 0.9541666507720947 test loss: 1.3328843116760254 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2426 train loss: 1.0886858701705933 train acc: 0.9541666507720947 test loss: 1.3341585397720337 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2427 train loss: 1.08895742893219 train acc: 0.9541666507720947 test loss: 1.3270858526229858 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2428 train loss: 1.0889294147491455 train acc: 0.9541666507720947 test loss: 1.3200305700302124 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2429 train loss: 1.0887433290481567 train acc: 0.9541666507720947 test loss: 1.343674898147583 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2430 train loss: 1.0887607336044312 train acc: 0.9541666507720947 test loss: 1.3221344947814941 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2431 train loss: 1.0886839628219604 train acc: 0.9541666507720947 test loss: 1.3263404369354248 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2432 train loss: 1.088687539100647 train acc: 0.9541666507720947 test loss: 1.3231500387191772 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2433 train loss: 1.0886200666427612 train acc: 0.9541666507720947 test loss: 1.3428407907485962 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2434 train loss: 1.0887175798416138 train acc: 0.9541666507720947 test loss: 1.305385708808899 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2435 train loss: 1.088636875152588 train acc: 0.9541666507720947 test loss: 1.3160961866378784 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2436 train loss: 1.0886085033416748 train acc: 0.9541666507720947 test loss: 1.3073763847351074 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2437 train loss: 1.0889631509780884 train acc: 0.9541666507720947 test loss: 1.3187364339828491 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2438 train loss: 1.0889034271240234 train acc: 0.9541666507720947 test loss: 1.3297306299209595 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2439 train loss: 1.0888725519180298 train acc: 0.9541666507720947 test loss: 1.3031808137893677 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2440 train loss: 1.0889016389846802 train acc: 0.9541666507720947 test loss: 1.317981481552124 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2441 train loss: 1.0886701345443726 train acc: 0.9541666507720947 test loss: 1.3229721784591675 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2442 train loss: 1.0887476205825806 train acc: 0.9541666507720947 test loss: 1.3232256174087524 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2443 train loss: 1.0885714292526245 train acc: 0.9541666507720947 test loss: 1.3167996406555176 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2444 train loss: 1.0886203050613403 train acc: 0.9541666507720947 test loss: 1.3278108835220337 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2445 train loss: 1.0886750221252441 train acc: 0.9541666507720947 test loss: 1.3174346685409546 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2446 train loss: 1.0885789394378662 train acc: 0.9541666507720947 test loss: 1.3248560428619385 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2447 train loss: 1.0887824296951294 train acc: 0.9541666507720947 test loss: 1.324236273765564 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2448 train loss: 1.088752031326294 train acc: 0.9541666507720947 test loss: 1.3389543294906616 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2449 train loss: 1.0885510444641113 train acc: 0.9541666507720947 test loss: 1.3158290386199951 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2450 train loss: 1.0886098146438599 train acc: 0.9541666507720947 test loss: 1.331037998199463 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2451 train loss: 1.0884928703308105 train acc: 0.9541666507720947 test loss: 1.321898341178894 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2452 train loss: 1.0887235403060913 train acc: 0.9541666507720947 test loss: 1.3200438022613525 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2453 train loss: 1.088880181312561 train acc: 0.9541666507720947 test loss: 1.323784589767456 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2454 train loss: 1.0887088775634766 train acc: 0.9541666507720947 test loss: 1.314426302909851 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2455 train loss: 1.0885355472564697 train acc: 0.9541666507720947 test loss: 1.3141560554504395 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2456 train loss: 1.088898777961731 train acc: 0.9541666507720947 test loss: 1.3337171077728271 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2457 train loss: 1.0887900590896606 train acc: 0.9541666507720947 test loss: 1.3181341886520386 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2458 train loss: 1.0888733863830566 train acc: 0.9541666507720947 test loss: 1.3292183876037598 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2459 train loss: 1.0886785984039307 train acc: 0.9541666507720947 test loss: 1.3292213678359985 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2460 train loss: 1.0887339115142822 train acc: 0.9541666507720947 test loss: 1.3280948400497437 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2461 train loss: 1.0887751579284668 train acc: 0.9541666507720947 test loss: 1.3322316408157349 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2462 train loss: 1.088704228401184 train acc: 0.9541666507720947 test loss: 1.309908390045166 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2463 train loss: 1.0888445377349854 train acc: 0.9541666507720947 test loss: 1.3404810428619385 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2464 train loss: 1.0888748168945312 train acc: 0.9541666507720947 test loss: 1.3363416194915771 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2465 train loss: 1.08888840675354 train acc: 0.9541666507720947 test loss: 1.336959958076477 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2466 train loss: 1.0888289213180542 train acc: 0.9541666507720947 test loss: 1.3146612644195557 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2467 train loss: 1.0888161659240723 train acc: 0.9541666507720947 test loss: 1.3399548530578613 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2468 train loss: 1.0887494087219238 train acc: 0.9541666507720947 test loss: 1.3106340169906616 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2469 train loss: 1.0887386798858643 train acc: 0.9541666507720947 test loss: 1.3316441774368286 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2470 train loss: 1.0887678861618042 train acc: 0.9541666507720947 test loss: 1.3182882070541382 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2471 train loss: 1.0886456966400146 train acc: 0.9541666507720947 test loss: 1.3162000179290771 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2472 train loss: 1.0888142585754395 train acc: 0.9541666507720947 test loss: 1.3251439332962036 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2473 train loss: 1.0886849164962769 train acc: 0.9541666507720947 test loss: 1.3133991956710815 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2474 train loss: 1.0887031555175781 train acc: 0.9541666507720947 test loss: 1.3223233222961426 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2475 train loss: 1.088729739189148 train acc: 0.9541666507720947 test loss: 1.3234294652938843 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2476 train loss: 1.088755488395691 train acc: 0.9541666507720947 test loss: 1.3176155090332031 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2477 train loss: 1.0888231992721558 train acc: 0.9541666507720947 test loss: 1.3308182954788208 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2478 train loss: 1.0886634588241577 train acc: 0.9541666507720947 test loss: 1.3434604406356812 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2479 train loss: 1.0884811878204346 train acc: 0.9541666507720947 test loss: 1.326770544052124 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2480 train loss: 1.0888034105300903 train acc: 0.9541666507720947 test loss: 1.313124418258667 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2481 train loss: 1.0889045000076294 train acc: 0.9541666507720947 test loss: 1.3546898365020752 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2482 train loss: 1.088807225227356 train acc: 0.9541666507720947 test loss: 1.3215891122817993 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2483 train loss: 1.0889251232147217 train acc: 0.9541666507720947 test loss: 1.3166279792785645 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2484 train loss: 1.0887452363967896 train acc: 0.9541666507720947 test loss: 1.323046088218689 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2485 train loss: 1.0886703729629517 train acc: 0.9541666507720947 test loss: 1.3122104406356812 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2486 train loss: 1.0886378288269043 train acc: 0.9541666507720947 test loss: 1.3055164813995361 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2487 train loss: 1.0887423753738403 train acc: 0.9541666507720947 test loss: 1.329681158065796 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2488 train loss: 1.0887072086334229 train acc: 0.9541666507720947 test loss: 1.3263914585113525 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2489 train loss: 1.0890381336212158 train acc: 0.9541666507720947 test loss: 1.3181838989257812 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2490 train loss: 1.0885899066925049 train acc: 0.9541666507720947 test loss: 1.3253118991851807 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2491 train loss: 1.0888224840164185 train acc: 0.9541666507720947 test loss: 1.3211911916732788 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2492 train loss: 1.0886740684509277 train acc: 0.9541666507720947 test loss: 1.3182005882263184 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2493 train loss: 1.0887408256530762 train acc: 0.9541666507720947 test loss: 1.338043451309204 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2494 train loss: 1.088592767715454 train acc: 0.9541666507720947 test loss: 1.3334108591079712 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2495 train loss: 1.0891543626785278 train acc: 0.9541666507720947 test loss: 1.3138384819030762 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2496 train loss: 1.0888229608535767 train acc: 0.9541666507720947 test loss: 1.3309295177459717 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2497 train loss: 1.0887000560760498 train acc: 0.9541666507720947 test loss: 1.3140032291412354 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2498 train loss: 1.088639497756958 train acc: 0.9541666507720947 test loss: 1.309635877609253 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2499 train loss: 1.0887844562530518 train acc: 0.9541666507720947 test loss: 1.343442440032959 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2500 train loss: 1.0885525941848755 train acc: 0.9541666507720947 test loss: 1.3313695192337036 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2501 train loss: 1.0887932777404785 train acc: 0.9541666507720947 test loss: 1.3282774686813354 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2502 train loss: 1.0882987976074219 train acc: 0.9541666507720947 test loss: 1.3216698169708252 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2503 train loss: 1.0888992547988892 train acc: 0.9541666507720947 test loss: 1.3149161338806152 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2504 train loss: 1.088813066482544 train acc: 0.9541666507720947 test loss: 1.319644570350647 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2505 train loss: 1.0888115167617798 train acc: 0.9541666507720947 test loss: 1.3186404705047607 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2506 train loss: 1.0886574983596802 train acc: 0.9541666507720947 test loss: 1.317008376121521 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2507 train loss: 1.0886095762252808 train acc: 0.9541666507720947 test loss: 1.3127899169921875 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2508 train loss: 1.0887445211410522 train acc: 0.9541666507720947 test loss: 1.3148480653762817 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2509 train loss: 1.0886151790618896 train acc: 0.9541666507720947 test loss: 1.3204530477523804 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2510 train loss: 1.0887770652770996 train acc: 0.9541666507720947 test loss: 1.3004372119903564 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2511 train loss: 1.0887736082077026 train acc: 0.9541666507720947 test loss: 1.3239705562591553 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2512 train loss: 1.088423252105713 train acc: 0.9541666507720947 test loss: 1.3192440271377563 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2513 train loss: 1.0886340141296387 train acc: 0.9541666507720947 test loss: 1.3353224992752075 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2514 train loss: 1.0885728597640991 train acc: 0.9541666507720947 test loss: 1.3170859813690186 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2515 train loss: 1.0883219242095947 train acc: 0.9541666507720947 test loss: 1.3337324857711792 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2516 train loss: 1.0872802734375 train acc: 0.956250011920929 test loss: 1.3181909322738647 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2517 train loss: 1.0876401662826538 train acc: 0.956250011920929 test loss: 1.322899341583252 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2518 train loss: 1.0879864692687988 train acc: 0.9541666507720947 test loss: 1.3070539236068726 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2519 train loss: 1.0874336957931519 train acc: 0.956250011920929 test loss: 1.3324062824249268 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2520 train loss: 1.0872970819473267 train acc: 0.956250011920929 test loss: 1.3163002729415894 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2521 train loss: 1.0872061252593994 train acc: 0.956250011920929 test loss: 1.3321231603622437 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2522 train loss: 1.0871937274932861 train acc: 0.956250011920929 test loss: 1.3335992097854614 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2523 train loss: 1.0869793891906738 train acc: 0.956250011920929 test loss: 1.3177300691604614 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2524 train loss: 1.0871368646621704 train acc: 0.956250011920929 test loss: 1.307952642440796 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2525 train loss: 1.0869969129562378 train acc: 0.956250011920929 test loss: 1.3196481466293335 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2526 train loss: 1.0870475769042969 train acc: 0.956250011920929 test loss: 1.3320149183273315 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2527 train loss: 1.0872973203659058 train acc: 0.956250011920929 test loss: 1.3167184591293335 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2528 train loss: 1.0870249271392822 train acc: 0.956250011920929 test loss: 1.3216050863265991 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2529 train loss: 1.0867769718170166 train acc: 0.956250011920929 test loss: 1.3346110582351685 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2530 train loss: 1.087079644203186 train acc: 0.956250011920929 test loss: 1.3183468580245972 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2531 train loss: 1.0868167877197266 train acc: 0.956250011920929 test loss: 1.3303807973861694 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2532 train loss: 1.086819052696228 train acc: 0.956250011920929 test loss: 1.350610375404358 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2533 train loss: 1.0867496728897095 train acc: 0.956250011920929 test loss: 1.3297199010849 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2534 train loss: 1.0868728160858154 train acc: 0.956250011920929 test loss: 1.310597538948059 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2535 train loss: 1.0868875980377197 train acc: 0.956250011920929 test loss: 1.3330246210098267 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2536 train loss: 1.0870124101638794 train acc: 0.956250011920929 test loss: 1.3410576581954956 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2537 train loss: 1.0869895219802856 train acc: 0.956250011920929 test loss: 1.322614312171936 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2538 train loss: 1.0868326425552368 train acc: 0.956250011920929 test loss: 1.3074493408203125 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2539 train loss: 1.0870766639709473 train acc: 0.956250011920929 test loss: 1.3202863931655884 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2540 train loss: 1.0870321989059448 train acc: 0.956250011920929 test loss: 1.3275809288024902 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2541 train loss: 1.0869247913360596 train acc: 0.956250011920929 test loss: 1.3183163404464722 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2542 train loss: 1.0868960618972778 train acc: 0.956250011920929 test loss: 1.3385592699050903 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2543 train loss: 1.086851954460144 train acc: 0.956250011920929 test loss: 1.3215590715408325 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2544 train loss: 1.0867862701416016 train acc: 0.956250011920929 test loss: 1.338036298751831 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2545 train loss: 1.0868009328842163 train acc: 0.956250011920929 test loss: 1.3501676321029663 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2546 train loss: 1.0869542360305786 train acc: 0.956250011920929 test loss: 1.3147234916687012 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2547 train loss: 1.0867478847503662 train acc: 0.956250011920929 test loss: 1.3289822340011597 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2548 train loss: 1.0868476629257202 train acc: 0.956250011920929 test loss: 1.3249062299728394 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2549 train loss: 1.086694598197937 train acc: 0.956250011920929 test loss: 1.313470721244812 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2550 train loss: 1.0869218111038208 train acc: 0.956250011920929 test loss: 1.3025730848312378 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2551 train loss: 1.0867244005203247 train acc: 0.956250011920929 test loss: 1.3226841688156128 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2552 train loss: 1.0868887901306152 train acc: 0.956250011920929 test loss: 1.3288196325302124 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2553 train loss: 1.0868542194366455 train acc: 0.956250011920929 test loss: 1.3264294862747192 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2554 train loss: 1.0869015455245972 train acc: 0.956250011920929 test loss: 1.3504180908203125 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2555 train loss: 1.0868892669677734 train acc: 0.956250011920929 test loss: 1.3394066095352173 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2556 train loss: 1.086610198020935 train acc: 0.956250011920929 test loss: 1.318697214126587 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2557 train loss: 1.0866897106170654 train acc: 0.956250011920929 test loss: 1.3332914113998413 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2558 train loss: 1.0866808891296387 train acc: 0.956250011920929 test loss: 1.3461378812789917 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2559 train loss: 1.0867482423782349 train acc: 0.956250011920929 test loss: 1.3454300165176392 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2560 train loss: 1.086679220199585 train acc: 0.956250011920929 test loss: 1.3185169696807861 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2561 train loss: 1.0868638753890991 train acc: 0.956250011920929 test loss: 1.3415268659591675 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2562 train loss: 1.0864980220794678 train acc: 0.956250011920929 test loss: 1.3221105337142944 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2563 train loss: 1.0869165658950806 train acc: 0.956250011920929 test loss: 1.31784987449646 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2564 train loss: 1.0866551399230957 train acc: 0.956250011920929 test loss: 1.334294080734253 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2565 train loss: 1.086651086807251 train acc: 0.956250011920929 test loss: 1.3283569812774658 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2566 train loss: 1.0866755247116089 train acc: 0.956250011920929 test loss: 1.3266083002090454 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2567 train loss: 1.087073564529419 train acc: 0.956250011920929 test loss: 1.3450348377227783 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2568 train loss: 1.086661696434021 train acc: 0.956250011920929 test loss: 1.3146235942840576 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2569 train loss: 1.0867997407913208 train acc: 0.956250011920929 test loss: 1.3080729246139526 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2570 train loss: 1.0867867469787598 train acc: 0.956250011920929 test loss: 1.3249396085739136 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2571 train loss: 1.086911916732788 train acc: 0.956250011920929 test loss: 1.3424601554870605 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2572 train loss: 1.087030291557312 train acc: 0.956250011920929 test loss: 1.3340511322021484 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2573 train loss: 1.0867764949798584 train acc: 0.956250011920929 test loss: 1.3176636695861816 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2574 train loss: 1.086807370185852 train acc: 0.956250011920929 test loss: 1.31423819065094 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2575 train loss: 1.0866245031356812 train acc: 0.956250011920929 test loss: 1.3165220022201538 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2576 train loss: 1.0866515636444092 train acc: 0.956250011920929 test loss: 1.344464898109436 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2577 train loss: 1.0866811275482178 train acc: 0.956250011920929 test loss: 1.3304636478424072 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2578 train loss: 1.086564302444458 train acc: 0.956250011920929 test loss: 1.326339840888977 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2579 train loss: 1.0866050720214844 train acc: 0.956250011920929 test loss: 1.3147084712982178 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2580 train loss: 1.0865472555160522 train acc: 0.956250011920929 test loss: 1.3300684690475464 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2581 train loss: 1.086756944656372 train acc: 0.956250011920929 test loss: 1.3315333127975464 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2582 train loss: 1.0867862701416016 train acc: 0.956250011920929 test loss: 1.339948296546936 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2583 train loss: 1.0867763757705688 train acc: 0.956250011920929 test loss: 1.3200668096542358 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2584 train loss: 1.0866063833236694 train acc: 0.956250011920929 test loss: 1.3225687742233276 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2585 train loss: 1.0865709781646729 train acc: 0.956250011920929 test loss: 1.32144296169281 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2586 train loss: 1.0868027210235596 train acc: 0.956250011920929 test loss: 1.3334556818008423 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2587 train loss: 1.0868308544158936 train acc: 0.956250011920929 test loss: 1.3573520183563232 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2588 train loss: 1.086814522743225 train acc: 0.956250011920929 test loss: 1.3214585781097412 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2589 train loss: 1.0868901014328003 train acc: 0.956250011920929 test loss: 1.343530297279358 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2590 train loss: 1.086605429649353 train acc: 0.956250011920929 test loss: 1.328356146812439 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2591 train loss: 1.0867868661880493 train acc: 0.956250011920929 test loss: 1.3271275758743286 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2592 train loss: 1.0866984128952026 train acc: 0.956250011920929 test loss: 1.327914834022522 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2593 train loss: 1.0869677066802979 train acc: 0.956250011920929 test loss: 1.3306251764297485 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2594 train loss: 1.086680293083191 train acc: 0.956250011920929 test loss: 1.3478604555130005 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2595 train loss: 1.0867668390274048 train acc: 0.956250011920929 test loss: 1.3232452869415283 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2596 train loss: 1.086744785308838 train acc: 0.956250011920929 test loss: 1.3227734565734863 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2597 train loss: 1.0866893529891968 train acc: 0.956250011920929 test loss: 1.3373371362686157 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2598 train loss: 1.086501955986023 train acc: 0.956250011920929 test loss: 1.3178291320800781 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2599 train loss: 1.0867606401443481 train acc: 0.956250011920929 test loss: 1.3289666175842285 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2600 train loss: 1.0867135524749756 train acc: 0.956250011920929 test loss: 1.320395588874817 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2601 train loss: 1.0868016481399536 train acc: 0.956250011920929 test loss: 1.3265488147735596 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2602 train loss: 1.086836338043213 train acc: 0.956250011920929 test loss: 1.3367588520050049 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2603 train loss: 1.0867424011230469 train acc: 0.956250011920929 test loss: 1.3380367755889893 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2604 train loss: 1.0867666006088257 train acc: 0.956250011920929 test loss: 1.3174972534179688 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2605 train loss: 1.0867894887924194 train acc: 0.956250011920929 test loss: 1.3306885957717896 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2606 train loss: 1.0868141651153564 train acc: 0.956250011920929 test loss: 1.311652660369873 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2607 train loss: 1.0867091417312622 train acc: 0.956250011920929 test loss: 1.3200631141662598 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2608 train loss: 1.0866615772247314 train acc: 0.956250011920929 test loss: 1.3378932476043701 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2609 train loss: 1.0867486000061035 train acc: 0.956250011920929 test loss: 1.3229420185089111 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2610 train loss: 1.0867226123809814 train acc: 0.956250011920929 test loss: 1.3088455200195312 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2611 train loss: 1.08655846118927 train acc: 0.956250011920929 test loss: 1.3264811038970947 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2612 train loss: 1.086977481842041 train acc: 0.956250011920929 test loss: 1.3310672044754028 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2613 train loss: 1.086870789527893 train acc: 0.956250011920929 test loss: 1.3231544494628906 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2614 train loss: 1.0867689847946167 train acc: 0.956250011920929 test loss: 1.3384649753570557 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2615 train loss: 1.0866947174072266 train acc: 0.956250011920929 test loss: 1.3340622186660767 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2616 train loss: 1.0867081880569458 train acc: 0.956250011920929 test loss: 1.3269070386886597 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2617 train loss: 1.086694359779358 train acc: 0.956250011920929 test loss: 1.327072262763977 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2618 train loss: 1.0867455005645752 train acc: 0.956250011920929 test loss: 1.3389341831207275 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2619 train loss: 1.0867860317230225 train acc: 0.956250011920929 test loss: 1.3254755735397339 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2620 train loss: 1.086713433265686 train acc: 0.956250011920929 test loss: 1.335506558418274 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2621 train loss: 1.086916446685791 train acc: 0.956250011920929 test loss: 1.3022620677947998 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2622 train loss: 1.0868428945541382 train acc: 0.956250011920929 test loss: 1.3210890293121338 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2623 train loss: 1.0867823362350464 train acc: 0.956250011920929 test loss: 1.318674921989441 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2624 train loss: 1.086848497390747 train acc: 0.956250011920929 test loss: 1.3267263174057007 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2625 train loss: 1.0866409540176392 train acc: 0.956250011920929 test loss: 1.3215693235397339 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2626 train loss: 1.0866550207138062 train acc: 0.956250011920929 test loss: 1.3321411609649658 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2627 train loss: 1.086761236190796 train acc: 0.956250011920929 test loss: 1.3249543905258179 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2628 train loss: 1.0866718292236328 train acc: 0.956250011920929 test loss: 1.3369027376174927 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2629 train loss: 1.0869057178497314 train acc: 0.956250011920929 test loss: 1.3359134197235107 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2630 train loss: 1.0867711305618286 train acc: 0.956250011920929 test loss: 1.3251357078552246 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2631 train loss: 1.0867441892623901 train acc: 0.956250011920929 test loss: 1.3221739530563354 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2632 train loss: 1.086714267730713 train acc: 0.956250011920929 test loss: 1.3321759700775146 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2633 train loss: 1.08670973777771 train acc: 0.956250011920929 test loss: 1.323089599609375 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2634 train loss: 1.0867979526519775 train acc: 0.956250011920929 test loss: 1.3227633237838745 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2635 train loss: 1.086789608001709 train acc: 0.956250011920929 test loss: 1.3234344720840454 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2636 train loss: 1.086858868598938 train acc: 0.956250011920929 test loss: 1.3237168788909912 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2637 train loss: 1.0869070291519165 train acc: 0.956250011920929 test loss: 1.3101508617401123 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2638 train loss: 1.0868934392929077 train acc: 0.956250011920929 test loss: 1.3490833044052124 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2639 train loss: 1.0866427421569824 train acc: 0.956250011920929 test loss: 1.3341251611709595 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2640 train loss: 1.0866904258728027 train acc: 0.956250011920929 test loss: 1.3275395631790161 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2641 train loss: 1.0867501497268677 train acc: 0.956250011920929 test loss: 1.3120354413986206 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2642 train loss: 1.0866889953613281 train acc: 0.956250011920929 test loss: 1.327696442604065 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2643 train loss: 1.0867723226547241 train acc: 0.956250011920929 test loss: 1.3383742570877075 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2644 train loss: 1.0868083238601685 train acc: 0.956250011920929 test loss: 1.34153413772583 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2645 train loss: 1.0866949558258057 train acc: 0.956250011920929 test loss: 1.3193219900131226 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2646 train loss: 1.086704969406128 train acc: 0.956250011920929 test loss: 1.333457112312317 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2647 train loss: 1.086817979812622 train acc: 0.956250011920929 test loss: 1.3391422033309937 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2648 train loss: 1.0867341756820679 train acc: 0.956250011920929 test loss: 1.3236116170883179 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2649 train loss: 1.0866429805755615 train acc: 0.956250011920929 test loss: 1.3365867137908936 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2650 train loss: 1.087040662765503 train acc: 0.956250011920929 test loss: 1.3425183296203613 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2651 train loss: 1.0866222381591797 train acc: 0.956250011920929 test loss: 1.3314013481140137 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2652 train loss: 1.0866105556488037 train acc: 0.956250011920929 test loss: 1.3122422695159912 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2653 train loss: 1.087032437324524 train acc: 0.956250011920929 test loss: 1.3313424587249756 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2654 train loss: 1.0867981910705566 train acc: 0.956250011920929 test loss: 1.3341023921966553 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2655 train loss: 1.0867875814437866 train acc: 0.956250011920929 test loss: 1.3184980154037476 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2656 train loss: 1.0869476795196533 train acc: 0.956250011920929 test loss: 1.3189247846603394 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2657 train loss: 1.086936593055725 train acc: 0.956250011920929 test loss: 1.3261513710021973 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2658 train loss: 1.0866180658340454 train acc: 0.956250011920929 test loss: 1.3264695405960083 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2659 train loss: 1.0866364240646362 train acc: 0.956250011920929 test loss: 1.3197486400604248 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2660 train loss: 1.0868862867355347 train acc: 0.956250011920929 test loss: 1.3223174810409546 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2661 train loss: 1.0865553617477417 train acc: 0.956250011920929 test loss: 1.3280586004257202 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2662 train loss: 1.0866316556930542 train acc: 0.956250011920929 test loss: 1.3123087882995605 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2663 train loss: 1.086815357208252 train acc: 0.956250011920929 test loss: 1.3144885301589966 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2664 train loss: 1.0868206024169922 train acc: 0.956250011920929 test loss: 1.3103911876678467 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2665 train loss: 1.0867222547531128 train acc: 0.956250011920929 test loss: 1.3204761743545532 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2666 train loss: 1.0867856740951538 train acc: 0.956250011920929 test loss: 1.331135630607605 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2667 train loss: 1.0869067907333374 train acc: 0.956250011920929 test loss: 1.3100687265396118 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2668 train loss: 1.0865588188171387 train acc: 0.956250011920929 test loss: 1.3004816770553589 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2669 train loss: 1.0867031812667847 train acc: 0.956250011920929 test loss: 1.3050334453582764 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2670 train loss: 1.086762547492981 train acc: 0.956250011920929 test loss: 1.314697265625 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2671 train loss: 1.0867304801940918 train acc: 0.956250011920929 test loss: 1.333997130393982 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2672 train loss: 1.086730718612671 train acc: 0.956250011920929 test loss: 1.3219934701919556 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2673 train loss: 1.0865867137908936 train acc: 0.956250011920929 test loss: 1.3179166316986084 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2674 train loss: 1.0868122577667236 train acc: 0.956250011920929 test loss: 1.2950224876403809 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2675 train loss: 1.0867754220962524 train acc: 0.956250011920929 test loss: 1.3264464139938354 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2676 train loss: 1.0866496562957764 train acc: 0.956250011920929 test loss: 1.3270533084869385 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2677 train loss: 1.0868382453918457 train acc: 0.956250011920929 test loss: 1.3196974992752075 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2678 train loss: 1.0866940021514893 train acc: 0.956250011920929 test loss: 1.31940495967865 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2679 train loss: 1.0867936611175537 train acc: 0.956250011920929 test loss: 1.3032050132751465 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2680 train loss: 1.0867516994476318 train acc: 0.956250011920929 test loss: 1.315529704093933 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2681 train loss: 1.0867860317230225 train acc: 0.956250011920929 test loss: 1.3082008361816406 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2682 train loss: 1.0867894887924194 train acc: 0.956250011920929 test loss: 1.3047219514846802 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2683 train loss: 1.0867359638214111 train acc: 0.956250011920929 test loss: 1.3428492546081543 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2684 train loss: 1.0867189168930054 train acc: 0.956250011920929 test loss: 1.306640386581421 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2685 train loss: 1.086701512336731 train acc: 0.956250011920929 test loss: 1.3353325128555298 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2686 train loss: 1.086767315864563 train acc: 0.956250011920929 test loss: 1.3258051872253418 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2687 train loss: 1.086714267730713 train acc: 0.956250011920929 test loss: 1.3216843605041504 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2688 train loss: 1.0866817235946655 train acc: 0.956250011920929 test loss: 1.335947036743164 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2689 train loss: 1.0865590572357178 train acc: 0.956250011920929 test loss: 1.3168256282806396 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2690 train loss: 1.0867999792099 train acc: 0.956250011920929 test loss: 1.3154851198196411 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2691 train loss: 1.0866597890853882 train acc: 0.956250011920929 test loss: 1.313989520072937 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2692 train loss: 1.0867096185684204 train acc: 0.956250011920929 test loss: 1.3161890506744385 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2693 train loss: 1.086652159690857 train acc: 0.956250011920929 test loss: 1.3236721754074097 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2694 train loss: 1.0867359638214111 train acc: 0.956250011920929 test loss: 1.3274532556533813 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2695 train loss: 1.086766004562378 train acc: 0.956250011920929 test loss: 1.3114341497421265 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2696 train loss: 1.086723804473877 train acc: 0.956250011920929 test loss: 1.3195350170135498 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2697 train loss: 1.0867955684661865 train acc: 0.956250011920929 test loss: 1.3144866228103638 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2698 train loss: 1.0867016315460205 train acc: 0.956250011920929 test loss: 1.3198916912078857 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2699 train loss: 1.0868346691131592 train acc: 0.956250011920929 test loss: 1.3302638530731201 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2700 train loss: 1.086617350578308 train acc: 0.956250011920929 test loss: 1.308319330215454 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2701 train loss: 1.086812973022461 train acc: 0.956250011920929 test loss: 1.3228938579559326 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2702 train loss: 1.0869351625442505 train acc: 0.956250011920929 test loss: 1.3220571279525757 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2703 train loss: 1.086944341659546 train acc: 0.956250011920929 test loss: 1.3262511491775513 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2704 train loss: 1.0868911743164062 train acc: 0.956250011920929 test loss: 1.3134071826934814 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2705 train loss: 1.0868066549301147 train acc: 0.956250011920929 test loss: 1.3311588764190674 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2706 train loss: 1.0868295431137085 train acc: 0.956250011920929 test loss: 1.3365336656570435 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2707 train loss: 1.0869433879852295 train acc: 0.956250011920929 test loss: 1.3035757541656494 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2708 train loss: 1.0868042707443237 train acc: 0.956250011920929 test loss: 1.3194152116775513 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2709 train loss: 1.0866398811340332 train acc: 0.956250011920929 test loss: 1.3004592657089233 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2710 train loss: 1.0866854190826416 train acc: 0.956250011920929 test loss: 1.3302783966064453 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2711 train loss: 1.0868167877197266 train acc: 0.956250011920929 test loss: 1.3202251195907593 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2712 train loss: 1.0865737199783325 train acc: 0.956250011920929 test loss: 1.3293170928955078 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2713 train loss: 1.0867416858673096 train acc: 0.956250011920929 test loss: 1.3101171255111694 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2714 train loss: 1.0867921113967896 train acc: 0.956250011920929 test loss: 1.3317551612854004 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2715 train loss: 1.0866856575012207 train acc: 0.956250011920929 test loss: 1.3131351470947266 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2716 train loss: 1.086841344833374 train acc: 0.956250011920929 test loss: 1.3161550760269165 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2717 train loss: 1.0868074893951416 train acc: 0.956250011920929 test loss: 1.3215197324752808 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2718 train loss: 1.086938500404358 train acc: 0.956250011920929 test loss: 1.3091752529144287 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2719 train loss: 1.0866621732711792 train acc: 0.956250011920929 test loss: 1.3175711631774902 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2720 train loss: 1.0867631435394287 train acc: 0.956250011920929 test loss: 1.327279806137085 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2721 train loss: 1.0867295265197754 train acc: 0.956250011920929 test loss: 1.321249008178711 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2722 train loss: 1.0866488218307495 train acc: 0.956250011920929 test loss: 1.3211212158203125 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2723 train loss: 1.0867856740951538 train acc: 0.956250011920929 test loss: 1.3274744749069214 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2724 train loss: 1.0868104696273804 train acc: 0.956250011920929 test loss: 1.3189815282821655 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2725 train loss: 1.0866934061050415 train acc: 0.956250011920929 test loss: 1.3087947368621826 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2726 train loss: 1.0868031978607178 train acc: 0.956250011920929 test loss: 1.3132413625717163 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2727 train loss: 1.0867403745651245 train acc: 0.956250011920929 test loss: 1.3242090940475464 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2728 train loss: 1.0864861011505127 train acc: 0.956250011920929 test loss: 1.3281594514846802 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2729 train loss: 1.0873806476593018 train acc: 0.956250011920929 test loss: 1.307619571685791 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2730 train loss: 1.0869046449661255 train acc: 0.956250011920929 test loss: 1.325477123260498 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2731 train loss: 1.08668851852417 train acc: 0.956250011920929 test loss: 1.3213528394699097 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2732 train loss: 1.0867016315460205 train acc: 0.956250011920929 test loss: 1.312933325767517 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2733 train loss: 1.0868366956710815 train acc: 0.956250011920929 test loss: 1.332624077796936 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2734 train loss: 1.0866472721099854 train acc: 0.956250011920929 test loss: 1.3089921474456787 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2735 train loss: 1.0866715908050537 train acc: 0.956250011920929 test loss: 1.3227342367172241 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2736 train loss: 1.0867284536361694 train acc: 0.956250011920929 test loss: 1.326017141342163 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2737 train loss: 1.0867671966552734 train acc: 0.956250011920929 test loss: 1.3014225959777832 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2738 train loss: 1.086663007736206 train acc: 0.956250011920929 test loss: 1.3042254447937012 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2739 train loss: 1.086574912071228 train acc: 0.956250011920929 test loss: 1.307998538017273 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2740 train loss: 1.0868027210235596 train acc: 0.956250011920929 test loss: 1.316998839378357 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2741 train loss: 1.0867524147033691 train acc: 0.956250011920929 test loss: 1.3275353908538818 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2742 train loss: 1.0867146253585815 train acc: 0.956250011920929 test loss: 1.3142527341842651 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2743 train loss: 1.0868617296218872 train acc: 0.956250011920929 test loss: 1.3355517387390137 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2744 train loss: 1.0867372751235962 train acc: 0.956250011920929 test loss: 1.3317698240280151 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2745 train loss: 1.086992859840393 train acc: 0.956250011920929 test loss: 1.3199466466903687 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2746 train loss: 1.0867609977722168 train acc: 0.956250011920929 test loss: 1.3408945798873901 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2747 train loss: 1.0868544578552246 train acc: 0.956250011920929 test loss: 1.3166214227676392 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2748 train loss: 1.0867793560028076 train acc: 0.956250011920929 test loss: 1.298696517944336 best test loss: 1.283445954322815 test acc: 0.7666666507720947\n",
      "Epoch 2749 train loss: 1.0865180492401123 train acc: 0.956250011920929 test loss: 1.3236156702041626 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2750 train loss: 1.0865861177444458 train acc: 0.956250011920929 test loss: 1.3138909339904785 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2751 train loss: 1.0866968631744385 train acc: 0.956250011920929 test loss: 1.3352704048156738 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2752 train loss: 1.0868077278137207 train acc: 0.956250011920929 test loss: 1.3169306516647339 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2753 train loss: 1.0866904258728027 train acc: 0.956250011920929 test loss: 1.336919903755188 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2754 train loss: 1.086784839630127 train acc: 0.956250011920929 test loss: 1.3106403350830078 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2755 train loss: 1.0866248607635498 train acc: 0.956250011920929 test loss: 1.314495325088501 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2756 train loss: 1.0866374969482422 train acc: 0.956250011920929 test loss: 1.3287582397460938 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2757 train loss: 1.0866146087646484 train acc: 0.956250011920929 test loss: 1.3471179008483887 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2758 train loss: 1.086567997932434 train acc: 0.956250011920929 test loss: 1.3241355419158936 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2759 train loss: 1.0868030786514282 train acc: 0.956250011920929 test loss: 1.3338778018951416 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2760 train loss: 1.0866869688034058 train acc: 0.956250011920929 test loss: 1.3153249025344849 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2761 train loss: 1.0868265628814697 train acc: 0.956250011920929 test loss: 1.3247956037521362 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2762 train loss: 1.0866930484771729 train acc: 0.956250011920929 test loss: 1.3304781913757324 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2763 train loss: 1.0867502689361572 train acc: 0.956250011920929 test loss: 1.3181201219558716 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2764 train loss: 1.0865873098373413 train acc: 0.956250011920929 test loss: 1.3275586366653442 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2765 train loss: 1.0867969989776611 train acc: 0.956250011920929 test loss: 1.3134064674377441 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2766 train loss: 1.0868682861328125 train acc: 0.956250011920929 test loss: 1.3154033422470093 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2767 train loss: 1.0869187116622925 train acc: 0.956250011920929 test loss: 1.31271493434906 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2768 train loss: 1.0866678953170776 train acc: 0.956250011920929 test loss: 1.34698486328125 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2769 train loss: 1.0867599248886108 train acc: 0.956250011920929 test loss: 1.3177387714385986 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2770 train loss: 1.0867584943771362 train acc: 0.956250011920929 test loss: 1.3136917352676392 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2771 train loss: 1.086721658706665 train acc: 0.956250011920929 test loss: 1.3294578790664673 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2772 train loss: 1.0868326425552368 train acc: 0.956250011920929 test loss: 1.317311406135559 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2773 train loss: 1.0868263244628906 train acc: 0.956250011920929 test loss: 1.3221631050109863 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2774 train loss: 1.0867750644683838 train acc: 0.956250011920929 test loss: 1.3281153440475464 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2775 train loss: 1.086844801902771 train acc: 0.956250011920929 test loss: 1.326794147491455 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2776 train loss: 1.0867019891738892 train acc: 0.956250011920929 test loss: 1.314031720161438 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2777 train loss: 1.0866485834121704 train acc: 0.956250011920929 test loss: 1.3234994411468506 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2778 train loss: 1.0867207050323486 train acc: 0.956250011920929 test loss: 1.3419522047042847 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2779 train loss: 1.0868971347808838 train acc: 0.956250011920929 test loss: 1.310071587562561 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2780 train loss: 1.0866724252700806 train acc: 0.956250011920929 test loss: 1.3099409341812134 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2781 train loss: 1.0867273807525635 train acc: 0.956250011920929 test loss: 1.3236045837402344 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2782 train loss: 1.0868598222732544 train acc: 0.956250011920929 test loss: 1.3067424297332764 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2783 train loss: 1.0867044925689697 train acc: 0.956250011920929 test loss: 1.3138031959533691 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2784 train loss: 1.086729884147644 train acc: 0.956250011920929 test loss: 1.3218421936035156 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2785 train loss: 1.0868122577667236 train acc: 0.956250011920929 test loss: 1.3285548686981201 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2786 train loss: 1.0868700742721558 train acc: 0.956250011920929 test loss: 1.3231518268585205 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2787 train loss: 1.0867847204208374 train acc: 0.956250011920929 test loss: 1.3246408700942993 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2788 train loss: 1.0868536233901978 train acc: 0.956250011920929 test loss: 1.3265583515167236 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2789 train loss: 1.0867964029312134 train acc: 0.956250011920929 test loss: 1.3356614112854004 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2790 train loss: 1.0867329835891724 train acc: 0.956250011920929 test loss: 1.3172950744628906 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2791 train loss: 1.0867809057235718 train acc: 0.956250011920929 test loss: 1.3316566944122314 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2792 train loss: 1.0867562294006348 train acc: 0.956250011920929 test loss: 1.3058204650878906 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2793 train loss: 1.0867573022842407 train acc: 0.956250011920929 test loss: 1.3460192680358887 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2794 train loss: 1.086722493171692 train acc: 0.956250011920929 test loss: 1.3145772218704224 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2795 train loss: 1.086692214012146 train acc: 0.956250011920929 test loss: 1.3235118389129639 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2796 train loss: 1.0866926908493042 train acc: 0.956250011920929 test loss: 1.31231689453125 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2797 train loss: 1.086728572845459 train acc: 0.956250011920929 test loss: 1.334396243095398 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2798 train loss: 1.0866485834121704 train acc: 0.956250011920929 test loss: 1.3235301971435547 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2799 train loss: 1.086570143699646 train acc: 0.956250011920929 test loss: 1.3182499408721924 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2800 train loss: 1.08682382106781 train acc: 0.956250011920929 test loss: 1.3255716562271118 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2801 train loss: 1.0865050554275513 train acc: 0.956250011920929 test loss: 1.3214000463485718 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2802 train loss: 1.0866715908050537 train acc: 0.956250011920929 test loss: 1.324391484260559 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2803 train loss: 1.086835265159607 train acc: 0.956250011920929 test loss: 1.3087810277938843 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2804 train loss: 1.0868182182312012 train acc: 0.956250011920929 test loss: 1.3225125074386597 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2805 train loss: 1.086788535118103 train acc: 0.956250011920929 test loss: 1.3368538618087769 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2806 train loss: 1.0868791341781616 train acc: 0.956250011920929 test loss: 1.3058719635009766 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2807 train loss: 1.0867085456848145 train acc: 0.956250011920929 test loss: 1.3249037265777588 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2808 train loss: 1.0866949558258057 train acc: 0.956250011920929 test loss: 1.329304814338684 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2809 train loss: 1.0866844654083252 train acc: 0.956250011920929 test loss: 1.304002285003662 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2810 train loss: 1.0867987871170044 train acc: 0.956250011920929 test loss: 1.3115324974060059 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2811 train loss: 1.086715817451477 train acc: 0.956250011920929 test loss: 1.348075032234192 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2812 train loss: 1.0866330862045288 train acc: 0.956250011920929 test loss: 1.3311647176742554 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2813 train loss: 1.0866668224334717 train acc: 0.956250011920929 test loss: 1.308497667312622 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2814 train loss: 1.086708426475525 train acc: 0.956250011920929 test loss: 1.3218036890029907 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2815 train loss: 1.0867456197738647 train acc: 0.956250011920929 test loss: 1.3061400651931763 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2816 train loss: 1.0867468118667603 train acc: 0.956250011920929 test loss: 1.3304380178451538 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2817 train loss: 1.0866966247558594 train acc: 0.956250011920929 test loss: 1.334749698638916 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2818 train loss: 1.0868662595748901 train acc: 0.956250011920929 test loss: 1.3329651355743408 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2819 train loss: 1.0867831707000732 train acc: 0.956250011920929 test loss: 1.3188234567642212 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2820 train loss: 1.0867044925689697 train acc: 0.956250011920929 test loss: 1.319009780883789 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2821 train loss: 1.0866045951843262 train acc: 0.956250011920929 test loss: 1.3374569416046143 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2822 train loss: 1.0868409872055054 train acc: 0.956250011920929 test loss: 1.316978931427002 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2823 train loss: 1.0868792533874512 train acc: 0.956250011920929 test loss: 1.319366693496704 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2824 train loss: 1.086785078048706 train acc: 0.956250011920929 test loss: 1.323750376701355 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2825 train loss: 1.0868648290634155 train acc: 0.956250011920929 test loss: 1.3245075941085815 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2826 train loss: 1.086519718170166 train acc: 0.956250011920929 test loss: 1.3179177045822144 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2827 train loss: 1.086561679840088 train acc: 0.956250011920929 test loss: 1.3150031566619873 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2828 train loss: 1.086882472038269 train acc: 0.956250011920929 test loss: 1.3333240747451782 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2829 train loss: 1.086803913116455 train acc: 0.956250011920929 test loss: 1.3189771175384521 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2830 train loss: 1.0866687297821045 train acc: 0.956250011920929 test loss: 1.322891116142273 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2831 train loss: 1.0865613222122192 train acc: 0.956250011920929 test loss: 1.319551944732666 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2832 train loss: 1.0866762399673462 train acc: 0.956250011920929 test loss: 1.301526427268982 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 2833 train loss: 1.0865083932876587 train acc: 0.956250011920929 test loss: 1.3206746578216553 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2834 train loss: 1.0868278741836548 train acc: 0.956250011920929 test loss: 1.3202518224716187 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2835 train loss: 1.0866791009902954 train acc: 0.956250011920929 test loss: 1.331146240234375 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2836 train loss: 1.0865333080291748 train acc: 0.956250011920929 test loss: 1.3300018310546875 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2837 train loss: 1.0867661237716675 train acc: 0.956250011920929 test loss: 1.3162543773651123 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2838 train loss: 1.0867012739181519 train acc: 0.956250011920929 test loss: 1.3343490362167358 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2839 train loss: 1.0866512060165405 train acc: 0.956250011920929 test loss: 1.3207855224609375 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2840 train loss: 1.0866349935531616 train acc: 0.956250011920929 test loss: 1.3460907936096191 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2841 train loss: 1.0867608785629272 train acc: 0.956250011920929 test loss: 1.3092941045761108 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2842 train loss: 1.0869126319885254 train acc: 0.956250011920929 test loss: 1.3292691707611084 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2843 train loss: 1.0867139101028442 train acc: 0.956250011920929 test loss: 1.3342541456222534 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2844 train loss: 1.086766004562378 train acc: 0.956250011920929 test loss: 1.3085254430770874 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2845 train loss: 1.0866942405700684 train acc: 0.956250011920929 test loss: 1.3218846321105957 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2846 train loss: 1.0868778228759766 train acc: 0.956250011920929 test loss: 1.3082162141799927 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2847 train loss: 1.0866544246673584 train acc: 0.956250011920929 test loss: 1.3381325006484985 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2848 train loss: 1.086557388305664 train acc: 0.956250011920929 test loss: 1.3332973718643188 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2849 train loss: 1.0865097045898438 train acc: 0.956250011920929 test loss: 1.328521490097046 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2850 train loss: 1.086757779121399 train acc: 0.956250011920929 test loss: 1.3088152408599854 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2851 train loss: 1.086667776107788 train acc: 0.956250011920929 test loss: 1.342246413230896 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2852 train loss: 1.0867325067520142 train acc: 0.956250011920929 test loss: 1.35003662109375 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2853 train loss: 1.0865814685821533 train acc: 0.956250011920929 test loss: 1.3251683712005615 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2854 train loss: 1.086875557899475 train acc: 0.956250011920929 test loss: 1.3155885934829712 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2855 train loss: 1.0866795778274536 train acc: 0.956250011920929 test loss: 1.3336540460586548 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2856 train loss: 1.0869147777557373 train acc: 0.956250011920929 test loss: 1.3349946737289429 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2857 train loss: 1.0867329835891724 train acc: 0.956250011920929 test loss: 1.344560980796814 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2858 train loss: 1.0867925882339478 train acc: 0.956250011920929 test loss: 1.31427001953125 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2859 train loss: 1.086722493171692 train acc: 0.956250011920929 test loss: 1.328245997428894 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2860 train loss: 1.0865352153778076 train acc: 0.956250011920929 test loss: 1.330242395401001 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2861 train loss: 1.0866931676864624 train acc: 0.956250011920929 test loss: 1.3140491247177124 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2862 train loss: 1.0866529941558838 train acc: 0.956250011920929 test loss: 1.3440197706222534 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2863 train loss: 1.0867257118225098 train acc: 0.956250011920929 test loss: 1.3240357637405396 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2864 train loss: 1.0865811109542847 train acc: 0.956250011920929 test loss: 1.342150330543518 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2865 train loss: 1.0869113206863403 train acc: 0.956250011920929 test loss: 1.3414032459259033 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2866 train loss: 1.0867035388946533 train acc: 0.956250011920929 test loss: 1.333689570426941 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2867 train loss: 1.0867890119552612 train acc: 0.956250011920929 test loss: 1.336411952972412 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2868 train loss: 1.0868645906448364 train acc: 0.956250011920929 test loss: 1.3212312459945679 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2869 train loss: 1.086508870124817 train acc: 0.956250011920929 test loss: 1.3286558389663696 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2870 train loss: 1.0865470170974731 train acc: 0.956250011920929 test loss: 1.3315045833587646 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2871 train loss: 1.0865809917449951 train acc: 0.956250011920929 test loss: 1.308062195777893 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2872 train loss: 1.0867398977279663 train acc: 0.956250011920929 test loss: 1.3319023847579956 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2873 train loss: 1.0866782665252686 train acc: 0.956250011920929 test loss: 1.3226035833358765 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2874 train loss: 1.0868622064590454 train acc: 0.956250011920929 test loss: 1.3215559720993042 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2875 train loss: 1.086727261543274 train acc: 0.956250011920929 test loss: 1.3375297784805298 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2876 train loss: 1.0866857767105103 train acc: 0.956250011920929 test loss: 1.333374261856079 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2877 train loss: 1.0867596864700317 train acc: 0.956250011920929 test loss: 1.3261375427246094 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2878 train loss: 1.0869464874267578 train acc: 0.956250011920929 test loss: 1.3198188543319702 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2879 train loss: 1.0866036415100098 train acc: 0.956250011920929 test loss: 1.3031837940216064 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2880 train loss: 1.0866808891296387 train acc: 0.956250011920929 test loss: 1.3149582147598267 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2881 train loss: 1.0867618322372437 train acc: 0.956250011920929 test loss: 1.3083908557891846 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2882 train loss: 1.0867751836776733 train acc: 0.956250011920929 test loss: 1.329827904701233 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2883 train loss: 1.0866845846176147 train acc: 0.956250011920929 test loss: 1.3182735443115234 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2884 train loss: 1.0868141651153564 train acc: 0.956250011920929 test loss: 1.3310168981552124 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2885 train loss: 1.08673095703125 train acc: 0.956250011920929 test loss: 1.3294810056686401 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2886 train loss: 1.0868324041366577 train acc: 0.956250011920929 test loss: 1.3221362829208374 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2887 train loss: 1.0867340564727783 train acc: 0.956250011920929 test loss: 1.322296380996704 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2888 train loss: 1.086807131767273 train acc: 0.956250011920929 test loss: 1.3286677598953247 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2889 train loss: 1.0867849588394165 train acc: 0.956250011920929 test loss: 1.324592113494873 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2890 train loss: 1.0865284204483032 train acc: 0.956250011920929 test loss: 1.3314841985702515 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2891 train loss: 1.0868144035339355 train acc: 0.956250011920929 test loss: 1.3176144361495972 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2892 train loss: 1.0866159200668335 train acc: 0.956250011920929 test loss: 1.321266770362854 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2893 train loss: 1.0866023302078247 train acc: 0.956250011920929 test loss: 1.3304616212844849 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2894 train loss: 1.0865815877914429 train acc: 0.956250011920929 test loss: 1.3307526111602783 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2895 train loss: 1.086664080619812 train acc: 0.956250011920929 test loss: 1.3376195430755615 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2896 train loss: 1.0867184400558472 train acc: 0.956250011920929 test loss: 1.3285423517227173 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2897 train loss: 1.0866408348083496 train acc: 0.956250011920929 test loss: 1.3468929529190063 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2898 train loss: 1.0865521430969238 train acc: 0.956250011920929 test loss: 1.3233648538589478 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2899 train loss: 1.086754322052002 train acc: 0.956250011920929 test loss: 1.312261939048767 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2900 train loss: 1.086558222770691 train acc: 0.956250011920929 test loss: 1.3535583019256592 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2901 train loss: 1.0865987539291382 train acc: 0.956250011920929 test loss: 1.334101915359497 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2902 train loss: 1.0866458415985107 train acc: 0.956250011920929 test loss: 1.3184964656829834 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2903 train loss: 1.086668848991394 train acc: 0.956250011920929 test loss: 1.3276787996292114 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2904 train loss: 1.0866825580596924 train acc: 0.956250011920929 test loss: 1.3316113948822021 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2905 train loss: 1.0866857767105103 train acc: 0.956250011920929 test loss: 1.355782151222229 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2906 train loss: 1.086706280708313 train acc: 0.956250011920929 test loss: 1.3233428001403809 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2907 train loss: 1.0866044759750366 train acc: 0.956250011920929 test loss: 1.325645089149475 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2908 train loss: 1.0864931344985962 train acc: 0.956250011920929 test loss: 1.3352198600769043 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2909 train loss: 1.0866059064865112 train acc: 0.956250011920929 test loss: 1.3229061365127563 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2910 train loss: 1.0869234800338745 train acc: 0.956250011920929 test loss: 1.3197238445281982 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2911 train loss: 1.0866541862487793 train acc: 0.956250011920929 test loss: 1.32568359375 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2912 train loss: 1.0867432355880737 train acc: 0.956250011920929 test loss: 1.3349778652191162 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2913 train loss: 1.0867469310760498 train acc: 0.956250011920929 test loss: 1.3438917398452759 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2914 train loss: 1.0869038105010986 train acc: 0.956250011920929 test loss: 1.3495370149612427 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2915 train loss: 1.0865366458892822 train acc: 0.956250011920929 test loss: 1.333198070526123 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2916 train loss: 1.0867550373077393 train acc: 0.956250011920929 test loss: 1.3281968832015991 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2917 train loss: 1.086669683456421 train acc: 0.956250011920929 test loss: 1.3334392309188843 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2918 train loss: 1.086720585823059 train acc: 0.956250011920929 test loss: 1.3381035327911377 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2919 train loss: 1.08677339553833 train acc: 0.956250011920929 test loss: 1.33124577999115 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2920 train loss: 1.0866754055023193 train acc: 0.956250011920929 test loss: 1.3347632884979248 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2921 train loss: 1.0866955518722534 train acc: 0.956250011920929 test loss: 1.3391114473342896 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2922 train loss: 1.0868580341339111 train acc: 0.956250011920929 test loss: 1.3141745328903198 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2923 train loss: 1.0866750478744507 train acc: 0.956250011920929 test loss: 1.329030156135559 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2924 train loss: 1.0867557525634766 train acc: 0.956250011920929 test loss: 1.3147023916244507 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2925 train loss: 1.0866941213607788 train acc: 0.956250011920929 test loss: 1.3308613300323486 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2926 train loss: 1.0867551565170288 train acc: 0.956250011920929 test loss: 1.318084716796875 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2927 train loss: 1.086761713027954 train acc: 0.956250011920929 test loss: 1.3213211297988892 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2928 train loss: 1.0867118835449219 train acc: 0.956250011920929 test loss: 1.3296527862548828 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2929 train loss: 1.086484670639038 train acc: 0.956250011920929 test loss: 1.3193718194961548 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2930 train loss: 1.086672306060791 train acc: 0.956250011920929 test loss: 1.332775592803955 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2931 train loss: 1.0864595174789429 train acc: 0.956250011920929 test loss: 1.345489740371704 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2932 train loss: 1.0865315198898315 train acc: 0.956250011920929 test loss: 1.3270797729492188 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2933 train loss: 1.0866150856018066 train acc: 0.956250011920929 test loss: 1.330998182296753 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2934 train loss: 1.0866153240203857 train acc: 0.956250011920929 test loss: 1.3358863592147827 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2935 train loss: 1.0865784883499146 train acc: 0.956250011920929 test loss: 1.3437926769256592 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2936 train loss: 1.0868237018585205 train acc: 0.956250011920929 test loss: 1.3193200826644897 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2937 train loss: 1.0868345499038696 train acc: 0.956250011920929 test loss: 1.3251177072525024 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2938 train loss: 1.0865775346755981 train acc: 0.956250011920929 test loss: 1.3296040296554565 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2939 train loss: 1.0866659879684448 train acc: 0.956250011920929 test loss: 1.3023300170898438 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 2940 train loss: 1.0866962671279907 train acc: 0.956250011920929 test loss: 1.3308089971542358 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2941 train loss: 1.0868403911590576 train acc: 0.956250011920929 test loss: 1.3392565250396729 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2942 train loss: 1.0868173837661743 train acc: 0.956250011920929 test loss: 1.317747950553894 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2943 train loss: 1.0863685607910156 train acc: 0.956250011920929 test loss: 1.3242169618606567 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2944 train loss: 1.0866450071334839 train acc: 0.956250011920929 test loss: 1.3274394273757935 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2945 train loss: 1.086571216583252 train acc: 0.956250011920929 test loss: 1.338720440864563 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2946 train loss: 1.0866367816925049 train acc: 0.956250011920929 test loss: 1.3228617906570435 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2947 train loss: 1.0865615606307983 train acc: 0.956250011920929 test loss: 1.3115074634552002 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2948 train loss: 1.086694359779358 train acc: 0.956250011920929 test loss: 1.331581711769104 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2949 train loss: 1.0866751670837402 train acc: 0.956250011920929 test loss: 1.326362133026123 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2950 train loss: 1.086647868156433 train acc: 0.956250011920929 test loss: 1.314801812171936 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2951 train loss: 1.0863220691680908 train acc: 0.956250011920929 test loss: 1.3368557691574097 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2952 train loss: 1.0865821838378906 train acc: 0.956250011920929 test loss: 1.3262885808944702 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2953 train loss: 1.0863490104675293 train acc: 0.956250011920929 test loss: 1.3189226388931274 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2954 train loss: 1.085873007774353 train acc: 0.9583333134651184 test loss: 1.3341740369796753 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2955 train loss: 1.0859416723251343 train acc: 0.956250011920929 test loss: 1.3436719179153442 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2956 train loss: 1.0849602222442627 train acc: 0.9583333134651184 test loss: 1.3243001699447632 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2957 train loss: 1.0848652124404907 train acc: 0.9583333134651184 test loss: 1.3306469917297363 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2958 train loss: 1.0849055051803589 train acc: 0.9583333134651184 test loss: 1.3224716186523438 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2959 train loss: 1.084553837776184 train acc: 0.9583333134651184 test loss: 1.3309290409088135 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2960 train loss: 1.0847892761230469 train acc: 0.9583333134651184 test loss: 1.3243833780288696 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2961 train loss: 1.0846610069274902 train acc: 0.9583333134651184 test loss: 1.3150194883346558 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2962 train loss: 1.0848288536071777 train acc: 0.9583333134651184 test loss: 1.3146228790283203 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2963 train loss: 1.0848686695098877 train acc: 0.9583333134651184 test loss: 1.3222267627716064 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2964 train loss: 1.0849443674087524 train acc: 0.9583333134651184 test loss: 1.3145992755889893 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2965 train loss: 1.0849590301513672 train acc: 0.9583333134651184 test loss: 1.329546332359314 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2966 train loss: 1.0847851037979126 train acc: 0.9583333134651184 test loss: 1.3184888362884521 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2967 train loss: 1.0848968029022217 train acc: 0.9583333134651184 test loss: 1.3268064260482788 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2968 train loss: 1.084953784942627 train acc: 0.9583333134651184 test loss: 1.3189343214035034 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2969 train loss: 1.0848432779312134 train acc: 0.9583333134651184 test loss: 1.3439068794250488 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2970 train loss: 1.0848376750946045 train acc: 0.9583333134651184 test loss: 1.31590735912323 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 2971 train loss: 1.0849398374557495 train acc: 0.9583333134651184 test loss: 1.328600525856018 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 2972 train loss: 1.0846236944198608 train acc: 0.9583333134651184 test loss: 1.3531626462936401 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2973 train loss: 1.0848126411437988 train acc: 0.9583333134651184 test loss: 1.3367141485214233 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2974 train loss: 1.0847934484481812 train acc: 0.9583333134651184 test loss: 1.3586680889129639 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 2975 train loss: 1.0846664905548096 train acc: 0.9583333134651184 test loss: 1.3505486249923706 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2976 train loss: 1.0849674940109253 train acc: 0.9583333134651184 test loss: 1.3514938354492188 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2977 train loss: 1.0848280191421509 train acc: 0.9583333134651184 test loss: 1.359970211982727 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2978 train loss: 1.0849353075027466 train acc: 0.9583333134651184 test loss: 1.3263440132141113 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2979 train loss: 1.0847245454788208 train acc: 0.9583333134651184 test loss: 1.3584288358688354 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2980 train loss: 1.0849593877792358 train acc: 0.9583333134651184 test loss: 1.3488825559616089 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2981 train loss: 1.084656834602356 train acc: 0.9583333134651184 test loss: 1.3551733493804932 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2982 train loss: 1.084658145904541 train acc: 0.9583333134651184 test loss: 1.3469456434249878 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2983 train loss: 1.0850093364715576 train acc: 0.9583333134651184 test loss: 1.3658427000045776 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 2984 train loss: 1.0846027135849 train acc: 0.9583333134651184 test loss: 1.3485156297683716 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2985 train loss: 1.084949016571045 train acc: 0.9583333134651184 test loss: 1.3272507190704346 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 2986 train loss: 1.084765076637268 train acc: 0.9583333134651184 test loss: 1.3500393629074097 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2987 train loss: 1.0846965312957764 train acc: 0.9583333134651184 test loss: 1.3228501081466675 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 2988 train loss: 1.0847307443618774 train acc: 0.9583333134651184 test loss: 1.3466720581054688 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2989 train loss: 1.084826946258545 train acc: 0.9583333134651184 test loss: 1.3488826751708984 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2990 train loss: 1.084678053855896 train acc: 0.9583333134651184 test loss: 1.3390079736709595 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 2991 train loss: 1.0849543809890747 train acc: 0.9583333134651184 test loss: 1.3487277030944824 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2992 train loss: 1.0848596096038818 train acc: 0.9583333134651184 test loss: 1.3485370874404907 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2993 train loss: 1.0847516059875488 train acc: 0.9583333134651184 test loss: 1.3490445613861084 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2994 train loss: 1.0847715139389038 train acc: 0.9583333134651184 test loss: 1.3528281450271606 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 2995 train loss: 1.0846971273422241 train acc: 0.9583333134651184 test loss: 1.354928731918335 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 2996 train loss: 1.0846425294876099 train acc: 0.9583333134651184 test loss: 1.3352348804473877 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 2997 train loss: 1.0849138498306274 train acc: 0.9583333134651184 test loss: 1.3470550775527954 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 2998 train loss: 1.084654688835144 train acc: 0.9583333134651184 test loss: 1.3614075183868408 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 2999 train loss: 1.0847094058990479 train acc: 0.9583333134651184 test loss: 1.3485028743743896 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3000 train loss: 1.0846658945083618 train acc: 0.9583333134651184 test loss: 1.3416869640350342 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3001 train loss: 1.0847817659378052 train acc: 0.9583333134651184 test loss: 1.3332183361053467 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3002 train loss: 1.0848172903060913 train acc: 0.9583333134651184 test loss: 1.3415489196777344 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3003 train loss: 1.0847567319869995 train acc: 0.9583333134651184 test loss: 1.3460944890975952 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3004 train loss: 1.0847450494766235 train acc: 0.9583333134651184 test loss: 1.3281490802764893 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3005 train loss: 1.0848701000213623 train acc: 0.9583333134651184 test loss: 1.3362699747085571 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3006 train loss: 1.0846939086914062 train acc: 0.9583333134651184 test loss: 1.3385517597198486 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3007 train loss: 1.084539771080017 train acc: 0.9583333134651184 test loss: 1.3410300016403198 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3008 train loss: 1.0849229097366333 train acc: 0.9583333134651184 test loss: 1.3359204530715942 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3009 train loss: 1.0847697257995605 train acc: 0.9583333134651184 test loss: 1.3262754678726196 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3010 train loss: 1.0848852396011353 train acc: 0.9583333134651184 test loss: 1.3404107093811035 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3011 train loss: 1.084824800491333 train acc: 0.9583333134651184 test loss: 1.3407095670700073 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3012 train loss: 1.0848625898361206 train acc: 0.9583333134651184 test loss: 1.3302819728851318 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3013 train loss: 1.0847094058990479 train acc: 0.9583333134651184 test loss: 1.3503035306930542 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3014 train loss: 1.0846059322357178 train acc: 0.9583333134651184 test loss: 1.3401858806610107 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3015 train loss: 1.0847498178482056 train acc: 0.9583333134651184 test loss: 1.3181456327438354 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3016 train loss: 1.084717035293579 train acc: 0.9583333134651184 test loss: 1.3419533967971802 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3017 train loss: 1.08473539352417 train acc: 0.9583333134651184 test loss: 1.3318601846694946 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3018 train loss: 1.084968090057373 train acc: 0.9583333134651184 test loss: 1.3333035707473755 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3019 train loss: 1.0846569538116455 train acc: 0.9583333134651184 test loss: 1.3305604457855225 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3020 train loss: 1.0846821069717407 train acc: 0.9583333134651184 test loss: 1.3231557607650757 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3021 train loss: 1.084652066230774 train acc: 0.9583333134651184 test loss: 1.3472920656204224 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3022 train loss: 1.0847049951553345 train acc: 0.9583333134651184 test loss: 1.3293681144714355 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3023 train loss: 1.0846610069274902 train acc: 0.9583333134651184 test loss: 1.350473165512085 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3024 train loss: 1.0847922563552856 train acc: 0.9583333134651184 test loss: 1.332574486732483 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3025 train loss: 1.0846843719482422 train acc: 0.9583333134651184 test loss: 1.3224117755889893 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3026 train loss: 1.0845698118209839 train acc: 0.9583333134651184 test loss: 1.3517521619796753 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3027 train loss: 1.0846021175384521 train acc: 0.9583333134651184 test loss: 1.345236897468567 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3028 train loss: 1.0846412181854248 train acc: 0.9583333134651184 test loss: 1.3315867185592651 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3029 train loss: 1.084734320640564 train acc: 0.9583333134651184 test loss: 1.3518460988998413 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3030 train loss: 1.0847779512405396 train acc: 0.9583333134651184 test loss: 1.363662600517273 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3031 train loss: 1.0847269296646118 train acc: 0.9583333134651184 test loss: 1.3410955667495728 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3032 train loss: 1.0848270654678345 train acc: 0.9583333134651184 test loss: 1.3501851558685303 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3033 train loss: 1.0847630500793457 train acc: 0.9583333134651184 test loss: 1.3160184621810913 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3034 train loss: 1.0847259759902954 train acc: 0.9583333134651184 test loss: 1.3335881233215332 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3035 train loss: 1.0845508575439453 train acc: 0.9583333134651184 test loss: 1.350314736366272 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3036 train loss: 1.084851622581482 train acc: 0.9583333134651184 test loss: 1.323911428451538 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3037 train loss: 1.08454430103302 train acc: 0.9583333134651184 test loss: 1.3381991386413574 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3038 train loss: 1.0850313901901245 train acc: 0.9583333134651184 test loss: 1.35293447971344 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3039 train loss: 1.0846843719482422 train acc: 0.9583333134651184 test loss: 1.3522034883499146 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3040 train loss: 1.0848673582077026 train acc: 0.9583333134651184 test loss: 1.3425298929214478 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3041 train loss: 1.0849164724349976 train acc: 0.9583333134651184 test loss: 1.3231562376022339 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3042 train loss: 1.0847464799880981 train acc: 0.9583333134651184 test loss: 1.3570328950881958 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3043 train loss: 1.0845805406570435 train acc: 0.9583333134651184 test loss: 1.3339372873306274 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3044 train loss: 1.0845237970352173 train acc: 0.9583333134651184 test loss: 1.3482335805892944 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3045 train loss: 1.084640383720398 train acc: 0.9583333134651184 test loss: 1.3448433876037598 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3046 train loss: 1.0846575498580933 train acc: 0.9583333134651184 test loss: 1.3207520246505737 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3047 train loss: 1.084686517715454 train acc: 0.9583333134651184 test loss: 1.3498451709747314 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 3048 train loss: 1.0848430395126343 train acc: 0.9583333134651184 test loss: 1.3481053113937378 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3049 train loss: 1.0847324132919312 train acc: 0.9583333134651184 test loss: 1.3323637247085571 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3050 train loss: 1.0846304893493652 train acc: 0.9583333134651184 test loss: 1.339599609375 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3051 train loss: 1.0846788883209229 train acc: 0.9583333134651184 test loss: 1.3360389471054077 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3052 train loss: 1.0846199989318848 train acc: 0.9583333134651184 test loss: 1.3360828161239624 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3053 train loss: 1.0846620798110962 train acc: 0.9583333134651184 test loss: 1.332770586013794 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3054 train loss: 1.084527850151062 train acc: 0.9583333134651184 test loss: 1.3178654909133911 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3055 train loss: 1.0845245122909546 train acc: 0.9583333134651184 test loss: 1.356652021408081 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3056 train loss: 1.0846507549285889 train acc: 0.9583333134651184 test loss: 1.3589483499526978 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3057 train loss: 1.084794282913208 train acc: 0.9583333134651184 test loss: 1.334634780883789 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3058 train loss: 1.084567904472351 train acc: 0.9583333134651184 test loss: 1.3324222564697266 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3059 train loss: 1.0848243236541748 train acc: 0.9583333134651184 test loss: 1.33206045627594 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3060 train loss: 1.0848273038864136 train acc: 0.9583333134651184 test loss: 1.3572337627410889 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3061 train loss: 1.0846282243728638 train acc: 0.9583333134651184 test loss: 1.340894341468811 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3062 train loss: 1.0848881006240845 train acc: 0.9583333134651184 test loss: 1.3417054414749146 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3063 train loss: 1.0845932960510254 train acc: 0.9583333134651184 test loss: 1.323083758354187 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3064 train loss: 1.0846359729766846 train acc: 0.9583333134651184 test loss: 1.3237766027450562 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3065 train loss: 1.0846351385116577 train acc: 0.9583333134651184 test loss: 1.3455922603607178 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3066 train loss: 1.0847758054733276 train acc: 0.9583333134651184 test loss: 1.3211569786071777 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3067 train loss: 1.0847325325012207 train acc: 0.9583333134651184 test loss: 1.3429170846939087 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3068 train loss: 1.0846606492996216 train acc: 0.9583333134651184 test loss: 1.3282238245010376 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3069 train loss: 1.0847359895706177 train acc: 0.9583333134651184 test loss: 1.340262532234192 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3070 train loss: 1.0847229957580566 train acc: 0.9583333134651184 test loss: 1.326636791229248 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3071 train loss: 1.084623098373413 train acc: 0.9583333134651184 test loss: 1.3609615564346313 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3072 train loss: 1.0847340822219849 train acc: 0.9583333134651184 test loss: 1.3329136371612549 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3073 train loss: 1.084808349609375 train acc: 0.9583333134651184 test loss: 1.3419572114944458 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3074 train loss: 1.0845317840576172 train acc: 0.9583333134651184 test loss: 1.3226555585861206 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3075 train loss: 1.0849415063858032 train acc: 0.9583333134651184 test loss: 1.3246105909347534 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3076 train loss: 1.084704875946045 train acc: 0.9583333134651184 test loss: 1.3257601261138916 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3077 train loss: 1.0847526788711548 train acc: 0.9583333134651184 test loss: 1.3324456214904785 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3078 train loss: 1.0848931074142456 train acc: 0.9583333134651184 test loss: 1.336463451385498 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3079 train loss: 1.084712266921997 train acc: 0.9583333134651184 test loss: 1.3272154331207275 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3080 train loss: 1.0846197605133057 train acc: 0.9583333134651184 test loss: 1.3459447622299194 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3081 train loss: 1.0847035646438599 train acc: 0.9583333134651184 test loss: 1.3296597003936768 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3082 train loss: 1.084779977798462 train acc: 0.9583333134651184 test loss: 1.3418561220169067 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3083 train loss: 1.0847121477127075 train acc: 0.9583333134651184 test loss: 1.3149572610855103 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3084 train loss: 1.0846288204193115 train acc: 0.9583333134651184 test loss: 1.335852861404419 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3085 train loss: 1.0847697257995605 train acc: 0.9583333134651184 test loss: 1.3441804647445679 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3086 train loss: 1.0846445560455322 train acc: 0.9583333134651184 test loss: 1.3386833667755127 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3087 train loss: 1.0847318172454834 train acc: 0.9583333134651184 test loss: 1.3269331455230713 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3088 train loss: 1.0847235918045044 train acc: 0.9583333134651184 test loss: 1.330221176147461 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3089 train loss: 1.0848203897476196 train acc: 0.9583333134651184 test loss: 1.3277764320373535 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3090 train loss: 1.0846197605133057 train acc: 0.9583333134651184 test loss: 1.3394694328308105 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3091 train loss: 1.0848416090011597 train acc: 0.9583333134651184 test loss: 1.3349230289459229 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3092 train loss: 1.084572434425354 train acc: 0.9583333134651184 test loss: 1.3314610719680786 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3093 train loss: 1.084807276725769 train acc: 0.9583333134651184 test loss: 1.337962031364441 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3094 train loss: 1.0846292972564697 train acc: 0.9583333134651184 test loss: 1.3457458019256592 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3095 train loss: 1.0848891735076904 train acc: 0.9583333134651184 test loss: 1.3128139972686768 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3096 train loss: 1.0845876932144165 train acc: 0.9583333134651184 test loss: 1.3367527723312378 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3097 train loss: 1.0847481489181519 train acc: 0.9583333134651184 test loss: 1.3358864784240723 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3098 train loss: 1.0846384763717651 train acc: 0.9583333134651184 test loss: 1.3481056690216064 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3099 train loss: 1.0845969915390015 train acc: 0.9583333134651184 test loss: 1.3313078880310059 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3100 train loss: 1.0846201181411743 train acc: 0.9583333134651184 test loss: 1.3546243906021118 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3101 train loss: 1.0847771167755127 train acc: 0.9583333134651184 test loss: 1.350990891456604 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3102 train loss: 1.084643006324768 train acc: 0.9583333134651184 test loss: 1.3370224237442017 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3103 train loss: 1.0847965478897095 train acc: 0.9583333134651184 test loss: 1.3283839225769043 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3104 train loss: 1.0845776796340942 train acc: 0.9583333134651184 test loss: 1.333128809928894 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3105 train loss: 1.084680438041687 train acc: 0.9583333134651184 test loss: 1.333287000656128 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3106 train loss: 1.0846970081329346 train acc: 0.9583333134651184 test loss: 1.33438241481781 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3107 train loss: 1.0847148895263672 train acc: 0.9583333134651184 test loss: 1.323781967163086 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3108 train loss: 1.0846048593521118 train acc: 0.9583333134651184 test loss: 1.3329875469207764 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3109 train loss: 1.0848513841629028 train acc: 0.9583333134651184 test loss: 1.3384968042373657 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3110 train loss: 1.084742784500122 train acc: 0.9583333134651184 test loss: 1.3457025289535522 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3111 train loss: 1.0846943855285645 train acc: 0.9583333134651184 test loss: 1.3464552164077759 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3112 train loss: 1.0846573114395142 train acc: 0.9583333134651184 test loss: 1.347654938697815 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3113 train loss: 1.0848925113677979 train acc: 0.9583333134651184 test loss: 1.3407453298568726 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3114 train loss: 1.0846372842788696 train acc: 0.9583333134651184 test loss: 1.3552839756011963 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3115 train loss: 1.0848442316055298 train acc: 0.9583333134651184 test loss: 1.3261640071868896 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3116 train loss: 1.0847383737564087 train acc: 0.9583333134651184 test loss: 1.3250547647476196 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3117 train loss: 1.0846693515777588 train acc: 0.9583333134651184 test loss: 1.3260784149169922 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3118 train loss: 1.084867000579834 train acc: 0.9583333134651184 test loss: 1.3464537858963013 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3119 train loss: 1.08479642868042 train acc: 0.9583333134651184 test loss: 1.322359561920166 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3120 train loss: 1.0846501588821411 train acc: 0.9583333134651184 test loss: 1.3232976198196411 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3121 train loss: 1.0848480463027954 train acc: 0.9583333134651184 test loss: 1.3299808502197266 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3122 train loss: 1.0846061706542969 train acc: 0.9583333134651184 test loss: 1.3392460346221924 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3123 train loss: 1.0846600532531738 train acc: 0.9583333134651184 test loss: 1.3411519527435303 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3124 train loss: 1.0846271514892578 train acc: 0.9583333134651184 test loss: 1.3309707641601562 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3125 train loss: 1.084743618965149 train acc: 0.9583333134651184 test loss: 1.3401721715927124 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3126 train loss: 1.0847105979919434 train acc: 0.9583333134651184 test loss: 1.3253456354141235 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3127 train loss: 1.0847132205963135 train acc: 0.9583333134651184 test loss: 1.3463802337646484 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3128 train loss: 1.0846956968307495 train acc: 0.9583333134651184 test loss: 1.342389702796936 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3129 train loss: 1.0847257375717163 train acc: 0.9583333134651184 test loss: 1.3279191255569458 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3130 train loss: 1.084841251373291 train acc: 0.9583333134651184 test loss: 1.3294867277145386 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3131 train loss: 1.085126280784607 train acc: 0.9583333134651184 test loss: 1.3350558280944824 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3132 train loss: 1.0846638679504395 train acc: 0.9583333134651184 test loss: 1.321804404258728 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3133 train loss: 1.0847166776657104 train acc: 0.9583333134651184 test loss: 1.3308322429656982 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3134 train loss: 1.0847995281219482 train acc: 0.9583333134651184 test loss: 1.3369394540786743 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3135 train loss: 1.08506441116333 train acc: 0.9583333134651184 test loss: 1.316124439239502 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3136 train loss: 1.084726333618164 train acc: 0.9583333134651184 test loss: 1.343185305595398 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3137 train loss: 1.0847556591033936 train acc: 0.9583333134651184 test loss: 1.3364369869232178 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3138 train loss: 1.0848238468170166 train acc: 0.9583333134651184 test loss: 1.3345379829406738 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3139 train loss: 1.0846366882324219 train acc: 0.9583333134651184 test loss: 1.3285248279571533 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3140 train loss: 1.08475661277771 train acc: 0.9583333134651184 test loss: 1.333677053451538 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3141 train loss: 1.0848311185836792 train acc: 0.9583333134651184 test loss: 1.3376553058624268 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3142 train loss: 1.0846271514892578 train acc: 0.9583333134651184 test loss: 1.3236916065216064 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3143 train loss: 1.0846880674362183 train acc: 0.9583333134651184 test loss: 1.3357398509979248 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3144 train loss: 1.0846251249313354 train acc: 0.9583333134651184 test loss: 1.3126803636550903 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3145 train loss: 1.0845915079116821 train acc: 0.9583333134651184 test loss: 1.322601079940796 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3146 train loss: 1.0845749378204346 train acc: 0.9583333134651184 test loss: 1.3273200988769531 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3147 train loss: 1.0846346616744995 train acc: 0.9583333134651184 test loss: 1.3437196016311646 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3148 train loss: 1.084593653678894 train acc: 0.9583333134651184 test loss: 1.3097721338272095 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3149 train loss: 1.084696888923645 train acc: 0.9583333134651184 test loss: 1.3233487606048584 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3150 train loss: 1.0849536657333374 train acc: 0.9583333134651184 test loss: 1.3356188535690308 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3151 train loss: 1.0847352743148804 train acc: 0.9583333134651184 test loss: 1.316878080368042 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3152 train loss: 1.0845664739608765 train acc: 0.9583333134651184 test loss: 1.321775197982788 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3153 train loss: 1.08480966091156 train acc: 0.9583333134651184 test loss: 1.3221269845962524 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3154 train loss: 1.0847169160842896 train acc: 0.9583333134651184 test loss: 1.3332659006118774 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3155 train loss: 1.0849766731262207 train acc: 0.9583333134651184 test loss: 1.3281384706497192 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3156 train loss: 1.0847084522247314 train acc: 0.9583333134651184 test loss: 1.315332293510437 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3157 train loss: 1.0847418308258057 train acc: 0.9583333134651184 test loss: 1.3472180366516113 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3158 train loss: 1.0847078561782837 train acc: 0.9583333134651184 test loss: 1.3452534675598145 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3159 train loss: 1.0847125053405762 train acc: 0.9583333134651184 test loss: 1.3336259126663208 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3160 train loss: 1.0846381187438965 train acc: 0.9583333134651184 test loss: 1.320601224899292 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3161 train loss: 1.0846654176712036 train acc: 0.9583333134651184 test loss: 1.3271862268447876 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3162 train loss: 1.0846449136734009 train acc: 0.9583333134651184 test loss: 1.3344521522521973 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3163 train loss: 1.0846995115280151 train acc: 0.9583333134651184 test loss: 1.3346024751663208 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3164 train loss: 1.084883213043213 train acc: 0.9583333134651184 test loss: 1.3235529661178589 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3165 train loss: 1.084798812866211 train acc: 0.9583333134651184 test loss: 1.3370859622955322 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3166 train loss: 1.0847653150558472 train acc: 0.9583333134651184 test loss: 1.3255881071090698 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3167 train loss: 1.0845940113067627 train acc: 0.9583333134651184 test loss: 1.349685549736023 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3168 train loss: 1.0847015380859375 train acc: 0.9583333134651184 test loss: 1.3359726667404175 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3169 train loss: 1.0845249891281128 train acc: 0.9583333134651184 test loss: 1.321645975112915 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3170 train loss: 1.0847418308258057 train acc: 0.9583333134651184 test loss: 1.3281925916671753 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3171 train loss: 1.0847136974334717 train acc: 0.9583333134651184 test loss: 1.326582670211792 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3172 train loss: 1.0848041772842407 train acc: 0.9583333134651184 test loss: 1.339522123336792 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3173 train loss: 1.0846209526062012 train acc: 0.9583333134651184 test loss: 1.316843867301941 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3174 train loss: 1.0846349000930786 train acc: 0.9583333134651184 test loss: 1.3242871761322021 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3175 train loss: 1.084728717803955 train acc: 0.9583333134651184 test loss: 1.3628884553909302 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3176 train loss: 1.0846878290176392 train acc: 0.9583333134651184 test loss: 1.3253439664840698 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3177 train loss: 1.0848498344421387 train acc: 0.9583333134651184 test loss: 1.3327412605285645 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3178 train loss: 1.084587812423706 train acc: 0.9583333134651184 test loss: 1.3623555898666382 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3179 train loss: 1.0848047733306885 train acc: 0.9583333134651184 test loss: 1.33847177028656 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3180 train loss: 1.0846081972122192 train acc: 0.9583333134651184 test loss: 1.3378002643585205 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3181 train loss: 1.0847976207733154 train acc: 0.9583333134651184 test loss: 1.3483517169952393 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3182 train loss: 1.0847519636154175 train acc: 0.9583333134651184 test loss: 1.334664225578308 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3183 train loss: 1.0847415924072266 train acc: 0.9583333134651184 test loss: 1.3459030389785767 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3184 train loss: 1.0847440958023071 train acc: 0.9583333134651184 test loss: 1.335334300994873 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3185 train loss: 1.0846213102340698 train acc: 0.9583333134651184 test loss: 1.3382152318954468 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3186 train loss: 1.0845786333084106 train acc: 0.9583333134651184 test loss: 1.3462735414505005 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3187 train loss: 1.0846501588821411 train acc: 0.9583333134651184 test loss: 1.335440993309021 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3188 train loss: 1.0848116874694824 train acc: 0.9583333134651184 test loss: 1.3343034982681274 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3189 train loss: 1.0845530033111572 train acc: 0.9583333134651184 test loss: 1.3430347442626953 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3190 train loss: 1.0847339630126953 train acc: 0.9583333134651184 test loss: 1.306477665901184 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 3191 train loss: 1.0847022533416748 train acc: 0.9583333134651184 test loss: 1.3212006092071533 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3192 train loss: 1.0846987962722778 train acc: 0.9583333134651184 test loss: 1.3257776498794556 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3193 train loss: 1.0846995115280151 train acc: 0.9583333134651184 test loss: 1.352227807044983 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3194 train loss: 1.0846046209335327 train acc: 0.9583333134651184 test loss: 1.3399032354354858 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3195 train loss: 1.0846543312072754 train acc: 0.9583333134651184 test loss: 1.3304152488708496 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3196 train loss: 1.0849099159240723 train acc: 0.9583333134651184 test loss: 1.348240613937378 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3197 train loss: 1.084634780883789 train acc: 0.9583333134651184 test loss: 1.3332730531692505 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3198 train loss: 1.0847350358963013 train acc: 0.9583333134651184 test loss: 1.3280502557754517 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3199 train loss: 1.0847666263580322 train acc: 0.9583333134651184 test loss: 1.328427791595459 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3200 train loss: 1.0849025249481201 train acc: 0.9583333134651184 test loss: 1.3391063213348389 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3201 train loss: 1.084755778312683 train acc: 0.9583333134651184 test loss: 1.3325331211090088 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3202 train loss: 1.0846362113952637 train acc: 0.9583333134651184 test loss: 1.3433680534362793 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3203 train loss: 1.084619402885437 train acc: 0.9583333134651184 test loss: 1.3379206657409668 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3204 train loss: 1.0843322277069092 train acc: 0.9583333134651184 test loss: 1.3249926567077637 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3205 train loss: 1.0847324132919312 train acc: 0.9583333134651184 test loss: 1.338373064994812 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3206 train loss: 1.0847182273864746 train acc: 0.9583333134651184 test loss: 1.3291923999786377 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3207 train loss: 1.084550380706787 train acc: 0.9583333134651184 test loss: 1.3290880918502808 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3208 train loss: 1.0847212076187134 train acc: 0.9583333134651184 test loss: 1.3248368501663208 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3209 train loss: 1.084736943244934 train acc: 0.9583333134651184 test loss: 1.3363896608352661 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3210 train loss: 1.0847678184509277 train acc: 0.9583333134651184 test loss: 1.3530505895614624 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3211 train loss: 1.0845718383789062 train acc: 0.9583333134651184 test loss: 1.324920415878296 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3212 train loss: 1.0846238136291504 train acc: 0.9583333134651184 test loss: 1.3368995189666748 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3213 train loss: 1.084509015083313 train acc: 0.9583333134651184 test loss: 1.3268487453460693 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3214 train loss: 1.0841468572616577 train acc: 0.9583333134651184 test loss: 1.347894549369812 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3215 train loss: 1.0836143493652344 train acc: 0.9583333134651184 test loss: 1.3260586261749268 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3216 train loss: 1.0826431512832642 train acc: 0.9604166746139526 test loss: 1.3139666318893433 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3217 train loss: 1.0814508199691772 train acc: 0.9624999761581421 test loss: 1.3023641109466553 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3218 train loss: 1.08154296875 train acc: 0.9624999761581421 test loss: 1.327621340751648 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3219 train loss: 1.082390308380127 train acc: 0.9624999761581421 test loss: 1.335390567779541 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3220 train loss: 1.08464515209198 train acc: 0.9583333134651184 test loss: 1.32748544216156 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3221 train loss: 1.0826247930526733 train acc: 0.9624999761581421 test loss: 1.3433765172958374 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3222 train loss: 1.0809345245361328 train acc: 0.9624999761581421 test loss: 1.3257676362991333 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3223 train loss: 1.0808690786361694 train acc: 0.9624999761581421 test loss: 1.3618515729904175 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3224 train loss: 1.0811007022857666 train acc: 0.9624999761581421 test loss: 1.3457287549972534 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3225 train loss: 1.085330605506897 train acc: 0.9583333134651184 test loss: 1.3459972143173218 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3226 train loss: 1.0843679904937744 train acc: 0.9604166746139526 test loss: 1.3544918298721313 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3227 train loss: 1.0828814506530762 train acc: 0.9624999761581421 test loss: 1.3550113439559937 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3228 train loss: 1.0809935331344604 train acc: 0.9624999761581421 test loss: 1.333219051361084 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3229 train loss: 1.0807528495788574 train acc: 0.9624999761581421 test loss: 1.3407235145568848 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3230 train loss: 1.0807398557662964 train acc: 0.9624999761581421 test loss: 1.345085620880127 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3231 train loss: 1.0838385820388794 train acc: 0.9583333134651184 test loss: 1.337330937385559 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3232 train loss: 1.0822460651397705 train acc: 0.9624999761581421 test loss: 1.3137632608413696 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3233 train loss: 1.080725073814392 train acc: 0.9624999761581421 test loss: 1.329253077507019 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3234 train loss: 1.0807671546936035 train acc: 0.9624999761581421 test loss: 1.3284475803375244 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3235 train loss: 1.0809636116027832 train acc: 0.9624999761581421 test loss: 1.332732915878296 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3236 train loss: 1.0807278156280518 train acc: 0.9624999761581421 test loss: 1.3391087055206299 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3237 train loss: 1.080716609954834 train acc: 0.9624999761581421 test loss: 1.3220707178115845 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3238 train loss: 1.0805600881576538 train acc: 0.9624999761581421 test loss: 1.3252609968185425 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3239 train loss: 1.0806114673614502 train acc: 0.9624999761581421 test loss: 1.3310356140136719 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3240 train loss: 1.080664038658142 train acc: 0.9624999761581421 test loss: 1.335578441619873 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3241 train loss: 1.0806766748428345 train acc: 0.9624999761581421 test loss: 1.3453983068466187 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3242 train loss: 1.080800175666809 train acc: 0.9624999761581421 test loss: 1.33157479763031 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3243 train loss: 1.080742359161377 train acc: 0.9624999761581421 test loss: 1.3414453268051147 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3244 train loss: 1.080769658088684 train acc: 0.9624999761581421 test loss: 1.317945122718811 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3245 train loss: 1.0808241367340088 train acc: 0.9624999761581421 test loss: 1.307576298713684 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 3246 train loss: 1.0808688402175903 train acc: 0.9624999761581421 test loss: 1.3390530347824097 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3247 train loss: 1.0807418823242188 train acc: 0.9624999761581421 test loss: 1.3320205211639404 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3248 train loss: 1.0807673931121826 train acc: 0.9624999761581421 test loss: 1.3290917873382568 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3249 train loss: 1.0805892944335938 train acc: 0.9624999761581421 test loss: 1.3351927995681763 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3250 train loss: 1.080670952796936 train acc: 0.9624999761581421 test loss: 1.336940050125122 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3251 train loss: 1.0807780027389526 train acc: 0.9624999761581421 test loss: 1.3342585563659668 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3252 train loss: 1.080696940422058 train acc: 0.9624999761581421 test loss: 1.3238002061843872 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3253 train loss: 1.0805803537368774 train acc: 0.9624999761581421 test loss: 1.3401219844818115 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3254 train loss: 1.0806280374526978 train acc: 0.9624999761581421 test loss: 1.3315328359603882 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3255 train loss: 1.0805939435958862 train acc: 0.9624999761581421 test loss: 1.3385454416275024 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3256 train loss: 1.080877661705017 train acc: 0.9624999761581421 test loss: 1.3458002805709839 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3257 train loss: 1.0807040929794312 train acc: 0.9624999761581421 test loss: 1.3240755796432495 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3258 train loss: 1.0807586908340454 train acc: 0.9624999761581421 test loss: 1.31868314743042 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3259 train loss: 1.0806865692138672 train acc: 0.9624999761581421 test loss: 1.337523341178894 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3260 train loss: 1.0806663036346436 train acc: 0.9624999761581421 test loss: 1.3319796323776245 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3261 train loss: 1.0808449983596802 train acc: 0.9624999761581421 test loss: 1.3473867177963257 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3262 train loss: 1.0805765390396118 train acc: 0.9624999761581421 test loss: 1.331965684890747 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3263 train loss: 1.0806705951690674 train acc: 0.9624999761581421 test loss: 1.3419857025146484 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3264 train loss: 1.0804831981658936 train acc: 0.9624999761581421 test loss: 1.3436270952224731 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3265 train loss: 1.0805869102478027 train acc: 0.9624999761581421 test loss: 1.3472754955291748 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3266 train loss: 1.0805474519729614 train acc: 0.9624999761581421 test loss: 1.3245460987091064 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3267 train loss: 1.0807722806930542 train acc: 0.9624999761581421 test loss: 1.3176058530807495 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3268 train loss: 1.0806113481521606 train acc: 0.9624999761581421 test loss: 1.3170650005340576 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3269 train loss: 1.0803966522216797 train acc: 0.9624999761581421 test loss: 1.318608045578003 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3270 train loss: 1.0805946588516235 train acc: 0.9624999761581421 test loss: 1.3154908418655396 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3271 train loss: 1.0806142091751099 train acc: 0.9624999761581421 test loss: 1.322965383529663 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3272 train loss: 1.080610990524292 train acc: 0.9624999761581421 test loss: 1.3251996040344238 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3273 train loss: 1.0807052850723267 train acc: 0.9624999761581421 test loss: 1.337903618812561 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3274 train loss: 1.080633521080017 train acc: 0.9624999761581421 test loss: 1.3290928602218628 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3275 train loss: 1.0807244777679443 train acc: 0.9624999761581421 test loss: 1.3145108222961426 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3276 train loss: 1.0805948972702026 train acc: 0.9624999761581421 test loss: 1.318591833114624 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3277 train loss: 1.080642819404602 train acc: 0.9624999761581421 test loss: 1.3120918273925781 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3278 train loss: 1.080506443977356 train acc: 0.9624999761581421 test loss: 1.328330636024475 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3279 train loss: 1.080584168434143 train acc: 0.9624999761581421 test loss: 1.319144606590271 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3280 train loss: 1.0804805755615234 train acc: 0.9624999761581421 test loss: 1.3200819492340088 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3281 train loss: 1.080451488494873 train acc: 0.9624999761581421 test loss: 1.3306487798690796 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3282 train loss: 1.0804933309555054 train acc: 0.9624999761581421 test loss: 1.3216830492019653 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3283 train loss: 1.0803130865097046 train acc: 0.9624999761581421 test loss: 1.328497290611267 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3284 train loss: 1.08033287525177 train acc: 0.9624999761581421 test loss: 1.3396981954574585 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3285 train loss: 1.0805773735046387 train acc: 0.9624999761581421 test loss: 1.3327893018722534 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3286 train loss: 1.0799740552902222 train acc: 0.9624999761581421 test loss: 1.3349649906158447 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3287 train loss: 1.0803254842758179 train acc: 0.9624999761581421 test loss: 1.3265892267227173 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3288 train loss: 1.080051064491272 train acc: 0.9624999761581421 test loss: 1.3239470720291138 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3289 train loss: 1.0801013708114624 train acc: 0.9624999761581421 test loss: 1.3435676097869873 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3290 train loss: 1.0787824392318726 train acc: 0.9645833373069763 test loss: 1.3168646097183228 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3291 train loss: 1.0786362886428833 train acc: 0.9645833373069763 test loss: 1.322112798690796 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3292 train loss: 1.0787729024887085 train acc: 0.9645833373069763 test loss: 1.338411808013916 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3293 train loss: 1.0787447690963745 train acc: 0.9645833373069763 test loss: 1.3297420740127563 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3294 train loss: 1.0789415836334229 train acc: 0.9645833373069763 test loss: 1.318758487701416 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3295 train loss: 1.0787467956542969 train acc: 0.9645833373069763 test loss: 1.3352431058883667 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3296 train loss: 1.0789319276809692 train acc: 0.9645833373069763 test loss: 1.3525383472442627 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3297 train loss: 1.0789703130722046 train acc: 0.9645833373069763 test loss: 1.3091845512390137 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3298 train loss: 1.0786383152008057 train acc: 0.9645833373069763 test loss: 1.3399850130081177 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3299 train loss: 1.0785915851593018 train acc: 0.9645833373069763 test loss: 1.3318787813186646 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3300 train loss: 1.0786328315734863 train acc: 0.9645833373069763 test loss: 1.3532602787017822 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3301 train loss: 1.0786575078964233 train acc: 0.9645833373069763 test loss: 1.3313205242156982 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3302 train loss: 1.078660249710083 train acc: 0.9645833373069763 test loss: 1.3211017847061157 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3303 train loss: 1.078818440437317 train acc: 0.9645833373069763 test loss: 1.3264187574386597 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3304 train loss: 1.078705906867981 train acc: 0.9645833373069763 test loss: 1.3362184762954712 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3305 train loss: 1.0784881114959717 train acc: 0.9645833373069763 test loss: 1.3279345035552979 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3306 train loss: 1.0787160396575928 train acc: 0.9645833373069763 test loss: 1.3241313695907593 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3307 train loss: 1.0787683725357056 train acc: 0.9645833373069763 test loss: 1.35187828540802 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3308 train loss: 1.078580379486084 train acc: 0.9645833373069763 test loss: 1.3457272052764893 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3309 train loss: 1.0788006782531738 train acc: 0.9645833373069763 test loss: 1.3215464353561401 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3310 train loss: 1.0787192583084106 train acc: 0.9645833373069763 test loss: 1.3344868421554565 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3311 train loss: 1.0788627862930298 train acc: 0.9645833373069763 test loss: 1.3453669548034668 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3312 train loss: 1.0787639617919922 train acc: 0.9645833373069763 test loss: 1.3450393676757812 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3313 train loss: 1.0786705017089844 train acc: 0.9645833373069763 test loss: 1.3345025777816772 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3314 train loss: 1.0787379741668701 train acc: 0.9645833373069763 test loss: 1.341638207435608 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3315 train loss: 1.0787519216537476 train acc: 0.9645833373069763 test loss: 1.3247816562652588 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3316 train loss: 1.078733205795288 train acc: 0.9645833373069763 test loss: 1.346333384513855 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3317 train loss: 1.078539490699768 train acc: 0.9645833373069763 test loss: 1.3281444311141968 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3318 train loss: 1.0786738395690918 train acc: 0.9645833373069763 test loss: 1.334761142730713 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3319 train loss: 1.0786010026931763 train acc: 0.9645833373069763 test loss: 1.3406773805618286 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3320 train loss: 1.0785702466964722 train acc: 0.9645833373069763 test loss: 1.3187938928604126 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3321 train loss: 1.078818678855896 train acc: 0.9645833373069763 test loss: 1.3376948833465576 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3322 train loss: 1.07871675491333 train acc: 0.9645833373069763 test loss: 1.3143929243087769 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3323 train loss: 1.0785834789276123 train acc: 0.9645833373069763 test loss: 1.3263533115386963 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3324 train loss: 1.0787349939346313 train acc: 0.9645833373069763 test loss: 1.346315622329712 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3325 train loss: 1.0786411762237549 train acc: 0.9645833373069763 test loss: 1.3326733112335205 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3326 train loss: 1.078553557395935 train acc: 0.9645833373069763 test loss: 1.3363357782363892 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3327 train loss: 1.078641414642334 train acc: 0.9645833373069763 test loss: 1.3391810655593872 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3328 train loss: 1.0785161256790161 train acc: 0.9645833373069763 test loss: 1.3375322818756104 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3329 train loss: 1.0785565376281738 train acc: 0.9645833373069763 test loss: 1.3245919942855835 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3330 train loss: 1.0785812139511108 train acc: 0.9645833373069763 test loss: 1.34446382522583 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3331 train loss: 1.0786709785461426 train acc: 0.9645833373069763 test loss: 1.3276492357254028 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3332 train loss: 1.0787551403045654 train acc: 0.9645833373069763 test loss: 1.336188554763794 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3333 train loss: 1.0787739753723145 train acc: 0.9645833373069763 test loss: 1.3374744653701782 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3334 train loss: 1.0789532661437988 train acc: 0.9645833373069763 test loss: 1.3489888906478882 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3335 train loss: 1.0785813331604004 train acc: 0.9645833373069763 test loss: 1.3256572484970093 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3336 train loss: 1.07862389087677 train acc: 0.9645833373069763 test loss: 1.3454742431640625 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3337 train loss: 1.0786933898925781 train acc: 0.9645833373069763 test loss: 1.3361046314239502 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3338 train loss: 1.0786747932434082 train acc: 0.9645833373069763 test loss: 1.3125605583190918 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3339 train loss: 1.0784475803375244 train acc: 0.9645833373069763 test loss: 1.346134066581726 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3340 train loss: 1.078607201576233 train acc: 0.9645833373069763 test loss: 1.32991361618042 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3341 train loss: 1.0787266492843628 train acc: 0.9645833373069763 test loss: 1.3357558250427246 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3342 train loss: 1.0786043405532837 train acc: 0.9645833373069763 test loss: 1.3444750308990479 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3343 train loss: 1.0786908864974976 train acc: 0.9645833373069763 test loss: 1.3488495349884033 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3344 train loss: 1.0785274505615234 train acc: 0.9645833373069763 test loss: 1.3307132720947266 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3345 train loss: 1.0786916017532349 train acc: 0.9645833373069763 test loss: 1.3267269134521484 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3346 train loss: 1.0784168243408203 train acc: 0.9645833373069763 test loss: 1.349290370941162 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3347 train loss: 1.0786510705947876 train acc: 0.9645833373069763 test loss: 1.318076729774475 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3348 train loss: 1.0785609483718872 train acc: 0.9645833373069763 test loss: 1.3324049711227417 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3349 train loss: 1.0787190198898315 train acc: 0.9645833373069763 test loss: 1.3303338289260864 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3350 train loss: 1.0784838199615479 train acc: 0.9645833373069763 test loss: 1.3238377571105957 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3351 train loss: 1.0787297487258911 train acc: 0.9645833373069763 test loss: 1.3298542499542236 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3352 train loss: 1.0798085927963257 train acc: 0.9624999761581421 test loss: 1.331829309463501 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3353 train loss: 1.0785257816314697 train acc: 0.9645833373069763 test loss: 1.346686601638794 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3354 train loss: 1.078683614730835 train acc: 0.9645833373069763 test loss: 1.3333293199539185 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3355 train loss: 1.0786386728286743 train acc: 0.9645833373069763 test loss: 1.3453729152679443 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3356 train loss: 1.0786216259002686 train acc: 0.9645833373069763 test loss: 1.3243248462677002 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3357 train loss: 1.0787593126296997 train acc: 0.9645833373069763 test loss: 1.3375415802001953 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3358 train loss: 1.0786930322647095 train acc: 0.9645833373069763 test loss: 1.3429107666015625 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3359 train loss: 1.0785601139068604 train acc: 0.9645833373069763 test loss: 1.33664071559906 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3360 train loss: 1.078702449798584 train acc: 0.9645833373069763 test loss: 1.347429633140564 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3361 train loss: 1.0792582035064697 train acc: 0.9645833373069763 test loss: 1.3388588428497314 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3362 train loss: 1.0787791013717651 train acc: 0.9645833373069763 test loss: 1.3399937152862549 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3363 train loss: 1.0785787105560303 train acc: 0.9645833373069763 test loss: 1.341307282447815 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3364 train loss: 1.078734040260315 train acc: 0.9645833373069763 test loss: 1.343063473701477 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3365 train loss: 1.0783761739730835 train acc: 0.9645833373069763 test loss: 1.3230178356170654 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3366 train loss: 1.0785006284713745 train acc: 0.9645833373069763 test loss: 1.328636646270752 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3367 train loss: 1.078662395477295 train acc: 0.9645833373069763 test loss: 1.3238928318023682 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3368 train loss: 1.0784869194030762 train acc: 0.9645833373069763 test loss: 1.328620433807373 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3369 train loss: 1.0787079334259033 train acc: 0.9645833373069763 test loss: 1.3418262004852295 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3370 train loss: 1.0786138772964478 train acc: 0.9645833373069763 test loss: 1.3177318572998047 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3371 train loss: 1.0787484645843506 train acc: 0.9645833373069763 test loss: 1.3285952806472778 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3372 train loss: 1.0787315368652344 train acc: 0.9645833373069763 test loss: 1.353110909461975 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3373 train loss: 1.078843593597412 train acc: 0.9645833373069763 test loss: 1.3388816118240356 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3374 train loss: 1.078840732574463 train acc: 0.9645833373069763 test loss: 1.334216833114624 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3375 train loss: 1.0785897970199585 train acc: 0.9645833373069763 test loss: 1.324381947517395 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3376 train loss: 1.0787161588668823 train acc: 0.9645833373069763 test loss: 1.3355547189712524 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3377 train loss: 1.0785070657730103 train acc: 0.9645833373069763 test loss: 1.3234139680862427 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3378 train loss: 1.0784991979599 train acc: 0.9645833373069763 test loss: 1.3302425146102905 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3379 train loss: 1.0785771608352661 train acc: 0.9645833373069763 test loss: 1.3216681480407715 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3380 train loss: 1.07852303981781 train acc: 0.9645833373069763 test loss: 1.3424689769744873 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3381 train loss: 1.0784848928451538 train acc: 0.9645833373069763 test loss: 1.3334439992904663 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3382 train loss: 1.0785884857177734 train acc: 0.9645833373069763 test loss: 1.3411508798599243 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3383 train loss: 1.0786288976669312 train acc: 0.9645833373069763 test loss: 1.3423380851745605 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3384 train loss: 1.078722596168518 train acc: 0.9645833373069763 test loss: 1.3442234992980957 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3385 train loss: 1.0786564350128174 train acc: 0.9645833373069763 test loss: 1.3255159854888916 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3386 train loss: 1.0785845518112183 train acc: 0.9645833373069763 test loss: 1.3204703330993652 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3387 train loss: 1.0787181854248047 train acc: 0.9645833373069763 test loss: 1.3391258716583252 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3388 train loss: 1.078606367111206 train acc: 0.9645833373069763 test loss: 1.3455166816711426 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3389 train loss: 1.0785311460494995 train acc: 0.9645833373069763 test loss: 1.342483639717102 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3390 train loss: 1.0787392854690552 train acc: 0.9645833373069763 test loss: 1.328284502029419 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3391 train loss: 1.0787380933761597 train acc: 0.9645833373069763 test loss: 1.320298433303833 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3392 train loss: 1.078589916229248 train acc: 0.9645833373069763 test loss: 1.343695044517517 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3393 train loss: 1.0785717964172363 train acc: 0.9645833373069763 test loss: 1.3419731855392456 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3394 train loss: 1.0785083770751953 train acc: 0.9645833373069763 test loss: 1.3292821645736694 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3395 train loss: 1.0785366296768188 train acc: 0.9645833373069763 test loss: 1.3268580436706543 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3396 train loss: 1.0787415504455566 train acc: 0.9645833373069763 test loss: 1.3309170007705688 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3397 train loss: 1.0786033868789673 train acc: 0.9645833373069763 test loss: 1.3370139598846436 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3398 train loss: 1.0786265134811401 train acc: 0.9645833373069763 test loss: 1.3556970357894897 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3399 train loss: 1.078640341758728 train acc: 0.9645833373069763 test loss: 1.3382030725479126 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3400 train loss: 1.0785709619522095 train acc: 0.9645833373069763 test loss: 1.3291854858398438 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3401 train loss: 1.0786837339401245 train acc: 0.9645833373069763 test loss: 1.3428007364273071 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3402 train loss: 1.0785157680511475 train acc: 0.9645833373069763 test loss: 1.333397626876831 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3403 train loss: 1.0786455869674683 train acc: 0.9645833373069763 test loss: 1.3498244285583496 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3404 train loss: 1.0786899328231812 train acc: 0.9645833373069763 test loss: 1.3368016481399536 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3405 train loss: 1.0784928798675537 train acc: 0.9645833373069763 test loss: 1.3312376737594604 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3406 train loss: 1.0785236358642578 train acc: 0.9645833373069763 test loss: 1.3389191627502441 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3407 train loss: 1.078453779220581 train acc: 0.9645833373069763 test loss: 1.3274040222167969 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3408 train loss: 1.07863187789917 train acc: 0.9645833373069763 test loss: 1.35554838180542 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3409 train loss: 1.0785675048828125 train acc: 0.9645833373069763 test loss: 1.3589760065078735 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3410 train loss: 1.0785199403762817 train acc: 0.9645833373069763 test loss: 1.3177298307418823 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3411 train loss: 1.0784021615982056 train acc: 0.9645833373069763 test loss: 1.3514248132705688 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3412 train loss: 1.0785337686538696 train acc: 0.9645833373069763 test loss: 1.3308321237564087 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3413 train loss: 1.0786364078521729 train acc: 0.9645833373069763 test loss: 1.3461381196975708 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3414 train loss: 1.078609824180603 train acc: 0.9645833373069763 test loss: 1.3434410095214844 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3415 train loss: 1.0786852836608887 train acc: 0.9645833373069763 test loss: 1.3470962047576904 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3416 train loss: 1.0786181688308716 train acc: 0.9645833373069763 test loss: 1.3500030040740967 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3417 train loss: 1.0786296129226685 train acc: 0.9645833373069763 test loss: 1.349345088005066 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3418 train loss: 1.0784281492233276 train acc: 0.9645833373069763 test loss: 1.3325244188308716 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3419 train loss: 1.0786091089248657 train acc: 0.9645833373069763 test loss: 1.3507723808288574 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3420 train loss: 1.0787408351898193 train acc: 0.9645833373069763 test loss: 1.3531862497329712 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3421 train loss: 1.0785470008850098 train acc: 0.9645833373069763 test loss: 1.3281852006912231 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3422 train loss: 1.0785901546478271 train acc: 0.9645833373069763 test loss: 1.3444615602493286 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3423 train loss: 1.0784553289413452 train acc: 0.9645833373069763 test loss: 1.3259327411651611 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3424 train loss: 1.078432321548462 train acc: 0.9645833373069763 test loss: 1.348833441734314 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3425 train loss: 1.0786845684051514 train acc: 0.9645833373069763 test loss: 1.335538387298584 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3426 train loss: 1.0786426067352295 train acc: 0.9645833373069763 test loss: 1.3278802633285522 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3427 train loss: 1.0786693096160889 train acc: 0.9645833373069763 test loss: 1.3458775281906128 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3428 train loss: 1.0785672664642334 train acc: 0.9645833373069763 test loss: 1.3295587301254272 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3429 train loss: 1.0785335302352905 train acc: 0.9645833373069763 test loss: 1.3288363218307495 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3430 train loss: 1.078697919845581 train acc: 0.9645833373069763 test loss: 1.342637538909912 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3431 train loss: 1.0785235166549683 train acc: 0.9645833373069763 test loss: 1.3432220220565796 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3432 train loss: 1.0785073041915894 train acc: 0.9645833373069763 test loss: 1.3347948789596558 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3433 train loss: 1.078682780265808 train acc: 0.9645833373069763 test loss: 1.3373228311538696 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3434 train loss: 1.0786527395248413 train acc: 0.9645833373069763 test loss: 1.3227818012237549 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3435 train loss: 1.07869553565979 train acc: 0.9645833373069763 test loss: 1.3262228965759277 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3436 train loss: 1.0787842273712158 train acc: 0.9645833373069763 test loss: 1.3409467935562134 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3437 train loss: 1.078558325767517 train acc: 0.9645833373069763 test loss: 1.3521606922149658 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3438 train loss: 1.0788341760635376 train acc: 0.9645833373069763 test loss: 1.3503886461257935 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3439 train loss: 1.0786076784133911 train acc: 0.9645833373069763 test loss: 1.333050012588501 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3440 train loss: 1.0786877870559692 train acc: 0.9645833373069763 test loss: 1.3355063199996948 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3441 train loss: 1.0786787271499634 train acc: 0.9645833373069763 test loss: 1.333008885383606 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3442 train loss: 1.0786489248275757 train acc: 0.9645833373069763 test loss: 1.3334767818450928 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3443 train loss: 1.0786707401275635 train acc: 0.9645833373069763 test loss: 1.3438963890075684 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3444 train loss: 1.0784329175949097 train acc: 0.9645833373069763 test loss: 1.35051691532135 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3445 train loss: 1.0785709619522095 train acc: 0.9645833373069763 test loss: 1.3453922271728516 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3446 train loss: 1.0785725116729736 train acc: 0.9645833373069763 test loss: 1.3475474119186401 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3447 train loss: 1.0785949230194092 train acc: 0.9645833373069763 test loss: 1.3472758531570435 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3448 train loss: 1.0785831212997437 train acc: 0.9645833373069763 test loss: 1.3399677276611328 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3449 train loss: 1.0785702466964722 train acc: 0.9645833373069763 test loss: 1.3352171182632446 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3450 train loss: 1.0785571336746216 train acc: 0.9645833373069763 test loss: 1.3544504642486572 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3451 train loss: 1.078590750694275 train acc: 0.9645833373069763 test loss: 1.3484119176864624 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3452 train loss: 1.0787615776062012 train acc: 0.9645833373069763 test loss: 1.3511083126068115 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3453 train loss: 1.0786129236221313 train acc: 0.9645833373069763 test loss: 1.342844843864441 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3454 train loss: 1.0785375833511353 train acc: 0.9645833373069763 test loss: 1.3464120626449585 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3455 train loss: 1.0785024166107178 train acc: 0.9645833373069763 test loss: 1.3472570180892944 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3456 train loss: 1.0787535905838013 train acc: 0.9645833373069763 test loss: 1.336659550666809 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3457 train loss: 1.0786806344985962 train acc: 0.9645833373069763 test loss: 1.3523648977279663 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3458 train loss: 1.0785635709762573 train acc: 0.9645833373069763 test loss: 1.344032645225525 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3459 train loss: 1.0785962343215942 train acc: 0.9645833373069763 test loss: 1.346517562866211 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3460 train loss: 1.0788379907608032 train acc: 0.9645833373069763 test loss: 1.3542590141296387 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3461 train loss: 1.078551173210144 train acc: 0.9645833373069763 test loss: 1.3448587656021118 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3462 train loss: 1.0785373449325562 train acc: 0.9645833373069763 test loss: 1.3416943550109863 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3463 train loss: 1.0786248445510864 train acc: 0.9645833373069763 test loss: 1.3568034172058105 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3464 train loss: 1.0785881280899048 train acc: 0.9645833373069763 test loss: 1.3306002616882324 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3465 train loss: 1.0786036252975464 train acc: 0.9645833373069763 test loss: 1.354831576347351 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3466 train loss: 1.0786948204040527 train acc: 0.9645833373069763 test loss: 1.3443794250488281 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3467 train loss: 1.078710675239563 train acc: 0.9645833373069763 test loss: 1.3415353298187256 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3468 train loss: 1.078579306602478 train acc: 0.9645833373069763 test loss: 1.345228672027588 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3469 train loss: 1.07852303981781 train acc: 0.9645833373069763 test loss: 1.341378092765808 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3470 train loss: 1.0785939693450928 train acc: 0.9645833373069763 test loss: 1.3277591466903687 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3471 train loss: 1.0785576105117798 train acc: 0.9645833373069763 test loss: 1.3405078649520874 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3472 train loss: 1.0785139799118042 train acc: 0.9645833373069763 test loss: 1.3506656885147095 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3473 train loss: 1.0787200927734375 train acc: 0.9645833373069763 test loss: 1.3342047929763794 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3474 train loss: 1.0785884857177734 train acc: 0.9645833373069763 test loss: 1.3563826084136963 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3475 train loss: 1.078442096710205 train acc: 0.9645833373069763 test loss: 1.3282665014266968 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3476 train loss: 1.0786479711532593 train acc: 0.9645833373069763 test loss: 1.3399783372879028 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3477 train loss: 1.0786021947860718 train acc: 0.9645833373069763 test loss: 1.319466233253479 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3478 train loss: 1.0785067081451416 train acc: 0.9645833373069763 test loss: 1.3277167081832886 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3479 train loss: 1.0786082744598389 train acc: 0.9645833373069763 test loss: 1.313647747039795 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 3480 train loss: 1.0785250663757324 train acc: 0.9645833373069763 test loss: 1.3443825244903564 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3481 train loss: 1.0786070823669434 train acc: 0.9645833373069763 test loss: 1.3298934698104858 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3482 train loss: 1.0784825086593628 train acc: 0.9645833373069763 test loss: 1.338449239730835 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3483 train loss: 1.0784955024719238 train acc: 0.9645833373069763 test loss: 1.3499149084091187 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3484 train loss: 1.0786521434783936 train acc: 0.9645833373069763 test loss: 1.3508498668670654 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3485 train loss: 1.0786303281784058 train acc: 0.9645833373069763 test loss: 1.3428579568862915 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3486 train loss: 1.078452467918396 train acc: 0.9645833373069763 test loss: 1.3272862434387207 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3487 train loss: 1.0783555507659912 train acc: 0.9645833373069763 test loss: 1.3507181406021118 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3488 train loss: 1.0786081552505493 train acc: 0.9645833373069763 test loss: 1.3315426111221313 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3489 train loss: 1.0785940885543823 train acc: 0.9645833373069763 test loss: 1.3260434865951538 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3490 train loss: 1.0786033868789673 train acc: 0.9645833373069763 test loss: 1.329097867012024 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3491 train loss: 1.0786741971969604 train acc: 0.9645833373069763 test loss: 1.3416606187820435 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3492 train loss: 1.0785750150680542 train acc: 0.9645833373069763 test loss: 1.3422489166259766 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3493 train loss: 1.0784026384353638 train acc: 0.9645833373069763 test loss: 1.3518272638320923 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3494 train loss: 1.0785828828811646 train acc: 0.9645833373069763 test loss: 1.3593865633010864 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3495 train loss: 1.0783685445785522 train acc: 0.9645833373069763 test loss: 1.3404765129089355 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3496 train loss: 1.0786075592041016 train acc: 0.9645833373069763 test loss: 1.3397884368896484 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3497 train loss: 1.0785062313079834 train acc: 0.9645833373069763 test loss: 1.3578715324401855 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3498 train loss: 1.0783449411392212 train acc: 0.9645833373069763 test loss: 1.333101749420166 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3499 train loss: 1.0784413814544678 train acc: 0.9645833373069763 test loss: 1.3407113552093506 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3500 train loss: 1.0786025524139404 train acc: 0.9645833373069763 test loss: 1.3181664943695068 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3501 train loss: 1.0785027742385864 train acc: 0.9645833373069763 test loss: 1.3360767364501953 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3502 train loss: 1.0784245729446411 train acc: 0.9645833373069763 test loss: 1.3376524448394775 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3503 train loss: 1.0784308910369873 train acc: 0.9645833373069763 test loss: 1.3484264612197876 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3504 train loss: 1.0786973237991333 train acc: 0.9645833373069763 test loss: 1.349352478981018 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3505 train loss: 1.0785033702850342 train acc: 0.9645833373069763 test loss: 1.3423196077346802 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3506 train loss: 1.07850980758667 train acc: 0.9645833373069763 test loss: 1.3348315954208374 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3507 train loss: 1.0787227153778076 train acc: 0.9645833373069763 test loss: 1.3405814170837402 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3508 train loss: 1.0786014795303345 train acc: 0.9645833373069763 test loss: 1.3292045593261719 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3509 train loss: 1.0784329175949097 train acc: 0.9645833373069763 test loss: 1.3396403789520264 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3510 train loss: 1.0785560607910156 train acc: 0.9645833373069763 test loss: 1.3285644054412842 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3511 train loss: 1.0785659551620483 train acc: 0.9645833373069763 test loss: 1.34355890750885 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3512 train loss: 1.0784398317337036 train acc: 0.9645833373069763 test loss: 1.3395476341247559 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3513 train loss: 1.0785026550292969 train acc: 0.9645833373069763 test loss: 1.3572266101837158 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3514 train loss: 1.0784655809402466 train acc: 0.9645833373069763 test loss: 1.323346495628357 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3515 train loss: 1.0785199403762817 train acc: 0.9645833373069763 test loss: 1.3320012092590332 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3516 train loss: 1.0786381959915161 train acc: 0.9645833373069763 test loss: 1.362999439239502 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3517 train loss: 1.0787646770477295 train acc: 0.9645833373069763 test loss: 1.3523467779159546 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3518 train loss: 1.0783337354660034 train acc: 0.9645833373069763 test loss: 1.3427640199661255 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3519 train loss: 1.0784738063812256 train acc: 0.9645833373069763 test loss: 1.3519768714904785 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3520 train loss: 1.0787937641143799 train acc: 0.9645833373069763 test loss: 1.3517616987228394 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3521 train loss: 1.0785200595855713 train acc: 0.9645833373069763 test loss: 1.3599083423614502 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3522 train loss: 1.0784341096878052 train acc: 0.9645833373069763 test loss: 1.3383702039718628 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3523 train loss: 1.0785810947418213 train acc: 0.9645833373069763 test loss: 1.353339433670044 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3524 train loss: 1.0785777568817139 train acc: 0.9645833373069763 test loss: 1.3561931848526 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3525 train loss: 1.078521490097046 train acc: 0.9645833373069763 test loss: 1.3228065967559814 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3526 train loss: 1.078550934791565 train acc: 0.9645833373069763 test loss: 1.3561537265777588 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3527 train loss: 1.078366994857788 train acc: 0.9645833373069763 test loss: 1.3466126918792725 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3528 train loss: 1.0786595344543457 train acc: 0.9645833373069763 test loss: 1.3477078676223755 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3529 train loss: 1.0786796808242798 train acc: 0.9645833373069763 test loss: 1.3411147594451904 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3530 train loss: 1.078473448753357 train acc: 0.9645833373069763 test loss: 1.3294669389724731 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3531 train loss: 1.0786536931991577 train acc: 0.9645833373069763 test loss: 1.3525604009628296 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3532 train loss: 1.0784554481506348 train acc: 0.9645833373069763 test loss: 1.3330974578857422 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3533 train loss: 1.078635573387146 train acc: 0.9645833373069763 test loss: 1.3398991823196411 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3534 train loss: 1.0783559083938599 train acc: 0.9645833373069763 test loss: 1.3391600847244263 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3535 train loss: 1.0785866975784302 train acc: 0.9645833373069763 test loss: 1.3437654972076416 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3536 train loss: 1.0784932374954224 train acc: 0.9645833373069763 test loss: 1.3268760442733765 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3537 train loss: 1.0785844326019287 train acc: 0.9645833373069763 test loss: 1.3354603052139282 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3538 train loss: 1.0784094333648682 train acc: 0.9645833373069763 test loss: 1.3418183326721191 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3539 train loss: 1.0785298347473145 train acc: 0.9645833373069763 test loss: 1.3518966436386108 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3540 train loss: 1.0786049365997314 train acc: 0.9645833373069763 test loss: 1.3326416015625 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3541 train loss: 1.0786232948303223 train acc: 0.9645833373069763 test loss: 1.3349196910858154 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3542 train loss: 1.078613042831421 train acc: 0.9645833373069763 test loss: 1.323418140411377 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3543 train loss: 1.0785374641418457 train acc: 0.9645833373069763 test loss: 1.3411496877670288 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3544 train loss: 1.0784422159194946 train acc: 0.9645833373069763 test loss: 1.3409327268600464 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3545 train loss: 1.0784927606582642 train acc: 0.9645833373069763 test loss: 1.3405015468597412 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3546 train loss: 1.0784591436386108 train acc: 0.9645833373069763 test loss: 1.3393211364746094 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3547 train loss: 1.0785744190216064 train acc: 0.9645833373069763 test loss: 1.3301533460617065 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3548 train loss: 1.0784446001052856 train acc: 0.9645833373069763 test loss: 1.3201556205749512 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3549 train loss: 1.0785331726074219 train acc: 0.9645833373069763 test loss: 1.3506425619125366 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3550 train loss: 1.07856285572052 train acc: 0.9645833373069763 test loss: 1.3491114377975464 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3551 train loss: 1.0786309242248535 train acc: 0.9645833373069763 test loss: 1.347809076309204 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3552 train loss: 1.0787898302078247 train acc: 0.9645833373069763 test loss: 1.3529622554779053 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3553 train loss: 1.0786725282669067 train acc: 0.9645833373069763 test loss: 1.3211514949798584 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3554 train loss: 1.078569769859314 train acc: 0.9645833373069763 test loss: 1.3419924974441528 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3555 train loss: 1.0785950422286987 train acc: 0.9645833373069763 test loss: 1.3553473949432373 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3556 train loss: 1.0785213708877563 train acc: 0.9645833373069763 test loss: 1.3390426635742188 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3557 train loss: 1.078636646270752 train acc: 0.9645833373069763 test loss: 1.3202303647994995 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3558 train loss: 1.0784929990768433 train acc: 0.9645833373069763 test loss: 1.337457299232483 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3559 train loss: 1.0785785913467407 train acc: 0.9645833373069763 test loss: 1.323320984840393 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3560 train loss: 1.0785635709762573 train acc: 0.9645833373069763 test loss: 1.3311787843704224 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3561 train loss: 1.0786726474761963 train acc: 0.9645833373069763 test loss: 1.3258036375045776 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3562 train loss: 1.0785863399505615 train acc: 0.9645833373069763 test loss: 1.32999587059021 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3563 train loss: 1.0785496234893799 train acc: 0.9645833373069763 test loss: 1.3270965814590454 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3564 train loss: 1.0786422491073608 train acc: 0.9645833373069763 test loss: 1.3384315967559814 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3565 train loss: 1.0786187648773193 train acc: 0.9645833373069763 test loss: 1.3417010307312012 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3566 train loss: 1.0785492658615112 train acc: 0.9645833373069763 test loss: 1.3094569444656372 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 3567 train loss: 1.0786478519439697 train acc: 0.9645833373069763 test loss: 1.3217089176177979 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3568 train loss: 1.0785642862319946 train acc: 0.9645833373069763 test loss: 1.3204798698425293 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3569 train loss: 1.078667402267456 train acc: 0.9645833373069763 test loss: 1.3039599657058716 best test loss: 1.283445954322815 test acc: 0.7583333253860474\n",
      "Epoch 3570 train loss: 1.0784869194030762 train acc: 0.9645833373069763 test loss: 1.3466030359268188 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3571 train loss: 1.0785467624664307 train acc: 0.9645833373069763 test loss: 1.331609845161438 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3572 train loss: 1.0786144733428955 train acc: 0.9645833373069763 test loss: 1.3302284479141235 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3573 train loss: 1.078550100326538 train acc: 0.9645833373069763 test loss: 1.3312369585037231 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3574 train loss: 1.07863187789917 train acc: 0.9645833373069763 test loss: 1.345220685005188 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3575 train loss: 1.0785995721817017 train acc: 0.9645833373069763 test loss: 1.3327559232711792 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3576 train loss: 1.0786125659942627 train acc: 0.9645833373069763 test loss: 1.3260668516159058 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3577 train loss: 1.0786106586456299 train acc: 0.9645833373069763 test loss: 1.3440054655075073 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3578 train loss: 1.078421950340271 train acc: 0.9645833373069763 test loss: 1.3204716444015503 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3579 train loss: 1.0787490606307983 train acc: 0.9645833373069763 test loss: 1.3244181871414185 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3580 train loss: 1.0787159204483032 train acc: 0.9645833373069763 test loss: 1.339398980140686 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3581 train loss: 1.078462839126587 train acc: 0.9645833373069763 test loss: 1.3579925298690796 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3582 train loss: 1.0784605741500854 train acc: 0.9645833373069763 test loss: 1.3301386833190918 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3583 train loss: 1.0785911083221436 train acc: 0.9645833373069763 test loss: 1.3230866193771362 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3584 train loss: 1.0785173177719116 train acc: 0.9645833373069763 test loss: 1.3182451725006104 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3585 train loss: 1.0785735845565796 train acc: 0.9645833373069763 test loss: 1.343044638633728 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3586 train loss: 1.0785326957702637 train acc: 0.9645833373069763 test loss: 1.3540844917297363 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3587 train loss: 1.0785393714904785 train acc: 0.9645833373069763 test loss: 1.3271414041519165 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3588 train loss: 1.0784752368927002 train acc: 0.9645833373069763 test loss: 1.340760588645935 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3589 train loss: 1.0784434080123901 train acc: 0.9645833373069763 test loss: 1.3573938608169556 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3590 train loss: 1.0786011219024658 train acc: 0.9645833373069763 test loss: 1.337547779083252 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3591 train loss: 1.0784751176834106 train acc: 0.9645833373069763 test loss: 1.3272252082824707 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3592 train loss: 1.0784618854522705 train acc: 0.9645833373069763 test loss: 1.332380771636963 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3593 train loss: 1.0784491300582886 train acc: 0.9645833373069763 test loss: 1.3308418989181519 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3594 train loss: 1.0784411430358887 train acc: 0.9645833373069763 test loss: 1.3492895364761353 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3595 train loss: 1.0785177946090698 train acc: 0.9645833373069763 test loss: 1.3578975200653076 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3596 train loss: 1.0785160064697266 train acc: 0.9645833373069763 test loss: 1.3343379497528076 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3597 train loss: 1.0787171125411987 train acc: 0.9645833373069763 test loss: 1.3498847484588623 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3598 train loss: 1.0784835815429688 train acc: 0.9645833373069763 test loss: 1.3498806953430176 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3599 train loss: 1.0785030126571655 train acc: 0.9645833373069763 test loss: 1.342138409614563 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3600 train loss: 1.0785881280899048 train acc: 0.9645833373069763 test loss: 1.3468737602233887 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3601 train loss: 1.0785304307937622 train acc: 0.9645833373069763 test loss: 1.3474030494689941 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3602 train loss: 1.0783963203430176 train acc: 0.9645833373069763 test loss: 1.3377535343170166 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3603 train loss: 1.07858407497406 train acc: 0.9645833373069763 test loss: 1.3456077575683594 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3604 train loss: 1.0785959959030151 train acc: 0.9645833373069763 test loss: 1.3399434089660645 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3605 train loss: 1.0785295963287354 train acc: 0.9645833373069763 test loss: 1.3363322019577026 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3606 train loss: 1.0786755084991455 train acc: 0.9645833373069763 test loss: 1.3424988985061646 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3607 train loss: 1.0785801410675049 train acc: 0.9645833373069763 test loss: 1.3463866710662842 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3608 train loss: 1.0785508155822754 train acc: 0.9645833373069763 test loss: 1.3349870443344116 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3609 train loss: 1.0784815549850464 train acc: 0.9645833373069763 test loss: 1.3345738649368286 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3610 train loss: 1.0785969495773315 train acc: 0.9645833373069763 test loss: 1.3337620496749878 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3611 train loss: 1.0786174535751343 train acc: 0.9645833373069763 test loss: 1.3361265659332275 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3612 train loss: 1.0785300731658936 train acc: 0.9645833373069763 test loss: 1.3385666608810425 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3613 train loss: 1.078549861907959 train acc: 0.9645833373069763 test loss: 1.356062650680542 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3614 train loss: 1.0785738229751587 train acc: 0.9645833373069763 test loss: 1.3393269777297974 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3615 train loss: 1.0784132480621338 train acc: 0.9645833373069763 test loss: 1.345822811126709 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3616 train loss: 1.0785635709762573 train acc: 0.9645833373069763 test loss: 1.3477122783660889 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3617 train loss: 1.0784759521484375 train acc: 0.9645833373069763 test loss: 1.3403383493423462 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3618 train loss: 1.0786473751068115 train acc: 0.9645833373069763 test loss: 1.3410592079162598 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3619 train loss: 1.0787302255630493 train acc: 0.9645833373069763 test loss: 1.3397560119628906 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3620 train loss: 1.0785397291183472 train acc: 0.9645833373069763 test loss: 1.3523800373077393 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3621 train loss: 1.078531265258789 train acc: 0.9645833373069763 test loss: 1.3434027433395386 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3622 train loss: 1.0786701440811157 train acc: 0.9645833373069763 test loss: 1.3458091020584106 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3623 train loss: 1.0785032510757446 train acc: 0.9645833373069763 test loss: 1.3327562808990479 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3624 train loss: 1.0785726308822632 train acc: 0.9645833373069763 test loss: 1.3287714719772339 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3625 train loss: 1.0785343647003174 train acc: 0.9645833373069763 test loss: 1.3309078216552734 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3626 train loss: 1.0784475803375244 train acc: 0.9645833373069763 test loss: 1.3493586778640747 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3627 train loss: 1.078526258468628 train acc: 0.9645833373069763 test loss: 1.343980073928833 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3628 train loss: 1.0784144401550293 train acc: 0.9645833373069763 test loss: 1.3363873958587646 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3629 train loss: 1.0785826444625854 train acc: 0.9645833373069763 test loss: 1.3469005823135376 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3630 train loss: 1.0784509181976318 train acc: 0.9645833373069763 test loss: 1.3390573263168335 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3631 train loss: 1.0784118175506592 train acc: 0.9645833373069763 test loss: 1.340126395225525 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3632 train loss: 1.0783625841140747 train acc: 0.9645833373069763 test loss: 1.3357243537902832 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3633 train loss: 1.0785163640975952 train acc: 0.9645833373069763 test loss: 1.3469882011413574 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3634 train loss: 1.0785772800445557 train acc: 0.9645833373069763 test loss: 1.3422603607177734 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3635 train loss: 1.0786559581756592 train acc: 0.9645833373069763 test loss: 1.351986289024353 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3636 train loss: 1.078541874885559 train acc: 0.9645833373069763 test loss: 1.3457427024841309 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3637 train loss: 1.0787142515182495 train acc: 0.9645833373069763 test loss: 1.3481806516647339 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3638 train loss: 1.0785953998565674 train acc: 0.9645833373069763 test loss: 1.3380465507507324 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3639 train loss: 1.0786455869674683 train acc: 0.9645833373069763 test loss: 1.3249977827072144 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3640 train loss: 1.078369379043579 train acc: 0.9645833373069763 test loss: 1.3488131761550903 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3641 train loss: 1.0785655975341797 train acc: 0.9645833373069763 test loss: 1.3394328355789185 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3642 train loss: 1.0785915851593018 train acc: 0.9645833373069763 test loss: 1.3383575677871704 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3643 train loss: 1.0784622430801392 train acc: 0.9645833373069763 test loss: 1.3410193920135498 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3644 train loss: 1.0784728527069092 train acc: 0.9645833373069763 test loss: 1.3347293138504028 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3645 train loss: 1.0783748626708984 train acc: 0.9645833373069763 test loss: 1.324839472770691 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3646 train loss: 1.0784960985183716 train acc: 0.9645833373069763 test loss: 1.3337162733078003 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3647 train loss: 1.0785590410232544 train acc: 0.9645833373069763 test loss: 1.345546841621399 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3648 train loss: 1.0785443782806396 train acc: 0.9645833373069763 test loss: 1.3371089696884155 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3649 train loss: 1.0784507989883423 train acc: 0.9645833373069763 test loss: 1.335530400276184 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3650 train loss: 1.0785835981369019 train acc: 0.9645833373069763 test loss: 1.3399275541305542 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3651 train loss: 1.0786213874816895 train acc: 0.9645833373069763 test loss: 1.3378098011016846 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3652 train loss: 1.0786319971084595 train acc: 0.9645833373069763 test loss: 1.3416805267333984 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3653 train loss: 1.0786004066467285 train acc: 0.9645833373069763 test loss: 1.33633553981781 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3654 train loss: 1.078481912612915 train acc: 0.9645833373069763 test loss: 1.337957739830017 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3655 train loss: 1.0785270929336548 train acc: 0.9645833373069763 test loss: 1.3422554731369019 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3656 train loss: 1.078347086906433 train acc: 0.9645833373069763 test loss: 1.33608877658844 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3657 train loss: 1.0786463022232056 train acc: 0.9645833373069763 test loss: 1.3524388074874878 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3658 train loss: 1.0784260034561157 train acc: 0.9645833373069763 test loss: 1.3434566259384155 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3659 train loss: 1.0784225463867188 train acc: 0.9645833373069763 test loss: 1.341291069984436 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3660 train loss: 1.0785456895828247 train acc: 0.9645833373069763 test loss: 1.3386389017105103 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3661 train loss: 1.0783435106277466 train acc: 0.9645833373069763 test loss: 1.32961106300354 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3662 train loss: 1.0785762071609497 train acc: 0.9645833373069763 test loss: 1.3480180501937866 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3663 train loss: 1.0786875486373901 train acc: 0.9645833373069763 test loss: 1.3296345472335815 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3664 train loss: 1.0786182880401611 train acc: 0.9645833373069763 test loss: 1.3544763326644897 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3665 train loss: 1.0785034894943237 train acc: 0.9645833373069763 test loss: 1.3419018983840942 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3666 train loss: 1.0784589052200317 train acc: 0.9645833373069763 test loss: 1.3390944004058838 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3667 train loss: 1.0786139965057373 train acc: 0.9645833373069763 test loss: 1.3587478399276733 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3668 train loss: 1.0784084796905518 train acc: 0.9645833373069763 test loss: 1.3336347341537476 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3669 train loss: 1.0785515308380127 train acc: 0.9645833373069763 test loss: 1.3280539512634277 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3670 train loss: 1.078429102897644 train acc: 0.9645833373069763 test loss: 1.353637456893921 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3671 train loss: 1.0785105228424072 train acc: 0.9645833373069763 test loss: 1.3395851850509644 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3672 train loss: 1.0785332918167114 train acc: 0.9645833373069763 test loss: 1.3505584001541138 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3673 train loss: 1.0785102844238281 train acc: 0.9645833373069763 test loss: 1.324265956878662 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3674 train loss: 1.0785878896713257 train acc: 0.9645833373069763 test loss: 1.3428120613098145 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3675 train loss: 1.0785481929779053 train acc: 0.9645833373069763 test loss: 1.3248471021652222 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3676 train loss: 1.078521490097046 train acc: 0.9645833373069763 test loss: 1.3297871351242065 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3677 train loss: 1.0785516500473022 train acc: 0.9645833373069763 test loss: 1.3542014360427856 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3678 train loss: 1.0785636901855469 train acc: 0.9645833373069763 test loss: 1.3313922882080078 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3679 train loss: 1.0785002708435059 train acc: 0.9645833373069763 test loss: 1.3413537740707397 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3680 train loss: 1.0786341428756714 train acc: 0.9645833373069763 test loss: 1.3566367626190186 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3681 train loss: 1.078508973121643 train acc: 0.9645833373069763 test loss: 1.3422844409942627 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3682 train loss: 1.0783932209014893 train acc: 0.9645833373069763 test loss: 1.3429549932479858 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3683 train loss: 1.0784811973571777 train acc: 0.9645833373069763 test loss: 1.328124761581421 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3684 train loss: 1.0784621238708496 train acc: 0.9645833373069763 test loss: 1.3434183597564697 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3685 train loss: 1.078494906425476 train acc: 0.9645833373069763 test loss: 1.3453775644302368 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3686 train loss: 1.0784227848052979 train acc: 0.9645833373069763 test loss: 1.3490698337554932 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3687 train loss: 1.078489899635315 train acc: 0.9645833373069763 test loss: 1.3348220586776733 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3688 train loss: 1.0785319805145264 train acc: 0.9645833373069763 test loss: 1.3483598232269287 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3689 train loss: 1.0783588886260986 train acc: 0.9645833373069763 test loss: 1.33420991897583 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3690 train loss: 1.078583836555481 train acc: 0.9645833373069763 test loss: 1.341352939605713 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3691 train loss: 1.0785415172576904 train acc: 0.9645833373069763 test loss: 1.3337854146957397 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3692 train loss: 1.0785871744155884 train acc: 0.9645833373069763 test loss: 1.3430521488189697 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3693 train loss: 1.0783861875534058 train acc: 0.9645833373069763 test loss: 1.3492431640625 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3694 train loss: 1.0784739255905151 train acc: 0.9645833373069763 test loss: 1.3474916219711304 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3695 train loss: 1.0784252882003784 train acc: 0.9645833373069763 test loss: 1.3521848917007446 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3696 train loss: 1.0784579515457153 train acc: 0.9645833373069763 test loss: 1.3613159656524658 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3697 train loss: 1.078533411026001 train acc: 0.9645833373069763 test loss: 1.334951400756836 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3698 train loss: 1.078947901725769 train acc: 0.9645833373069763 test loss: 1.3425720930099487 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3699 train loss: 1.0784156322479248 train acc: 0.9645833373069763 test loss: 1.3419357538223267 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3700 train loss: 1.0784993171691895 train acc: 0.9645833373069763 test loss: 1.3425687551498413 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3701 train loss: 1.0785564184188843 train acc: 0.9645833373069763 test loss: 1.339105248451233 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3702 train loss: 1.0785388946533203 train acc: 0.9645833373069763 test loss: 1.3455138206481934 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3703 train loss: 1.078378438949585 train acc: 0.9645833373069763 test loss: 1.3351083993911743 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3704 train loss: 1.0785690546035767 train acc: 0.9645833373069763 test loss: 1.3586676120758057 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3705 train loss: 1.07846999168396 train acc: 0.9645833373069763 test loss: 1.332763433456421 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3706 train loss: 1.0784660577774048 train acc: 0.9645833373069763 test loss: 1.345869541168213 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3707 train loss: 1.0785762071609497 train acc: 0.9645833373069763 test loss: 1.3519699573516846 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3708 train loss: 1.078356385231018 train acc: 0.9645833373069763 test loss: 1.3614245653152466 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3709 train loss: 1.0783568620681763 train acc: 0.9645833373069763 test loss: 1.3340061902999878 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3710 train loss: 1.078451156616211 train acc: 0.9645833373069763 test loss: 1.3380733728408813 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3711 train loss: 1.0784075260162354 train acc: 0.9645833373069763 test loss: 1.3550046682357788 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3712 train loss: 1.0785890817642212 train acc: 0.9645833373069763 test loss: 1.3619762659072876 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3713 train loss: 1.0785279273986816 train acc: 0.9645833373069763 test loss: 1.3416088819503784 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3714 train loss: 1.0784828662872314 train acc: 0.9645833373069763 test loss: 1.3587883710861206 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3715 train loss: 1.0786179304122925 train acc: 0.9645833373069763 test loss: 1.3297057151794434 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3716 train loss: 1.0785328149795532 train acc: 0.9645833373069763 test loss: 1.3566677570343018 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3717 train loss: 1.078489899635315 train acc: 0.9645833373069763 test loss: 1.3315894603729248 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3718 train loss: 1.0785342454910278 train acc: 0.9645833373069763 test loss: 1.3457602262496948 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3719 train loss: 1.0785192251205444 train acc: 0.9645833373069763 test loss: 1.3418502807617188 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3720 train loss: 1.0785882472991943 train acc: 0.9645833373069763 test loss: 1.3407095670700073 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3721 train loss: 1.0784300565719604 train acc: 0.9645833373069763 test loss: 1.3332585096359253 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3722 train loss: 1.0785976648330688 train acc: 0.9645833373069763 test loss: 1.3430266380310059 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3723 train loss: 1.078591227531433 train acc: 0.9645833373069763 test loss: 1.3475050926208496 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3724 train loss: 1.078586459159851 train acc: 0.9645833373069763 test loss: 1.3452154397964478 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3725 train loss: 1.078447937965393 train acc: 0.9645833373069763 test loss: 1.350390076637268 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3726 train loss: 1.0784603357315063 train acc: 0.9645833373069763 test loss: 1.345058798789978 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3727 train loss: 1.078678846359253 train acc: 0.9645833373069763 test loss: 1.3441162109375 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3728 train loss: 1.0785839557647705 train acc: 0.9645833373069763 test loss: 1.3363364934921265 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3729 train loss: 1.078588843345642 train acc: 0.9645833373069763 test loss: 1.3376518487930298 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3730 train loss: 1.0785893201828003 train acc: 0.9645833373069763 test loss: 1.3370394706726074 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3731 train loss: 1.0785846710205078 train acc: 0.9645833373069763 test loss: 1.3379336595535278 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3732 train loss: 1.078546404838562 train acc: 0.9645833373069763 test loss: 1.3577522039413452 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3733 train loss: 1.0784517526626587 train acc: 0.9645833373069763 test loss: 1.341480016708374 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3734 train loss: 1.0784752368927002 train acc: 0.9645833373069763 test loss: 1.3467700481414795 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3735 train loss: 1.0785905122756958 train acc: 0.9645833373069763 test loss: 1.335227131843567 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3736 train loss: 1.0785176753997803 train acc: 0.9645833373069763 test loss: 1.3540695905685425 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3737 train loss: 1.0784980058670044 train acc: 0.9645833373069763 test loss: 1.3304882049560547 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3738 train loss: 1.0785244703292847 train acc: 0.9645833373069763 test loss: 1.3579813241958618 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3739 train loss: 1.0784974098205566 train acc: 0.9645833373069763 test loss: 1.353593349456787 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3740 train loss: 1.0784389972686768 train acc: 0.9645833373069763 test loss: 1.351599931716919 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3741 train loss: 1.0784294605255127 train acc: 0.9645833373069763 test loss: 1.3455814123153687 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3742 train loss: 1.0784804821014404 train acc: 0.9645833373069763 test loss: 1.3311291933059692 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3743 train loss: 1.078398585319519 train acc: 0.9645833373069763 test loss: 1.3455407619476318 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3744 train loss: 1.0784939527511597 train acc: 0.9645833373069763 test loss: 1.3377100229263306 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3745 train loss: 1.0787241458892822 train acc: 0.9645833373069763 test loss: 1.3493373394012451 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3746 train loss: 1.0783863067626953 train acc: 0.9645833373069763 test loss: 1.3389793634414673 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3747 train loss: 1.0786497592926025 train acc: 0.9645833373069763 test loss: 1.35221266746521 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3748 train loss: 1.0784754753112793 train acc: 0.9645833373069763 test loss: 1.347954511642456 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3749 train loss: 1.078497290611267 train acc: 0.9645833373069763 test loss: 1.3391268253326416 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3750 train loss: 1.078421711921692 train acc: 0.9645833373069763 test loss: 1.3521311283111572 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3751 train loss: 1.0785419940948486 train acc: 0.9645833373069763 test loss: 1.3344131708145142 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3752 train loss: 1.078656554222107 train acc: 0.9645833373069763 test loss: 1.3606207370758057 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3753 train loss: 1.0784872770309448 train acc: 0.9645833373069763 test loss: 1.3384803533554077 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3754 train loss: 1.0785362720489502 train acc: 0.9645833373069763 test loss: 1.3644627332687378 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 3755 train loss: 1.078497052192688 train acc: 0.9645833373069763 test loss: 1.3447070121765137 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3756 train loss: 1.078466534614563 train acc: 0.9645833373069763 test loss: 1.3571476936340332 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3757 train loss: 1.0784364938735962 train acc: 0.9645833373069763 test loss: 1.3249695301055908 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3758 train loss: 1.078573226928711 train acc: 0.9645833373069763 test loss: 1.3264389038085938 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3759 train loss: 1.0785143375396729 train acc: 0.9645833373069763 test loss: 1.3440204858779907 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3760 train loss: 1.0785272121429443 train acc: 0.9645833373069763 test loss: 1.3536486625671387 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3761 train loss: 1.078396201133728 train acc: 0.9645833373069763 test loss: 1.3248677253723145 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3762 train loss: 1.0784772634506226 train acc: 0.9645833373069763 test loss: 1.3405325412750244 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3763 train loss: 1.0786347389221191 train acc: 0.9645833373069763 test loss: 1.35080087184906 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3764 train loss: 1.0786303281784058 train acc: 0.9645833373069763 test loss: 1.3554571866989136 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3765 train loss: 1.0783878564834595 train acc: 0.9645833373069763 test loss: 1.3242372274398804 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3766 train loss: 1.078410029411316 train acc: 0.9645833373069763 test loss: 1.3351647853851318 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3767 train loss: 1.0784709453582764 train acc: 0.9645833373069763 test loss: 1.3229385614395142 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3768 train loss: 1.0785716772079468 train acc: 0.9645833373069763 test loss: 1.327029824256897 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3769 train loss: 1.0784560441970825 train acc: 0.9645833373069763 test loss: 1.3398200273513794 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3770 train loss: 1.0783231258392334 train acc: 0.9645833373069763 test loss: 1.316123127937317 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3771 train loss: 1.0785415172576904 train acc: 0.9645833373069763 test loss: 1.3456395864486694 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3772 train loss: 1.0782877206802368 train acc: 0.9645833373069763 test loss: 1.335840106010437 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3773 train loss: 1.0783555507659912 train acc: 0.9645833373069763 test loss: 1.3650543689727783 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 3774 train loss: 1.0784568786621094 train acc: 0.9645833373069763 test loss: 1.32895827293396 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3775 train loss: 1.0785295963287354 train acc: 0.9645833373069763 test loss: 1.3448768854141235 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3776 train loss: 1.0784538984298706 train acc: 0.9645833373069763 test loss: 1.3300144672393799 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3777 train loss: 1.078674077987671 train acc: 0.9645833373069763 test loss: 1.3527494668960571 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3778 train loss: 1.078510046005249 train acc: 0.9645833373069763 test loss: 1.3465389013290405 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3779 train loss: 1.078492522239685 train acc: 0.9645833373069763 test loss: 1.3391177654266357 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3780 train loss: 1.0785315036773682 train acc: 0.9645833373069763 test loss: 1.322954773902893 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3781 train loss: 1.0785120725631714 train acc: 0.9645833373069763 test loss: 1.3515714406967163 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3782 train loss: 1.0784718990325928 train acc: 0.9645833373069763 test loss: 1.320515751838684 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3783 train loss: 1.078552007675171 train acc: 0.9645833373069763 test loss: 1.3296009302139282 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3784 train loss: 1.078391671180725 train acc: 0.9645833373069763 test loss: 1.3411365747451782 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3785 train loss: 1.078688144683838 train acc: 0.9645833373069763 test loss: 1.3294025659561157 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3786 train loss: 1.0785447359085083 train acc: 0.9645833373069763 test loss: 1.3593250513076782 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3787 train loss: 1.078439474105835 train acc: 0.9645833373069763 test loss: 1.3355708122253418 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3788 train loss: 1.0783864259719849 train acc: 0.9645833373069763 test loss: 1.345611810684204 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3789 train loss: 1.0784776210784912 train acc: 0.9645833373069763 test loss: 1.3384058475494385 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3790 train loss: 1.0786420106887817 train acc: 0.9645833373069763 test loss: 1.3310742378234863 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3791 train loss: 1.0784584283828735 train acc: 0.9645833373069763 test loss: 1.3405109643936157 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3792 train loss: 1.0785566568374634 train acc: 0.9645833373069763 test loss: 1.3161170482635498 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3793 train loss: 1.078428030014038 train acc: 0.9645833373069763 test loss: 1.3332219123840332 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3794 train loss: 1.0784580707550049 train acc: 0.9645833373069763 test loss: 1.3547132015228271 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3795 train loss: 1.0784140825271606 train acc: 0.9645833373069763 test loss: 1.3452214002609253 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3796 train loss: 1.0783963203430176 train acc: 0.9645833373069763 test loss: 1.351398229598999 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3797 train loss: 1.0785545110702515 train acc: 0.9645833373069763 test loss: 1.3498140573501587 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3798 train loss: 1.0784165859222412 train acc: 0.9645833373069763 test loss: 1.3390002250671387 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3799 train loss: 1.0784634351730347 train acc: 0.9645833373069763 test loss: 1.3493714332580566 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3800 train loss: 1.0784564018249512 train acc: 0.9645833373069763 test loss: 1.3359947204589844 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3801 train loss: 1.0783408880233765 train acc: 0.9645833373069763 test loss: 1.3393281698226929 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3802 train loss: 1.0784587860107422 train acc: 0.9645833373069763 test loss: 1.336245059967041 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3803 train loss: 1.0784908533096313 train acc: 0.9645833373069763 test loss: 1.334970474243164 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3804 train loss: 1.0783158540725708 train acc: 0.9645833373069763 test loss: 1.3662259578704834 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3805 train loss: 1.078593134880066 train acc: 0.9645833373069763 test loss: 1.336635708808899 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3806 train loss: 1.078538179397583 train acc: 0.9645833373069763 test loss: 1.3602620363235474 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3807 train loss: 1.0785869359970093 train acc: 0.9645833373069763 test loss: 1.3416945934295654 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3808 train loss: 1.0785969495773315 train acc: 0.9645833373069763 test loss: 1.3521212339401245 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3809 train loss: 1.0784530639648438 train acc: 0.9645833373069763 test loss: 1.341057538986206 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3810 train loss: 1.078429102897644 train acc: 0.9645833373069763 test loss: 1.3411492109298706 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3811 train loss: 1.0786328315734863 train acc: 0.9645833373069763 test loss: 1.3365164995193481 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3812 train loss: 1.078564167022705 train acc: 0.9645833373069763 test loss: 1.3535484075546265 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3813 train loss: 1.0786423683166504 train acc: 0.9645833373069763 test loss: 1.3551833629608154 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3814 train loss: 1.0785810947418213 train acc: 0.9645833373069763 test loss: 1.3340935707092285 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3815 train loss: 1.0782662630081177 train acc: 0.9645833373069763 test loss: 1.3433010578155518 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3816 train loss: 1.0786926746368408 train acc: 0.9645833373069763 test loss: 1.3420740365982056 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3817 train loss: 1.0785614252090454 train acc: 0.9645833373069763 test loss: 1.3256030082702637 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3818 train loss: 1.0785260200500488 train acc: 0.9645833373069763 test loss: 1.3490748405456543 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3819 train loss: 1.0786571502685547 train acc: 0.9645833373069763 test loss: 1.343489408493042 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3820 train loss: 1.0785342454910278 train acc: 0.9645833373069763 test loss: 1.3366737365722656 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3821 train loss: 1.078366756439209 train acc: 0.9645833373069763 test loss: 1.3445278406143188 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3822 train loss: 1.0785232782363892 train acc: 0.9645833373069763 test loss: 1.3384509086608887 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3823 train loss: 1.0784164667129517 train acc: 0.9645833373069763 test loss: 1.3310093879699707 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3824 train loss: 1.0785654783248901 train acc: 0.9645833373069763 test loss: 1.3537312746047974 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3825 train loss: 1.0784776210784912 train acc: 0.9645833373069763 test loss: 1.3274599313735962 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3826 train loss: 1.0783370733261108 train acc: 0.9645833373069763 test loss: 1.3374186754226685 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3827 train loss: 1.078386664390564 train acc: 0.9645833373069763 test loss: 1.3299673795700073 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3828 train loss: 1.0785795450210571 train acc: 0.9645833373069763 test loss: 1.342431902885437 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3829 train loss: 1.0786038637161255 train acc: 0.9645833373069763 test loss: 1.3360389471054077 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3830 train loss: 1.0784605741500854 train acc: 0.9645833373069763 test loss: 1.339110016822815 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3831 train loss: 1.0784085988998413 train acc: 0.9645833373069763 test loss: 1.3246957063674927 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3832 train loss: 1.0785282850265503 train acc: 0.9645833373069763 test loss: 1.3470804691314697 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3833 train loss: 1.078489065170288 train acc: 0.9645833373069763 test loss: 1.334102749824524 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3834 train loss: 1.0784623622894287 train acc: 0.9645833373069763 test loss: 1.326454758644104 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3835 train loss: 1.0784529447555542 train acc: 0.9645833373069763 test loss: 1.3619365692138672 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3836 train loss: 1.0784960985183716 train acc: 0.9645833373069763 test loss: 1.339958906173706 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3837 train loss: 1.0784627199172974 train acc: 0.9645833373069763 test loss: 1.3455517292022705 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3838 train loss: 1.07852041721344 train acc: 0.9645833373069763 test loss: 1.3368439674377441 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3839 train loss: 1.0785220861434937 train acc: 0.9645833373069763 test loss: 1.3395508527755737 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3840 train loss: 1.078507661819458 train acc: 0.9645833373069763 test loss: 1.3595887422561646 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3841 train loss: 1.0785349607467651 train acc: 0.9645833373069763 test loss: 1.3473856449127197 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3842 train loss: 1.0783536434173584 train acc: 0.9645833373069763 test loss: 1.3576608896255493 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3843 train loss: 1.0783933401107788 train acc: 0.9645833373069763 test loss: 1.351204514503479 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3844 train loss: 1.0786316394805908 train acc: 0.9645833373069763 test loss: 1.3339914083480835 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3845 train loss: 1.07852041721344 train acc: 0.9645833373069763 test loss: 1.3237336874008179 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3846 train loss: 1.0784993171691895 train acc: 0.9645833373069763 test loss: 1.3409448862075806 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3847 train loss: 1.0784441232681274 train acc: 0.9645833373069763 test loss: 1.3479971885681152 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3848 train loss: 1.0785495042800903 train acc: 0.9645833373069763 test loss: 1.3342318534851074 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3849 train loss: 1.0784175395965576 train acc: 0.9645833373069763 test loss: 1.346387267112732 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3850 train loss: 1.0786770582199097 train acc: 0.9645833373069763 test loss: 1.3516167402267456 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3851 train loss: 1.0784302949905396 train acc: 0.9645833373069763 test loss: 1.33476722240448 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3852 train loss: 1.0785762071609497 train acc: 0.9645833373069763 test loss: 1.3440831899642944 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3853 train loss: 1.0785268545150757 train acc: 0.9645833373069763 test loss: 1.3383750915527344 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3854 train loss: 1.0784186124801636 train acc: 0.9645833373069763 test loss: 1.3366053104400635 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3855 train loss: 1.0785616636276245 train acc: 0.9645833373069763 test loss: 1.3441416025161743 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3856 train loss: 1.0786935091018677 train acc: 0.9645833373069763 test loss: 1.3436566591262817 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3857 train loss: 1.0785642862319946 train acc: 0.9645833373069763 test loss: 1.3429625034332275 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3858 train loss: 1.0783920288085938 train acc: 0.9645833373069763 test loss: 1.3442471027374268 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3859 train loss: 1.0782743692398071 train acc: 0.9645833373069763 test loss: 1.3488539457321167 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3860 train loss: 1.0784432888031006 train acc: 0.9645833373069763 test loss: 1.335329294204712 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3861 train loss: 1.0783995389938354 train acc: 0.9645833373069763 test loss: 1.3300285339355469 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3862 train loss: 1.0786641836166382 train acc: 0.9645833373069763 test loss: 1.3373616933822632 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3863 train loss: 1.0785748958587646 train acc: 0.9645833373069763 test loss: 1.3508224487304688 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3864 train loss: 1.078561782836914 train acc: 0.9645833373069763 test loss: 1.348063588142395 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3865 train loss: 1.078533411026001 train acc: 0.9645833373069763 test loss: 1.3445082902908325 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3866 train loss: 1.0786274671554565 train acc: 0.9645833373069763 test loss: 1.3462719917297363 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3867 train loss: 1.0784332752227783 train acc: 0.9645833373069763 test loss: 1.3389493227005005 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3868 train loss: 1.0784536600112915 train acc: 0.9645833373069763 test loss: 1.3522204160690308 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3869 train loss: 1.0784122943878174 train acc: 0.9645833373069763 test loss: 1.343318223953247 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3870 train loss: 1.0786373615264893 train acc: 0.9645833373069763 test loss: 1.3417870998382568 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3871 train loss: 1.0784046649932861 train acc: 0.9645833373069763 test loss: 1.3303269147872925 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3872 train loss: 1.0784574747085571 train acc: 0.9645833373069763 test loss: 1.3366621732711792 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3873 train loss: 1.0786359310150146 train acc: 0.9645833373069763 test loss: 1.3513832092285156 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3874 train loss: 1.0786678791046143 train acc: 0.9645833373069763 test loss: 1.3469618558883667 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3875 train loss: 1.0784363746643066 train acc: 0.9645833373069763 test loss: 1.3337737321853638 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3876 train loss: 1.0784159898757935 train acc: 0.9645833373069763 test loss: 1.3231972455978394 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3877 train loss: 1.0786097049713135 train acc: 0.9645833373069763 test loss: 1.3508309125900269 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3878 train loss: 1.078454613685608 train acc: 0.9645833373069763 test loss: 1.3426569700241089 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3879 train loss: 1.0785470008850098 train acc: 0.9645833373069763 test loss: 1.3336656093597412 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3880 train loss: 1.0785019397735596 train acc: 0.9645833373069763 test loss: 1.3487378358840942 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3881 train loss: 1.07841956615448 train acc: 0.9645833373069763 test loss: 1.3354488611221313 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3882 train loss: 1.0783922672271729 train acc: 0.9645833373069763 test loss: 1.3350789546966553 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3883 train loss: 1.0785295963287354 train acc: 0.9645833373069763 test loss: 1.3398399353027344 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3884 train loss: 1.0784412622451782 train acc: 0.9645833373069763 test loss: 1.3356209993362427 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3885 train loss: 1.0785083770751953 train acc: 0.9645833373069763 test loss: 1.3448606729507446 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3886 train loss: 1.0785218477249146 train acc: 0.9645833373069763 test loss: 1.3361461162567139 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3887 train loss: 1.0786716938018799 train acc: 0.9645833373069763 test loss: 1.3340905904769897 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3888 train loss: 1.0785136222839355 train acc: 0.9645833373069763 test loss: 1.3376778364181519 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3889 train loss: 1.0784356594085693 train acc: 0.9645833373069763 test loss: 1.3275725841522217 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3890 train loss: 1.078404188156128 train acc: 0.9645833373069763 test loss: 1.339396357536316 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3891 train loss: 1.0785232782363892 train acc: 0.9645833373069763 test loss: 1.3206502199172974 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 3892 train loss: 1.0785183906555176 train acc: 0.9645833373069763 test loss: 1.354216456413269 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3893 train loss: 1.0786290168762207 train acc: 0.9645833373069763 test loss: 1.3429676294326782 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3894 train loss: 1.0784037113189697 train acc: 0.9645833373069763 test loss: 1.3529690504074097 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3895 train loss: 1.0785791873931885 train acc: 0.9645833373069763 test loss: 1.3490500450134277 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3896 train loss: 1.0786892175674438 train acc: 0.9645833373069763 test loss: 1.3314334154129028 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3897 train loss: 1.0784876346588135 train acc: 0.9645833373069763 test loss: 1.338031530380249 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3898 train loss: 1.0785647630691528 train acc: 0.9645833373069763 test loss: 1.3527710437774658 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3899 train loss: 1.0786049365997314 train acc: 0.9645833373069763 test loss: 1.3356684446334839 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3900 train loss: 1.0785560607910156 train acc: 0.9645833373069763 test loss: 1.3345892429351807 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3901 train loss: 1.0783740282058716 train acc: 0.9645833373069763 test loss: 1.3291270732879639 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3902 train loss: 1.0786212682724 train acc: 0.9645833373069763 test loss: 1.344602108001709 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3903 train loss: 1.078656792640686 train acc: 0.9645833373069763 test loss: 1.3599599599838257 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3904 train loss: 1.0784430503845215 train acc: 0.9645833373069763 test loss: 1.3549097776412964 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3905 train loss: 1.0785231590270996 train acc: 0.9645833373069763 test loss: 1.323639988899231 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3906 train loss: 1.078457236289978 train acc: 0.9645833373069763 test loss: 1.3337821960449219 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3907 train loss: 1.0786592960357666 train acc: 0.9645833373069763 test loss: 1.334694504737854 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3908 train loss: 1.078464150428772 train acc: 0.9645833373069763 test loss: 1.3378969430923462 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3909 train loss: 1.0785478353500366 train acc: 0.9645833373069763 test loss: 1.3318703174591064 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3910 train loss: 1.0785298347473145 train acc: 0.9645833373069763 test loss: 1.3264374732971191 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3911 train loss: 1.078428864479065 train acc: 0.9645833373069763 test loss: 1.355602502822876 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3912 train loss: 1.0784422159194946 train acc: 0.9645833373069763 test loss: 1.3578635454177856 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3913 train loss: 1.0785212516784668 train acc: 0.9645833373069763 test loss: 1.3439081907272339 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3914 train loss: 1.0786386728286743 train acc: 0.9645833373069763 test loss: 1.3462058305740356 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3915 train loss: 1.0784714221954346 train acc: 0.9645833373069763 test loss: 1.3318235874176025 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3916 train loss: 1.078546166419983 train acc: 0.9645833373069763 test loss: 1.355476975440979 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3917 train loss: 1.078520655632019 train acc: 0.9645833373069763 test loss: 1.3444119691848755 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3918 train loss: 1.0785845518112183 train acc: 0.9645833373069763 test loss: 1.3297299146652222 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3919 train loss: 1.0785351991653442 train acc: 0.9645833373069763 test loss: 1.3610073328018188 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3920 train loss: 1.0784670114517212 train acc: 0.9645833373069763 test loss: 1.333482027053833 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3921 train loss: 1.0783965587615967 train acc: 0.9645833373069763 test loss: 1.3277239799499512 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3922 train loss: 1.078526258468628 train acc: 0.9645833373069763 test loss: 1.3514626026153564 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3923 train loss: 1.078526258468628 train acc: 0.9645833373069763 test loss: 1.3451237678527832 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3924 train loss: 1.0784580707550049 train acc: 0.9645833373069763 test loss: 1.354274868965149 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3925 train loss: 1.0786397457122803 train acc: 0.9645833373069763 test loss: 1.3397412300109863 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3926 train loss: 1.0784471035003662 train acc: 0.9645833373069763 test loss: 1.3330777883529663 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3927 train loss: 1.0785630941390991 train acc: 0.9645833373069763 test loss: 1.350013256072998 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3928 train loss: 1.0786492824554443 train acc: 0.9645833373069763 test loss: 1.3409069776535034 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3929 train loss: 1.0784705877304077 train acc: 0.9645833373069763 test loss: 1.3480664491653442 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3930 train loss: 1.0784071683883667 train acc: 0.9645833373069763 test loss: 1.3536731004714966 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3931 train loss: 1.0785139799118042 train acc: 0.9645833373069763 test loss: 1.360320806503296 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3932 train loss: 1.0785386562347412 train acc: 0.9645833373069763 test loss: 1.3335696458816528 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3933 train loss: 1.0785250663757324 train acc: 0.9645833373069763 test loss: 1.3429679870605469 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3934 train loss: 1.0786552429199219 train acc: 0.9645833373069763 test loss: 1.3299715518951416 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3935 train loss: 1.0784480571746826 train acc: 0.9645833373069763 test loss: 1.3718363046646118 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3936 train loss: 1.0784022808074951 train acc: 0.9645833373069763 test loss: 1.3452186584472656 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3937 train loss: 1.0784856081008911 train acc: 0.9645833373069763 test loss: 1.3387227058410645 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3938 train loss: 1.0786038637161255 train acc: 0.9645833373069763 test loss: 1.3438352346420288 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3939 train loss: 1.0786151885986328 train acc: 0.9645833373069763 test loss: 1.338516354560852 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3940 train loss: 1.0785157680511475 train acc: 0.9645833373069763 test loss: 1.3292975425720215 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3941 train loss: 1.078485369682312 train acc: 0.9645833373069763 test loss: 1.336079478263855 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3942 train loss: 1.0784655809402466 train acc: 0.9645833373069763 test loss: 1.3489577770233154 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3943 train loss: 1.0785658359527588 train acc: 0.9645833373069763 test loss: 1.3430733680725098 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3944 train loss: 1.0785244703292847 train acc: 0.9645833373069763 test loss: 1.3467267751693726 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3945 train loss: 1.078539252281189 train acc: 0.9645833373069763 test loss: 1.3486791849136353 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3946 train loss: 1.0785126686096191 train acc: 0.9645833373069763 test loss: 1.3553963899612427 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3947 train loss: 1.078755497932434 train acc: 0.9645833373069763 test loss: 1.355029582977295 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3948 train loss: 1.07857084274292 train acc: 0.9645833373069763 test loss: 1.325103521347046 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3949 train loss: 1.078519344329834 train acc: 0.9645833373069763 test loss: 1.3446223735809326 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3950 train loss: 1.078453779220581 train acc: 0.9645833373069763 test loss: 1.3275179862976074 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3951 train loss: 1.0784735679626465 train acc: 0.9645833373069763 test loss: 1.3263468742370605 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3952 train loss: 1.0783838033676147 train acc: 0.9645833373069763 test loss: 1.33244788646698 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3953 train loss: 1.078525424003601 train acc: 0.9645833373069763 test loss: 1.3388442993164062 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3954 train loss: 1.0784651041030884 train acc: 0.9645833373069763 test loss: 1.3395322561264038 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3955 train loss: 1.0784863233566284 train acc: 0.9645833373069763 test loss: 1.3403277397155762 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3956 train loss: 1.078424096107483 train acc: 0.9645833373069763 test loss: 1.3558316230773926 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3957 train loss: 1.0785282850265503 train acc: 0.9645833373069763 test loss: 1.3408819437026978 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3958 train loss: 1.0785006284713745 train acc: 0.9645833373069763 test loss: 1.3298732042312622 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3959 train loss: 1.0786043405532837 train acc: 0.9645833373069763 test loss: 1.3446595668792725 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3960 train loss: 1.0785385370254517 train acc: 0.9645833373069763 test loss: 1.347933292388916 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3961 train loss: 1.0785832405090332 train acc: 0.9645833373069763 test loss: 1.3606666326522827 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3962 train loss: 1.078615665435791 train acc: 0.9645833373069763 test loss: 1.3551232814788818 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3963 train loss: 1.0783107280731201 train acc: 0.9645833373069763 test loss: 1.3590127229690552 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3964 train loss: 1.0785179138183594 train acc: 0.9645833373069763 test loss: 1.352038860321045 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3965 train loss: 1.078574538230896 train acc: 0.9645833373069763 test loss: 1.3396822214126587 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3966 train loss: 1.078552484512329 train acc: 0.9645833373069763 test loss: 1.3488590717315674 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3967 train loss: 1.0784679651260376 train acc: 0.9645833373069763 test loss: 1.3473525047302246 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3968 train loss: 1.0786012411117554 train acc: 0.9645833373069763 test loss: 1.3313106298446655 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3969 train loss: 1.07859468460083 train acc: 0.9645833373069763 test loss: 1.3546158075332642 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3970 train loss: 1.0784703493118286 train acc: 0.9645833373069763 test loss: 1.3338634967803955 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 3971 train loss: 1.0784032344818115 train acc: 0.9645833373069763 test loss: 1.3435606956481934 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3972 train loss: 1.0785285234451294 train acc: 0.9645833373069763 test loss: 1.3359934091567993 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3973 train loss: 1.078332543373108 train acc: 0.9645833373069763 test loss: 1.3459872007369995 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3974 train loss: 1.0784510374069214 train acc: 0.9645833373069763 test loss: 1.353347659111023 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3975 train loss: 1.078426718711853 train acc: 0.9645833373069763 test loss: 1.3238537311553955 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3976 train loss: 1.0785205364227295 train acc: 0.9645833373069763 test loss: 1.342344880104065 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3977 train loss: 1.0784786939620972 train acc: 0.9645833373069763 test loss: 1.355347752571106 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3978 train loss: 1.0784454345703125 train acc: 0.9645833373069763 test loss: 1.3395867347717285 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3979 train loss: 1.0784934759140015 train acc: 0.9645833373069763 test loss: 1.3319216966629028 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3980 train loss: 1.078491449356079 train acc: 0.9645833373069763 test loss: 1.3284449577331543 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3981 train loss: 1.078671932220459 train acc: 0.9645833373069763 test loss: 1.32789945602417 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3982 train loss: 1.0784732103347778 train acc: 0.9645833373069763 test loss: 1.3340200185775757 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3983 train loss: 1.0784584283828735 train acc: 0.9645833373069763 test loss: 1.3363767862319946 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3984 train loss: 1.0784873962402344 train acc: 0.9645833373069763 test loss: 1.360543131828308 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3985 train loss: 1.0785319805145264 train acc: 0.9645833373069763 test loss: 1.3572877645492554 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3986 train loss: 1.0784765481948853 train acc: 0.9645833373069763 test loss: 1.3381977081298828 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3987 train loss: 1.0785048007965088 train acc: 0.9645833373069763 test loss: 1.3481651544570923 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3988 train loss: 1.0783663988113403 train acc: 0.9645833373069763 test loss: 1.3623377084732056 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 3989 train loss: 1.0783485174179077 train acc: 0.9645833373069763 test loss: 1.3601689338684082 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3990 train loss: 1.0784603357315063 train acc: 0.9645833373069763 test loss: 1.3482578992843628 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3991 train loss: 1.078633427619934 train acc: 0.9645833373069763 test loss: 1.3550457954406738 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 3992 train loss: 1.0785362720489502 train acc: 0.9645833373069763 test loss: 1.3293795585632324 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3993 train loss: 1.0785444974899292 train acc: 0.9645833373069763 test loss: 1.3480820655822754 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 3994 train loss: 1.0784838199615479 train acc: 0.9645833373069763 test loss: 1.3450864553451538 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 3995 train loss: 1.0785335302352905 train acc: 0.9645833373069763 test loss: 1.3484028577804565 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 3996 train loss: 1.0785431861877441 train acc: 0.9645833373069763 test loss: 1.3409862518310547 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3997 train loss: 1.0784095525741577 train acc: 0.9645833373069763 test loss: 1.333685040473938 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 3998 train loss: 1.0784817934036255 train acc: 0.9645833373069763 test loss: 1.3228174448013306 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 3999 train loss: 1.0785396099090576 train acc: 0.9645833373069763 test loss: 1.3236480951309204 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4000 train loss: 1.0784687995910645 train acc: 0.9645833373069763 test loss: 1.3271785974502563 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4001 train loss: 1.0785101652145386 train acc: 0.9645833373069763 test loss: 1.3522332906723022 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4002 train loss: 1.0785309076309204 train acc: 0.9645833373069763 test loss: 1.3253051042556763 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4003 train loss: 1.0784080028533936 train acc: 0.9645833373069763 test loss: 1.3526591062545776 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4004 train loss: 1.0783993005752563 train acc: 0.9645833373069763 test loss: 1.3470091819763184 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4005 train loss: 1.0783140659332275 train acc: 0.9645833373069763 test loss: 1.3327518701553345 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4006 train loss: 1.0786339044570923 train acc: 0.9645833373069763 test loss: 1.359429121017456 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4007 train loss: 1.078568696975708 train acc: 0.9645833373069763 test loss: 1.335218071937561 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4008 train loss: 1.0784316062927246 train acc: 0.9645833373069763 test loss: 1.340264081954956 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4009 train loss: 1.0784367322921753 train acc: 0.9645833373069763 test loss: 1.3568071126937866 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4010 train loss: 1.0785738229751587 train acc: 0.9645833373069763 test loss: 1.3409271240234375 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4011 train loss: 1.0785832405090332 train acc: 0.9645833373069763 test loss: 1.3433829545974731 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4012 train loss: 1.0785174369812012 train acc: 0.9645833373069763 test loss: 1.332029938697815 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4013 train loss: 1.0785027742385864 train acc: 0.9645833373069763 test loss: 1.3535548448562622 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4014 train loss: 1.0784884691238403 train acc: 0.9645833373069763 test loss: 1.3394557237625122 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4015 train loss: 1.0785185098648071 train acc: 0.9645833373069763 test loss: 1.3463380336761475 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4016 train loss: 1.078491449356079 train acc: 0.9645833373069763 test loss: 1.3496289253234863 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4017 train loss: 1.0786480903625488 train acc: 0.9645833373069763 test loss: 1.3584131002426147 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4018 train loss: 1.078555703163147 train acc: 0.9645833373069763 test loss: 1.350557804107666 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4019 train loss: 1.0783919095993042 train acc: 0.9645833373069763 test loss: 1.3454737663269043 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4020 train loss: 1.078545093536377 train acc: 0.9645833373069763 test loss: 1.3464734554290771 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4021 train loss: 1.0785698890686035 train acc: 0.9645833373069763 test loss: 1.3379745483398438 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4022 train loss: 1.0783592462539673 train acc: 0.9645833373069763 test loss: 1.3426785469055176 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4023 train loss: 1.0786269903182983 train acc: 0.9645833373069763 test loss: 1.3419601917266846 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4024 train loss: 1.0785380601882935 train acc: 0.9645833373069763 test loss: 1.350738763809204 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4025 train loss: 1.078478455543518 train acc: 0.9645833373069763 test loss: 1.3533670902252197 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4026 train loss: 1.0785216093063354 train acc: 0.9645833373069763 test loss: 1.3329919576644897 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4027 train loss: 1.0784497261047363 train acc: 0.9645833373069763 test loss: 1.3535478115081787 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4028 train loss: 1.0784832239151 train acc: 0.9645833373069763 test loss: 1.3330739736557007 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4029 train loss: 1.07852041721344 train acc: 0.9645833373069763 test loss: 1.3456823825836182 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4030 train loss: 1.0784720182418823 train acc: 0.9645833373069763 test loss: 1.3435527086257935 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4031 train loss: 1.0784071683883667 train acc: 0.9645833373069763 test loss: 1.3481251001358032 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4032 train loss: 1.0784599781036377 train acc: 0.9645833373069763 test loss: 1.3389087915420532 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4033 train loss: 1.0783660411834717 train acc: 0.9645833373069763 test loss: 1.343425989151001 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4034 train loss: 1.0784919261932373 train acc: 0.9645833373069763 test loss: 1.3286652565002441 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4035 train loss: 1.078381896018982 train acc: 0.9645833373069763 test loss: 1.330098032951355 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4036 train loss: 1.0786168575286865 train acc: 0.9645833373069763 test loss: 1.3471629619598389 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4037 train loss: 1.0784753561019897 train acc: 0.9645833373069763 test loss: 1.3459279537200928 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4038 train loss: 1.0785237550735474 train acc: 0.9645833373069763 test loss: 1.3389180898666382 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4039 train loss: 1.078442931175232 train acc: 0.9645833373069763 test loss: 1.3328732252120972 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4040 train loss: 1.0786278247833252 train acc: 0.9645833373069763 test loss: 1.3330554962158203 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4041 train loss: 1.0784637928009033 train acc: 0.9645833373069763 test loss: 1.330914855003357 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4042 train loss: 1.0783405303955078 train acc: 0.9645833373069763 test loss: 1.352598786354065 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4043 train loss: 1.0784605741500854 train acc: 0.9645833373069763 test loss: 1.340742826461792 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4044 train loss: 1.0784505605697632 train acc: 0.9645833373069763 test loss: 1.3535149097442627 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4045 train loss: 1.0783687829971313 train acc: 0.9645833373069763 test loss: 1.3430862426757812 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4046 train loss: 1.078574776649475 train acc: 0.9645833373069763 test loss: 1.3501660823822021 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4047 train loss: 1.0784410238265991 train acc: 0.9645833373069763 test loss: 1.3398789167404175 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4048 train loss: 1.0786418914794922 train acc: 0.9645833373069763 test loss: 1.360950231552124 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4049 train loss: 1.0784934759140015 train acc: 0.9645833373069763 test loss: 1.3414230346679688 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4050 train loss: 1.0784525871276855 train acc: 0.9645833373069763 test loss: 1.3559582233428955 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4051 train loss: 1.0783722400665283 train acc: 0.9645833373069763 test loss: 1.3455557823181152 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4052 train loss: 1.0784790515899658 train acc: 0.9645833373069763 test loss: 1.338578462600708 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4053 train loss: 1.078434705734253 train acc: 0.9645833373069763 test loss: 1.3381434679031372 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4054 train loss: 1.0785295963287354 train acc: 0.9645833373069763 test loss: 1.3285964727401733 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4055 train loss: 1.0784324407577515 train acc: 0.9645833373069763 test loss: 1.3463783264160156 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4056 train loss: 1.0783882141113281 train acc: 0.9645833373069763 test loss: 1.3461896181106567 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4057 train loss: 1.0787014961242676 train acc: 0.9645833373069763 test loss: 1.329058289527893 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4058 train loss: 1.0785952806472778 train acc: 0.9645833373069763 test loss: 1.3494560718536377 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4059 train loss: 1.078579306602478 train acc: 0.9645833373069763 test loss: 1.3483116626739502 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4060 train loss: 1.07850980758667 train acc: 0.9645833373069763 test loss: 1.3308844566345215 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4061 train loss: 1.0785139799118042 train acc: 0.9645833373069763 test loss: 1.3423256874084473 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4062 train loss: 1.078395128250122 train acc: 0.9645833373069763 test loss: 1.3518927097320557 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4063 train loss: 1.0785634517669678 train acc: 0.9645833373069763 test loss: 1.3225775957107544 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4064 train loss: 1.0785094499588013 train acc: 0.9645833373069763 test loss: 1.3475204706192017 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4065 train loss: 1.07856023311615 train acc: 0.9645833373069763 test loss: 1.3270479440689087 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4066 train loss: 1.0784882307052612 train acc: 0.9645833373069763 test loss: 1.341949701309204 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4067 train loss: 1.0784461498260498 train acc: 0.9645833373069763 test loss: 1.350097417831421 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4068 train loss: 1.078522801399231 train acc: 0.9645833373069763 test loss: 1.3472799062728882 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4069 train loss: 1.0785870552062988 train acc: 0.9645833373069763 test loss: 1.355461835861206 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4070 train loss: 1.0786197185516357 train acc: 0.9645833373069763 test loss: 1.3382489681243896 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4071 train loss: 1.0785058736801147 train acc: 0.9645833373069763 test loss: 1.3392893075942993 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4072 train loss: 1.0785222053527832 train acc: 0.9645833373069763 test loss: 1.3618009090423584 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4073 train loss: 1.0784869194030762 train acc: 0.9645833373069763 test loss: 1.3386985063552856 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4074 train loss: 1.0786155462265015 train acc: 0.9645833373069763 test loss: 1.3353089094161987 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4075 train loss: 1.0783458948135376 train acc: 0.9645833373069763 test loss: 1.3353464603424072 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4076 train loss: 1.0785306692123413 train acc: 0.9645833373069763 test loss: 1.3386385440826416 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4077 train loss: 1.0784823894500732 train acc: 0.9645833373069763 test loss: 1.3444106578826904 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4078 train loss: 1.0783767700195312 train acc: 0.9645833373069763 test loss: 1.3439511060714722 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4079 train loss: 1.0785653591156006 train acc: 0.9645833373069763 test loss: 1.3400747776031494 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4080 train loss: 1.078362226486206 train acc: 0.9645833373069763 test loss: 1.3332674503326416 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4081 train loss: 1.078542709350586 train acc: 0.9645833373069763 test loss: 1.3482718467712402 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4082 train loss: 1.0784040689468384 train acc: 0.9645833373069763 test loss: 1.3470230102539062 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4083 train loss: 1.0784242153167725 train acc: 0.9645833373069763 test loss: 1.3506966829299927 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4084 train loss: 1.0784865617752075 train acc: 0.9645833373069763 test loss: 1.3391674757003784 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4085 train loss: 1.0784080028533936 train acc: 0.9645833373069763 test loss: 1.347590684890747 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4086 train loss: 1.0785719156265259 train acc: 0.9645833373069763 test loss: 1.33232843875885 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4087 train loss: 1.0784993171691895 train acc: 0.9645833373069763 test loss: 1.3338475227355957 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4088 train loss: 1.0784964561462402 train acc: 0.9645833373069763 test loss: 1.3263956308364868 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4089 train loss: 1.0785595178604126 train acc: 0.9645833373069763 test loss: 1.338449478149414 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4090 train loss: 1.0785672664642334 train acc: 0.9645833373069763 test loss: 1.3232848644256592 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4091 train loss: 1.0785529613494873 train acc: 0.9645833373069763 test loss: 1.3342868089675903 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4092 train loss: 1.0784566402435303 train acc: 0.9645833373069763 test loss: 1.324729084968567 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4093 train loss: 1.0785911083221436 train acc: 0.9645833373069763 test loss: 1.3331447839736938 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4094 train loss: 1.0786588191986084 train acc: 0.9645833373069763 test loss: 1.3265726566314697 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4095 train loss: 1.0784038305282593 train acc: 0.9645833373069763 test loss: 1.3342664241790771 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4096 train loss: 1.0783863067626953 train acc: 0.9645833373069763 test loss: 1.3401756286621094 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4097 train loss: 1.0784411430358887 train acc: 0.9645833373069763 test loss: 1.3415395021438599 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4098 train loss: 1.078359842300415 train acc: 0.9645833373069763 test loss: 1.3519304990768433 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4099 train loss: 1.0782928466796875 train acc: 0.9645833373069763 test loss: 1.3454617261886597 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4100 train loss: 1.0785397291183472 train acc: 0.9645833373069763 test loss: 1.3536800146102905 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4101 train loss: 1.0783970355987549 train acc: 0.9645833373069763 test loss: 1.3380115032196045 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4102 train loss: 1.0785877704620361 train acc: 0.9645833373069763 test loss: 1.3317898511886597 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4103 train loss: 1.078500509262085 train acc: 0.9645833373069763 test loss: 1.3229979276657104 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4104 train loss: 1.078611135482788 train acc: 0.9645833373069763 test loss: 1.345511555671692 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4105 train loss: 1.078626036643982 train acc: 0.9645833373069763 test loss: 1.3491284847259521 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4106 train loss: 1.07844078540802 train acc: 0.9645833373069763 test loss: 1.341797947883606 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4107 train loss: 1.078527569770813 train acc: 0.9645833373069763 test loss: 1.3460758924484253 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4108 train loss: 1.0783928632736206 train acc: 0.9645833373069763 test loss: 1.3437656164169312 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4109 train loss: 1.078368902206421 train acc: 0.9645833373069763 test loss: 1.3403162956237793 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4110 train loss: 1.0784205198287964 train acc: 0.9645833373069763 test loss: 1.3418320417404175 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4111 train loss: 1.078530192375183 train acc: 0.9645833373069763 test loss: 1.3438715934753418 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4112 train loss: 1.0783556699752808 train acc: 0.9645833373069763 test loss: 1.3565455675125122 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4113 train loss: 1.0786279439926147 train acc: 0.9645833373069763 test loss: 1.346405029296875 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4114 train loss: 1.0785796642303467 train acc: 0.9645833373069763 test loss: 1.3587208986282349 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4115 train loss: 1.078589916229248 train acc: 0.9645833373069763 test loss: 1.3450011014938354 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4116 train loss: 1.0785852670669556 train acc: 0.9645833373069763 test loss: 1.3387839794158936 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4117 train loss: 1.0784813165664673 train acc: 0.9645833373069763 test loss: 1.3629035949707031 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4118 train loss: 1.0785659551620483 train acc: 0.9645833373069763 test loss: 1.3345255851745605 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4119 train loss: 1.0785425901412964 train acc: 0.9645833373069763 test loss: 1.3416458368301392 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4120 train loss: 1.0785026550292969 train acc: 0.9645833373069763 test loss: 1.3370678424835205 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4121 train loss: 1.0783871412277222 train acc: 0.9645833373069763 test loss: 1.3377234935760498 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4122 train loss: 1.0784642696380615 train acc: 0.9645833373069763 test loss: 1.3527785539627075 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4123 train loss: 1.0784393548965454 train acc: 0.9645833373069763 test loss: 1.347631573677063 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4124 train loss: 1.0784629583358765 train acc: 0.9645833373069763 test loss: 1.3473846912384033 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4125 train loss: 1.0785565376281738 train acc: 0.9645833373069763 test loss: 1.3442600965499878 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4126 train loss: 1.0784846544265747 train acc: 0.9645833373069763 test loss: 1.3424862623214722 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4127 train loss: 1.0784962177276611 train acc: 0.9645833373069763 test loss: 1.328916311264038 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4128 train loss: 1.0784552097320557 train acc: 0.9645833373069763 test loss: 1.3441404104232788 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4129 train loss: 1.0783771276474 train acc: 0.9645833373069763 test loss: 1.3446170091629028 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4130 train loss: 1.0784881114959717 train acc: 0.9645833373069763 test loss: 1.346442699432373 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4131 train loss: 1.078521966934204 train acc: 0.9645833373069763 test loss: 1.3351906538009644 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4132 train loss: 1.0785725116729736 train acc: 0.9645833373069763 test loss: 1.357909083366394 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4133 train loss: 1.0785621404647827 train acc: 0.9645833373069763 test loss: 1.3331111669540405 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4134 train loss: 1.0785166025161743 train acc: 0.9645833373069763 test loss: 1.3282201290130615 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4135 train loss: 1.07853102684021 train acc: 0.9645833373069763 test loss: 1.341027855873108 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4136 train loss: 1.0784268379211426 train acc: 0.9645833373069763 test loss: 1.3271125555038452 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4137 train loss: 1.0784125328063965 train acc: 0.9645833373069763 test loss: 1.3414628505706787 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4138 train loss: 1.0786021947860718 train acc: 0.9645833373069763 test loss: 1.343044638633728 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4139 train loss: 1.0785502195358276 train acc: 0.9645833373069763 test loss: 1.3358714580535889 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4140 train loss: 1.078492522239685 train acc: 0.9645833373069763 test loss: 1.3672759532928467 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4141 train loss: 1.0784155130386353 train acc: 0.9645833373069763 test loss: 1.3356986045837402 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4142 train loss: 1.0784131288528442 train acc: 0.9645833373069763 test loss: 1.3521331548690796 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4143 train loss: 1.078454613685608 train acc: 0.9645833373069763 test loss: 1.3446012735366821 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4144 train loss: 1.0784282684326172 train acc: 0.9645833373069763 test loss: 1.3206175565719604 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4145 train loss: 1.0784554481506348 train acc: 0.9645833373069763 test loss: 1.3361142873764038 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4146 train loss: 1.0784224271774292 train acc: 0.9645833373069763 test loss: 1.3351445198059082 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4147 train loss: 1.0784801244735718 train acc: 0.9645833373069763 test loss: 1.3572973012924194 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4148 train loss: 1.0783002376556396 train acc: 0.9645833373069763 test loss: 1.342808485031128 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4149 train loss: 1.0785845518112183 train acc: 0.9645833373069763 test loss: 1.338498592376709 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4150 train loss: 1.0781702995300293 train acc: 0.9645833373069763 test loss: 1.3481844663619995 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4151 train loss: 1.0784698724746704 train acc: 0.9645833373069763 test loss: 1.336286187171936 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4152 train loss: 1.0780717134475708 train acc: 0.9645833373069763 test loss: 1.3362184762954712 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4153 train loss: 1.078015685081482 train acc: 0.9645833373069763 test loss: 1.3303519487380981 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4154 train loss: 1.0773955583572388 train acc: 0.9666666388511658 test loss: 1.3353155851364136 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4155 train loss: 1.0772098302841187 train acc: 0.9666666388511658 test loss: 1.353187084197998 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4156 train loss: 1.0767112970352173 train acc: 0.9666666388511658 test loss: 1.336474061012268 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4157 train loss: 1.0767154693603516 train acc: 0.9666666388511658 test loss: 1.3375743627548218 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4158 train loss: 1.0765488147735596 train acc: 0.9666666388511658 test loss: 1.3108237981796265 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 4159 train loss: 1.0770395994186401 train acc: 0.9666666388511658 test loss: 1.3348827362060547 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4160 train loss: 1.0766834020614624 train acc: 0.9666666388511658 test loss: 1.331222414970398 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4161 train loss: 1.076720952987671 train acc: 0.9666666388511658 test loss: 1.3391159772872925 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4162 train loss: 1.0768142938613892 train acc: 0.9666666388511658 test loss: 1.3401355743408203 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4163 train loss: 1.0771031379699707 train acc: 0.9666666388511658 test loss: 1.3440515995025635 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4164 train loss: 1.0766255855560303 train acc: 0.9666666388511658 test loss: 1.3416550159454346 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4165 train loss: 1.0765488147735596 train acc: 0.9666666388511658 test loss: 1.3626704216003418 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4166 train loss: 1.0766240358352661 train acc: 0.9666666388511658 test loss: 1.3510174751281738 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4167 train loss: 1.076708197593689 train acc: 0.9666666388511658 test loss: 1.3642948865890503 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4168 train loss: 1.076611042022705 train acc: 0.9666666388511658 test loss: 1.3608250617980957 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4169 train loss: 1.0766875743865967 train acc: 0.9666666388511658 test loss: 1.3635231256484985 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4170 train loss: 1.076654076576233 train acc: 0.9666666388511658 test loss: 1.3533438444137573 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4171 train loss: 1.0768752098083496 train acc: 0.9666666388511658 test loss: 1.3559458255767822 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4172 train loss: 1.076751470565796 train acc: 0.9666666388511658 test loss: 1.347349762916565 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4173 train loss: 1.076651692390442 train acc: 0.9666666388511658 test loss: 1.36156165599823 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4174 train loss: 1.0764672756195068 train acc: 0.9666666388511658 test loss: 1.3446711301803589 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4175 train loss: 1.0766347646713257 train acc: 0.9666666388511658 test loss: 1.343873143196106 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4176 train loss: 1.0764847993850708 train acc: 0.9666666388511658 test loss: 1.3399628400802612 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4177 train loss: 1.0765957832336426 train acc: 0.9666666388511658 test loss: 1.3558428287506104 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4178 train loss: 1.076783537864685 train acc: 0.9666666388511658 test loss: 1.3381785154342651 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4179 train loss: 1.0765159130096436 train acc: 0.9666666388511658 test loss: 1.3479111194610596 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4180 train loss: 1.0764439105987549 train acc: 0.9666666388511658 test loss: 1.3483004570007324 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4181 train loss: 1.076592206954956 train acc: 0.9666666388511658 test loss: 1.3299543857574463 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4182 train loss: 1.0766514539718628 train acc: 0.9666666388511658 test loss: 1.3194013833999634 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4183 train loss: 1.0765663385391235 train acc: 0.9666666388511658 test loss: 1.3403441905975342 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4184 train loss: 1.0765564441680908 train acc: 0.9666666388511658 test loss: 1.3302420377731323 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4185 train loss: 1.0766996145248413 train acc: 0.9666666388511658 test loss: 1.3353012800216675 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4186 train loss: 1.0766081809997559 train acc: 0.9666666388511658 test loss: 1.3342808485031128 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4187 train loss: 1.0763413906097412 train acc: 0.9666666388511658 test loss: 1.3491991758346558 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4188 train loss: 1.0765668153762817 train acc: 0.9666666388511658 test loss: 1.332411289215088 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4189 train loss: 1.0765376091003418 train acc: 0.9666666388511658 test loss: 1.3295539617538452 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4190 train loss: 1.0764936208724976 train acc: 0.9666666388511658 test loss: 1.3452028036117554 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4191 train loss: 1.0766962766647339 train acc: 0.9666666388511658 test loss: 1.3384016752243042 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4192 train loss: 1.0766899585723877 train acc: 0.9666666388511658 test loss: 1.3233222961425781 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4193 train loss: 1.0765860080718994 train acc: 0.9666666388511658 test loss: 1.341247797012329 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4194 train loss: 1.0763932466506958 train acc: 0.9666666388511658 test loss: 1.3307021856307983 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4195 train loss: 1.076539158821106 train acc: 0.9666666388511658 test loss: 1.3344995975494385 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4196 train loss: 1.0763640403747559 train acc: 0.9666666388511658 test loss: 1.3304171562194824 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4197 train loss: 1.076481580734253 train acc: 0.9666666388511658 test loss: 1.3169199228286743 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 4198 train loss: 1.0764073133468628 train acc: 0.9666666388511658 test loss: 1.3289998769760132 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4199 train loss: 1.0765565633773804 train acc: 0.9666666388511658 test loss: 1.3292628526687622 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4200 train loss: 1.076508641242981 train acc: 0.9666666388511658 test loss: 1.3058750629425049 best test loss: 1.283445954322815 test acc: 0.75\n",
      "Epoch 4201 train loss: 1.0763437747955322 train acc: 0.9666666388511658 test loss: 1.3225172758102417 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 4202 train loss: 1.076579213142395 train acc: 0.9666666388511658 test loss: 1.3377997875213623 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4203 train loss: 1.0766146183013916 train acc: 0.9666666388511658 test loss: 1.3206595182418823 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 4204 train loss: 1.076529860496521 train acc: 0.9666666388511658 test loss: 1.322008728981018 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4205 train loss: 1.0766193866729736 train acc: 0.9666666388511658 test loss: 1.3192100524902344 best test loss: 1.283445954322815 test acc: 0.7333333492279053\n",
      "Epoch 4206 train loss: 1.076442837715149 train acc: 0.9666666388511658 test loss: 1.3355450630187988 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4207 train loss: 1.0764836072921753 train acc: 0.9666666388511658 test loss: 1.3238682746887207 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4208 train loss: 1.0767461061477661 train acc: 0.9666666388511658 test loss: 1.3360333442687988 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4209 train loss: 1.0765255689620972 train acc: 0.9666666388511658 test loss: 1.345826506614685 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4210 train loss: 1.0763859748840332 train acc: 0.9666666388511658 test loss: 1.3470946550369263 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4211 train loss: 1.076593041419983 train acc: 0.9666666388511658 test loss: 1.3332566022872925 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4212 train loss: 1.0765715837478638 train acc: 0.9666666388511658 test loss: 1.334665298461914 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4213 train loss: 1.076425552368164 train acc: 0.9666666388511658 test loss: 1.3491170406341553 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4214 train loss: 1.0764174461364746 train acc: 0.9666666388511658 test loss: 1.340396523475647 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4215 train loss: 1.076433539390564 train acc: 0.9666666388511658 test loss: 1.336241602897644 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4216 train loss: 1.0763202905654907 train acc: 0.9666666388511658 test loss: 1.326635718345642 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4217 train loss: 1.0764877796173096 train acc: 0.9666666388511658 test loss: 1.3449121713638306 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4218 train loss: 1.0764387845993042 train acc: 0.9666666388511658 test loss: 1.3403815031051636 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4219 train loss: 1.0765190124511719 train acc: 0.9666666388511658 test loss: 1.3386732339859009 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4220 train loss: 1.076391339302063 train acc: 0.9666666388511658 test loss: 1.337816596031189 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4221 train loss: 1.076545000076294 train acc: 0.9666666388511658 test loss: 1.351686716079712 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4222 train loss: 1.076580286026001 train acc: 0.9666666388511658 test loss: 1.3228645324707031 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4223 train loss: 1.0763856172561646 train acc: 0.9666666388511658 test loss: 1.3611832857131958 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4224 train loss: 1.0765105485916138 train acc: 0.9666666388511658 test loss: 1.3484134674072266 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4225 train loss: 1.076583743095398 train acc: 0.9666666388511658 test loss: 1.3432801961898804 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4226 train loss: 1.076424241065979 train acc: 0.9666666388511658 test loss: 1.3160027265548706 best test loss: 1.283445954322815 test acc: 0.7416666746139526\n",
      "Epoch 4227 train loss: 1.076547622680664 train acc: 0.9666666388511658 test loss: 1.3344407081604004 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4228 train loss: 1.07652747631073 train acc: 0.9666666388511658 test loss: 1.3298554420471191 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4229 train loss: 1.0764591693878174 train acc: 0.9666666388511658 test loss: 1.345043420791626 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4230 train loss: 1.0764100551605225 train acc: 0.9666666388511658 test loss: 1.3231472969055176 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4231 train loss: 1.0764611959457397 train acc: 0.9666666388511658 test loss: 1.3440470695495605 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4232 train loss: 1.076647162437439 train acc: 0.9666666388511658 test loss: 1.3377926349639893 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4233 train loss: 1.0767008066177368 train acc: 0.9666666388511658 test loss: 1.329272747039795 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4234 train loss: 1.076485514640808 train acc: 0.9666666388511658 test loss: 1.3279831409454346 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4235 train loss: 1.076406478881836 train acc: 0.9666666388511658 test loss: 1.3347655534744263 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4236 train loss: 1.0764480829238892 train acc: 0.9666666388511658 test loss: 1.3313432931900024 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4237 train loss: 1.0764802694320679 train acc: 0.9666666388511658 test loss: 1.3581922054290771 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4238 train loss: 1.0765219926834106 train acc: 0.9666666388511658 test loss: 1.3512818813323975 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4239 train loss: 1.0765172243118286 train acc: 0.9666666388511658 test loss: 1.3507100343704224 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4240 train loss: 1.0765324831008911 train acc: 0.9666666388511658 test loss: 1.3378280401229858 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4241 train loss: 1.0766693353652954 train acc: 0.9666666388511658 test loss: 1.3540191650390625 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4242 train loss: 1.0766239166259766 train acc: 0.9666666388511658 test loss: 1.3334554433822632 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4243 train loss: 1.0765141248703003 train acc: 0.9666666388511658 test loss: 1.3363083600997925 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4244 train loss: 1.0765703916549683 train acc: 0.9666666388511658 test loss: 1.355204463005066 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4245 train loss: 1.0767652988433838 train acc: 0.9666666388511658 test loss: 1.3401097059249878 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4246 train loss: 1.076660394668579 train acc: 0.9666666388511658 test loss: 1.3546854257583618 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4247 train loss: 1.0764846801757812 train acc: 0.9666666388511658 test loss: 1.3429951667785645 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4248 train loss: 1.0765734910964966 train acc: 0.9666666388511658 test loss: 1.3423020839691162 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4249 train loss: 1.076424479484558 train acc: 0.9666666388511658 test loss: 1.3368732929229736 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4250 train loss: 1.0764378309249878 train acc: 0.9666666388511658 test loss: 1.3194528818130493 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4251 train loss: 1.076477289199829 train acc: 0.9666666388511658 test loss: 1.3499503135681152 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4252 train loss: 1.0764793157577515 train acc: 0.9666666388511658 test loss: 1.3457406759262085 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4253 train loss: 1.0764305591583252 train acc: 0.9666666388511658 test loss: 1.3467968702316284 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4254 train loss: 1.0766352415084839 train acc: 0.9666666388511658 test loss: 1.3260973691940308 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4255 train loss: 1.0764962434768677 train acc: 0.9666666388511658 test loss: 1.3275225162506104 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4256 train loss: 1.0766372680664062 train acc: 0.9666666388511658 test loss: 1.3441014289855957 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4257 train loss: 1.0765994787216187 train acc: 0.9666666388511658 test loss: 1.3320173025131226 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4258 train loss: 1.0765389204025269 train acc: 0.9666666388511658 test loss: 1.341661810874939 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4259 train loss: 1.076409101486206 train acc: 0.9666666388511658 test loss: 1.3519903421401978 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4260 train loss: 1.076723337173462 train acc: 0.9666666388511658 test loss: 1.3424150943756104 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4261 train loss: 1.076521396636963 train acc: 0.9666666388511658 test loss: 1.3351168632507324 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4262 train loss: 1.0765661001205444 train acc: 0.9666666388511658 test loss: 1.3383599519729614 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4263 train loss: 1.0764013528823853 train acc: 0.9666666388511658 test loss: 1.3485658168792725 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4264 train loss: 1.076400637626648 train acc: 0.9666666388511658 test loss: 1.3531315326690674 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4265 train loss: 1.0764344930648804 train acc: 0.9666666388511658 test loss: 1.3380093574523926 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4266 train loss: 1.0765432119369507 train acc: 0.9666666388511658 test loss: 1.3408538103103638 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4267 train loss: 1.076456904411316 train acc: 0.9666666388511658 test loss: 1.3411180973052979 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4268 train loss: 1.076390027999878 train acc: 0.9666666388511658 test loss: 1.3472074270248413 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4269 train loss: 1.0764964818954468 train acc: 0.9666666388511658 test loss: 1.3320631980895996 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4270 train loss: 1.0765187740325928 train acc: 0.9666666388511658 test loss: 1.3402369022369385 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4271 train loss: 1.0764919519424438 train acc: 0.9666666388511658 test loss: 1.3426415920257568 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4272 train loss: 1.0765900611877441 train acc: 0.9666666388511658 test loss: 1.3373494148254395 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4273 train loss: 1.0765599012374878 train acc: 0.9666666388511658 test loss: 1.3522528409957886 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4274 train loss: 1.0765053033828735 train acc: 0.9666666388511658 test loss: 1.3504774570465088 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4275 train loss: 1.0765089988708496 train acc: 0.9666666388511658 test loss: 1.3239617347717285 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4276 train loss: 1.076530933380127 train acc: 0.9666666388511658 test loss: 1.3413985967636108 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4277 train loss: 1.0764769315719604 train acc: 0.9666666388511658 test loss: 1.3578771352767944 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4278 train loss: 1.0764105319976807 train acc: 0.9666666388511658 test loss: 1.3561519384384155 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4279 train loss: 1.0764716863632202 train acc: 0.9666666388511658 test loss: 1.3479268550872803 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4280 train loss: 1.076361060142517 train acc: 0.9666666388511658 test loss: 1.3308732509613037 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4281 train loss: 1.0764175653457642 train acc: 0.9666666388511658 test loss: 1.319762110710144 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4282 train loss: 1.0763168334960938 train acc: 0.9666666388511658 test loss: 1.3430649042129517 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4283 train loss: 1.076408863067627 train acc: 0.9666666388511658 test loss: 1.3539516925811768 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4284 train loss: 1.0764516592025757 train acc: 0.9666666388511658 test loss: 1.346199631690979 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4285 train loss: 1.076614499092102 train acc: 0.9666666388511658 test loss: 1.3447076082229614 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4286 train loss: 1.0764483213424683 train acc: 0.9666666388511658 test loss: 1.3275291919708252 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4287 train loss: 1.0765552520751953 train acc: 0.9666666388511658 test loss: 1.3470360040664673 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4288 train loss: 1.076562762260437 train acc: 0.9666666388511658 test loss: 1.3412529230117798 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4289 train loss: 1.0764048099517822 train acc: 0.9666666388511658 test loss: 1.3428846597671509 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4290 train loss: 1.0764217376708984 train acc: 0.9666666388511658 test loss: 1.3406301736831665 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4291 train loss: 1.0766559839248657 train acc: 0.9666666388511658 test loss: 1.3456734418869019 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4292 train loss: 1.0763813257217407 train acc: 0.9666666388511658 test loss: 1.3409496545791626 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4293 train loss: 1.0765211582183838 train acc: 0.9666666388511658 test loss: 1.352107286453247 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4294 train loss: 1.076439380645752 train acc: 0.9666666388511658 test loss: 1.3508023023605347 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4295 train loss: 1.0765843391418457 train acc: 0.9666666388511658 test loss: 1.34425687789917 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4296 train loss: 1.076604962348938 train acc: 0.9666666388511658 test loss: 1.3492902517318726 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4297 train loss: 1.0763466358184814 train acc: 0.9666666388511658 test loss: 1.3481847047805786 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4298 train loss: 1.0765670537948608 train acc: 0.9666666388511658 test loss: 1.3481611013412476 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4299 train loss: 1.07643461227417 train acc: 0.9666666388511658 test loss: 1.3388476371765137 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4300 train loss: 1.0766055583953857 train acc: 0.9666666388511658 test loss: 1.3428618907928467 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4301 train loss: 1.0765868425369263 train acc: 0.9666666388511658 test loss: 1.3596923351287842 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4302 train loss: 1.0765862464904785 train acc: 0.9666666388511658 test loss: 1.3691381216049194 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4303 train loss: 1.0764905214309692 train acc: 0.9666666388511658 test loss: 1.345649003982544 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4304 train loss: 1.0765955448150635 train acc: 0.9666666388511658 test loss: 1.33884859085083 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4305 train loss: 1.0765807628631592 train acc: 0.9666666388511658 test loss: 1.359829306602478 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4306 train loss: 1.0763906240463257 train acc: 0.9666666388511658 test loss: 1.351675033569336 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4307 train loss: 1.0765241384506226 train acc: 0.9666666388511658 test loss: 1.3432979583740234 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4308 train loss: 1.0764269828796387 train acc: 0.9666666388511658 test loss: 1.3372786045074463 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4309 train loss: 1.0765578746795654 train acc: 0.9666666388511658 test loss: 1.3630419969558716 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4310 train loss: 1.0766046047210693 train acc: 0.9666666388511658 test loss: 1.335520625114441 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4311 train loss: 1.0765492916107178 train acc: 0.9666666388511658 test loss: 1.3353780508041382 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4312 train loss: 1.0767143964767456 train acc: 0.9666666388511658 test loss: 1.340094804763794 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4313 train loss: 1.0764261484146118 train acc: 0.9666666388511658 test loss: 1.3522871732711792 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4314 train loss: 1.0765656232833862 train acc: 0.9666666388511658 test loss: 1.350554347038269 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4315 train loss: 1.0764564275741577 train acc: 0.9666666388511658 test loss: 1.3411028385162354 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4316 train loss: 1.0764051675796509 train acc: 0.9666666388511658 test loss: 1.3248809576034546 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4317 train loss: 1.0765016078948975 train acc: 0.9666666388511658 test loss: 1.3525383472442627 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4318 train loss: 1.076473593711853 train acc: 0.9666666388511658 test loss: 1.3526229858398438 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4319 train loss: 1.0765100717544556 train acc: 0.9666666388511658 test loss: 1.3358274698257446 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4320 train loss: 1.0765563249588013 train acc: 0.9666666388511658 test loss: 1.3497062921524048 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4321 train loss: 1.0764936208724976 train acc: 0.9666666388511658 test loss: 1.3411084413528442 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4322 train loss: 1.0766152143478394 train acc: 0.9666666388511658 test loss: 1.3372944593429565 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4323 train loss: 1.0763980150222778 train acc: 0.9666666388511658 test loss: 1.3303765058517456 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4324 train loss: 1.0764660835266113 train acc: 0.9666666388511658 test loss: 1.343702793121338 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4325 train loss: 1.0765862464904785 train acc: 0.9666666388511658 test loss: 1.3416355848312378 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4326 train loss: 1.0766618251800537 train acc: 0.9666666388511658 test loss: 1.338076114654541 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4327 train loss: 1.0766332149505615 train acc: 0.9666666388511658 test loss: 1.353379726409912 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4328 train loss: 1.0766372680664062 train acc: 0.9666666388511658 test loss: 1.343454360961914 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4329 train loss: 1.0765191316604614 train acc: 0.9666666388511658 test loss: 1.3398741483688354 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4330 train loss: 1.076457142829895 train acc: 0.9666666388511658 test loss: 1.3453710079193115 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4331 train loss: 1.0764241218566895 train acc: 0.9666666388511658 test loss: 1.3423535823822021 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4332 train loss: 1.0765169858932495 train acc: 0.9666666388511658 test loss: 1.3588224649429321 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4333 train loss: 1.0766303539276123 train acc: 0.9666666388511658 test loss: 1.3362177610397339 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4334 train loss: 1.0763435363769531 train acc: 0.9666666388511658 test loss: 1.3488421440124512 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4335 train loss: 1.0764930248260498 train acc: 0.9666666388511658 test loss: 1.3467552661895752 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4336 train loss: 1.0765864849090576 train acc: 0.9666666388511658 test loss: 1.3522535562515259 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4337 train loss: 1.0764926671981812 train acc: 0.9666666388511658 test loss: 1.3544026613235474 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4338 train loss: 1.0766059160232544 train acc: 0.9666666388511658 test loss: 1.3491734266281128 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4339 train loss: 1.076495885848999 train acc: 0.9666666388511658 test loss: 1.3634029626846313 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4340 train loss: 1.0764371156692505 train acc: 0.9666666388511658 test loss: 1.3426260948181152 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4341 train loss: 1.076635718345642 train acc: 0.9666666388511658 test loss: 1.347140908241272 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4342 train loss: 1.0765200853347778 train acc: 0.9666666388511658 test loss: 1.354974627494812 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4343 train loss: 1.0765739679336548 train acc: 0.9666666388511658 test loss: 1.342459797859192 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4344 train loss: 1.0765043497085571 train acc: 0.9666666388511658 test loss: 1.3528838157653809 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4345 train loss: 1.0763356685638428 train acc: 0.9666666388511658 test loss: 1.342317819595337 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4346 train loss: 1.0765573978424072 train acc: 0.9666666388511658 test loss: 1.3441393375396729 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4347 train loss: 1.076545238494873 train acc: 0.9666666388511658 test loss: 1.336398720741272 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4348 train loss: 1.076568841934204 train acc: 0.9666666388511658 test loss: 1.3538587093353271 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4349 train loss: 1.076526403427124 train acc: 0.9666666388511658 test loss: 1.3106242418289185 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4350 train loss: 1.07649564743042 train acc: 0.9666666388511658 test loss: 1.3302960395812988 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4351 train loss: 1.0764433145523071 train acc: 0.9666666388511658 test loss: 1.3435416221618652 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4352 train loss: 1.0765843391418457 train acc: 0.9666666388511658 test loss: 1.3308327198028564 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4353 train loss: 1.0765860080718994 train acc: 0.9666666388511658 test loss: 1.3540339469909668 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4354 train loss: 1.0765631198883057 train acc: 0.9666666388511658 test loss: 1.3595401048660278 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4355 train loss: 1.0766152143478394 train acc: 0.9666666388511658 test loss: 1.3426274061203003 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4356 train loss: 1.0765184164047241 train acc: 0.9666666388511658 test loss: 1.3415915966033936 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4357 train loss: 1.0764259099960327 train acc: 0.9666666388511658 test loss: 1.3433610200881958 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4358 train loss: 1.0764800310134888 train acc: 0.9666666388511658 test loss: 1.3510574102401733 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4359 train loss: 1.0765018463134766 train acc: 0.9666666388511658 test loss: 1.353497862815857 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4360 train loss: 1.0766323804855347 train acc: 0.9666666388511658 test loss: 1.3436071872711182 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4361 train loss: 1.07638680934906 train acc: 0.9666666388511658 test loss: 1.3538707494735718 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4362 train loss: 1.076561450958252 train acc: 0.9666666388511658 test loss: 1.3223379850387573 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4363 train loss: 1.0765423774719238 train acc: 0.9666666388511658 test loss: 1.3505268096923828 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4364 train loss: 1.076482892036438 train acc: 0.9666666388511658 test loss: 1.344234585762024 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4365 train loss: 1.0764955282211304 train acc: 0.9666666388511658 test loss: 1.3589675426483154 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4366 train loss: 1.0766410827636719 train acc: 0.9666666388511658 test loss: 1.3501986265182495 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4367 train loss: 1.0766119956970215 train acc: 0.9666666388511658 test loss: 1.3394925594329834 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4368 train loss: 1.0765904188156128 train acc: 0.9666666388511658 test loss: 1.3484934568405151 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4369 train loss: 1.0765495300292969 train acc: 0.9666666388511658 test loss: 1.3509241342544556 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4370 train loss: 1.0766570568084717 train acc: 0.9666666388511658 test loss: 1.3417764902114868 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4371 train loss: 1.0766385793685913 train acc: 0.9666666388511658 test loss: 1.333747148513794 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4372 train loss: 1.076708197593689 train acc: 0.9666666388511658 test loss: 1.3417588472366333 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4373 train loss: 1.076515793800354 train acc: 0.9666666388511658 test loss: 1.3572572469711304 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4374 train loss: 1.0764871835708618 train acc: 0.9666666388511658 test loss: 1.3486586809158325 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4375 train loss: 1.0765068531036377 train acc: 0.9666666388511658 test loss: 1.3267749547958374 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4376 train loss: 1.076380968093872 train acc: 0.9666666388511658 test loss: 1.3343533277511597 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4377 train loss: 1.0765835046768188 train acc: 0.9666666388511658 test loss: 1.3296024799346924 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4378 train loss: 1.076614499092102 train acc: 0.9666666388511658 test loss: 1.3563529253005981 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4379 train loss: 1.076573133468628 train acc: 0.9666666388511658 test loss: 1.337227463722229 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4380 train loss: 1.076597809791565 train acc: 0.9666666388511658 test loss: 1.338263750076294 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4381 train loss: 1.0764790773391724 train acc: 0.9666666388511658 test loss: 1.3592455387115479 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4382 train loss: 1.076493740081787 train acc: 0.9666666388511658 test loss: 1.3278279304504395 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4383 train loss: 1.0764272212982178 train acc: 0.9666666388511658 test loss: 1.351058840751648 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4384 train loss: 1.076452612876892 train acc: 0.9666666388511658 test loss: 1.3473334312438965 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4385 train loss: 1.0763638019561768 train acc: 0.9666666388511658 test loss: 1.3695446252822876 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4386 train loss: 1.0763791799545288 train acc: 0.9666666388511658 test loss: 1.3405028581619263 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4387 train loss: 1.0766795873641968 train acc: 0.9666666388511658 test loss: 1.3526746034622192 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4388 train loss: 1.0764614343643188 train acc: 0.9666666388511658 test loss: 1.354305386543274 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4389 train loss: 1.0765944719314575 train acc: 0.9666666388511658 test loss: 1.355916142463684 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4390 train loss: 1.0765578746795654 train acc: 0.9666666388511658 test loss: 1.3472501039505005 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4391 train loss: 1.0765596628189087 train acc: 0.9666666388511658 test loss: 1.32734215259552 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4392 train loss: 1.0764013528823853 train acc: 0.9666666388511658 test loss: 1.3468546867370605 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4393 train loss: 1.076517939567566 train acc: 0.9666666388511658 test loss: 1.3484644889831543 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4394 train loss: 1.0765490531921387 train acc: 0.9666666388511658 test loss: 1.3459289073944092 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4395 train loss: 1.0764847993850708 train acc: 0.9666666388511658 test loss: 1.3411853313446045 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4396 train loss: 1.076364278793335 train acc: 0.9666666388511658 test loss: 1.344469666481018 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4397 train loss: 1.0763452053070068 train acc: 0.9666666388511658 test loss: 1.3614493608474731 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4398 train loss: 1.0764422416687012 train acc: 0.9666666388511658 test loss: 1.3536709547042847 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4399 train loss: 1.0765697956085205 train acc: 0.9666666388511658 test loss: 1.3607581853866577 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4400 train loss: 1.07634437084198 train acc: 0.9666666388511658 test loss: 1.3443539142608643 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4401 train loss: 1.0765100717544556 train acc: 0.9666666388511658 test loss: 1.3586699962615967 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4402 train loss: 1.076513648033142 train acc: 0.9666666388511658 test loss: 1.351426362991333 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4403 train loss: 1.0765615701675415 train acc: 0.9666666388511658 test loss: 1.3587684631347656 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4404 train loss: 1.0764076709747314 train acc: 0.9666666388511658 test loss: 1.3266093730926514 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4405 train loss: 1.0764470100402832 train acc: 0.9666666388511658 test loss: 1.330829381942749 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4406 train loss: 1.0764857530593872 train acc: 0.9666666388511658 test loss: 1.331644058227539 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4407 train loss: 1.076711893081665 train acc: 0.9666666388511658 test loss: 1.3476893901824951 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4408 train loss: 1.0764472484588623 train acc: 0.9666666388511658 test loss: 1.333241581916809 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4409 train loss: 1.076554775238037 train acc: 0.9666666388511658 test loss: 1.3642295598983765 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4410 train loss: 1.0766080617904663 train acc: 0.9666666388511658 test loss: 1.3358933925628662 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4411 train loss: 1.0766675472259521 train acc: 0.9666666388511658 test loss: 1.3508129119873047 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4412 train loss: 1.076386570930481 train acc: 0.9666666388511658 test loss: 1.3433640003204346 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4413 train loss: 1.0765067338943481 train acc: 0.9666666388511658 test loss: 1.3509786128997803 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4414 train loss: 1.0764620304107666 train acc: 0.9666666388511658 test loss: 1.3537468910217285 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4415 train loss: 1.076454997062683 train acc: 0.9666666388511658 test loss: 1.354986548423767 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4416 train loss: 1.0765948295593262 train acc: 0.9666666388511658 test loss: 1.361689805984497 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4417 train loss: 1.076574683189392 train acc: 0.9666666388511658 test loss: 1.3519017696380615 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4418 train loss: 1.0764182806015015 train acc: 0.9666666388511658 test loss: 1.3663424253463745 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4419 train loss: 1.0766206979751587 train acc: 0.9666666388511658 test loss: 1.3364832401275635 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4420 train loss: 1.0763511657714844 train acc: 0.9666666388511658 test loss: 1.3391526937484741 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4421 train loss: 1.0764540433883667 train acc: 0.9666666388511658 test loss: 1.3384649753570557 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4422 train loss: 1.0765725374221802 train acc: 0.9666666388511658 test loss: 1.351071834564209 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4423 train loss: 1.0765968561172485 train acc: 0.9666666388511658 test loss: 1.3520060777664185 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4424 train loss: 1.0764880180358887 train acc: 0.9666666388511658 test loss: 1.3520334959030151 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4425 train loss: 1.0765748023986816 train acc: 0.9666666388511658 test loss: 1.3295178413391113 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4426 train loss: 1.0765461921691895 train acc: 0.9666666388511658 test loss: 1.3375722169876099 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4427 train loss: 1.0764600038528442 train acc: 0.9666666388511658 test loss: 1.3654723167419434 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4428 train loss: 1.076598048210144 train acc: 0.9666666388511658 test loss: 1.3488131761550903 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4429 train loss: 1.0764353275299072 train acc: 0.9666666388511658 test loss: 1.3491164445877075 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4430 train loss: 1.0764119625091553 train acc: 0.9666666388511658 test loss: 1.3560000658035278 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4431 train loss: 1.0764142274856567 train acc: 0.9666666388511658 test loss: 1.335864543914795 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4432 train loss: 1.0765650272369385 train acc: 0.9666666388511658 test loss: 1.3423106670379639 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4433 train loss: 1.0763822793960571 train acc: 0.9666666388511658 test loss: 1.358181118965149 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4434 train loss: 1.076419711112976 train acc: 0.9666666388511658 test loss: 1.3455429077148438 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4435 train loss: 1.0763901472091675 train acc: 0.9666666388511658 test loss: 1.3390978574752808 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4436 train loss: 1.076506495475769 train acc: 0.9666666388511658 test loss: 1.3633522987365723 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4437 train loss: 1.0763967037200928 train acc: 0.9666666388511658 test loss: 1.3463542461395264 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4438 train loss: 1.0765408277511597 train acc: 0.9666666388511658 test loss: 1.357408881187439 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4439 train loss: 1.0765825510025024 train acc: 0.9666666388511658 test loss: 1.3494704961776733 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4440 train loss: 1.0765118598937988 train acc: 0.9666666388511658 test loss: 1.3498857021331787 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4441 train loss: 1.0764272212982178 train acc: 0.9666666388511658 test loss: 1.3563745021820068 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4442 train loss: 1.076459288597107 train acc: 0.9666666388511658 test loss: 1.3478299379348755 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4443 train loss: 1.0765795707702637 train acc: 0.9666666388511658 test loss: 1.3492183685302734 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4444 train loss: 1.076477289199829 train acc: 0.9666666388511658 test loss: 1.348649263381958 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4445 train loss: 1.0765092372894287 train acc: 0.9666666388511658 test loss: 1.363221526145935 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4446 train loss: 1.0763715505599976 train acc: 0.9666666388511658 test loss: 1.3578904867172241 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4447 train loss: 1.076372742652893 train acc: 0.9666666388511658 test loss: 1.332297682762146 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4448 train loss: 1.0765544176101685 train acc: 0.9666666388511658 test loss: 1.342992901802063 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4449 train loss: 1.0764083862304688 train acc: 0.9666666388511658 test loss: 1.3374696969985962 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4450 train loss: 1.0765140056610107 train acc: 0.9666666388511658 test loss: 1.3610236644744873 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4451 train loss: 1.076517105102539 train acc: 0.9666666388511658 test loss: 1.3529928922653198 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4452 train loss: 1.0764285326004028 train acc: 0.9666666388511658 test loss: 1.3559157848358154 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4453 train loss: 1.07655668258667 train acc: 0.9666666388511658 test loss: 1.3503831624984741 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4454 train loss: 1.0764949321746826 train acc: 0.9666666388511658 test loss: 1.34916090965271 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4455 train loss: 1.0765496492385864 train acc: 0.9666666388511658 test loss: 1.3390575647354126 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4456 train loss: 1.076348900794983 train acc: 0.9666666388511658 test loss: 1.3362783193588257 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4457 train loss: 1.0765464305877686 train acc: 0.9666666388511658 test loss: 1.337504506111145 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4458 train loss: 1.07643461227417 train acc: 0.9666666388511658 test loss: 1.3542073965072632 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4459 train loss: 1.0764400959014893 train acc: 0.9666666388511658 test loss: 1.3467166423797607 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4460 train loss: 1.07666015625 train acc: 0.9666666388511658 test loss: 1.3625129461288452 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4461 train loss: 1.0764731168746948 train acc: 0.9666666388511658 test loss: 1.3591275215148926 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4462 train loss: 1.0763928890228271 train acc: 0.9666666388511658 test loss: 1.3344975709915161 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4463 train loss: 1.0765838623046875 train acc: 0.9666666388511658 test loss: 1.344299077987671 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4464 train loss: 1.0767146348953247 train acc: 0.9666666388511658 test loss: 1.337649941444397 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4465 train loss: 1.076555848121643 train acc: 0.9666666388511658 test loss: 1.3558400869369507 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4466 train loss: 1.0765079259872437 train acc: 0.9666666388511658 test loss: 1.3537325859069824 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4467 train loss: 1.076568365097046 train acc: 0.9666666388511658 test loss: 1.3499335050582886 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4468 train loss: 1.076480746269226 train acc: 0.9666666388511658 test loss: 1.3507380485534668 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4469 train loss: 1.0765198469161987 train acc: 0.9666666388511658 test loss: 1.355261206626892 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4470 train loss: 1.0763617753982544 train acc: 0.9666666388511658 test loss: 1.3390589952468872 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4471 train loss: 1.0763362646102905 train acc: 0.9666666388511658 test loss: 1.356124758720398 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4472 train loss: 1.0764620304107666 train acc: 0.9666666388511658 test loss: 1.3481589555740356 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4473 train loss: 1.0763942003250122 train acc: 0.9666666388511658 test loss: 1.3444503545761108 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4474 train loss: 1.076515793800354 train acc: 0.9666666388511658 test loss: 1.3476475477218628 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4475 train loss: 1.07656729221344 train acc: 0.9666666388511658 test loss: 1.34367835521698 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4476 train loss: 1.0765671730041504 train acc: 0.9666666388511658 test loss: 1.360144853591919 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4477 train loss: 1.0764565467834473 train acc: 0.9666666388511658 test loss: 1.3516381978988647 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4478 train loss: 1.0764752626419067 train acc: 0.9666666388511658 test loss: 1.3773972988128662 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4479 train loss: 1.0765831470489502 train acc: 0.9666666388511658 test loss: 1.3533893823623657 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4480 train loss: 1.0763580799102783 train acc: 0.9666666388511658 test loss: 1.3692878484725952 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4481 train loss: 1.0766147375106812 train acc: 0.9666666388511658 test loss: 1.3672021627426147 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4482 train loss: 1.0764714479446411 train acc: 0.9666666388511658 test loss: 1.363295316696167 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4483 train loss: 1.0764367580413818 train acc: 0.9666666388511658 test loss: 1.3483920097351074 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4484 train loss: 1.0764577388763428 train acc: 0.9666666388511658 test loss: 1.3431981801986694 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4485 train loss: 1.0765190124511719 train acc: 0.9666666388511658 test loss: 1.371812343597412 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4486 train loss: 1.0763863325119019 train acc: 0.9666666388511658 test loss: 1.3495844602584839 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4487 train loss: 1.0764638185501099 train acc: 0.9666666388511658 test loss: 1.3687940835952759 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4488 train loss: 1.0764983892440796 train acc: 0.9666666388511658 test loss: 1.3448189496994019 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4489 train loss: 1.0765666961669922 train acc: 0.9666666388511658 test loss: 1.3440731763839722 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4490 train loss: 1.0764256715774536 train acc: 0.9666666388511658 test loss: 1.342864751815796 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4491 train loss: 1.0764944553375244 train acc: 0.9666666388511658 test loss: 1.3551937341690063 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4492 train loss: 1.076454758644104 train acc: 0.9666666388511658 test loss: 1.3476115465164185 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4493 train loss: 1.0764375925064087 train acc: 0.9666666388511658 test loss: 1.3618425130844116 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4494 train loss: 1.0764676332473755 train acc: 0.9666666388511658 test loss: 1.363765001296997 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4495 train loss: 1.0764237642288208 train acc: 0.9666666388511658 test loss: 1.356185793876648 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4496 train loss: 1.0764979124069214 train acc: 0.9666666388511658 test loss: 1.3449820280075073 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4497 train loss: 1.076589822769165 train acc: 0.9666666388511658 test loss: 1.3445926904678345 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4498 train loss: 1.0763787031173706 train acc: 0.9666666388511658 test loss: 1.352481722831726 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4499 train loss: 1.0763804912567139 train acc: 0.9666666388511658 test loss: 1.3482367992401123 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4500 train loss: 1.0766242742538452 train acc: 0.9666666388511658 test loss: 1.3485299348831177 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4501 train loss: 1.0766276121139526 train acc: 0.9666666388511658 test loss: 1.344577431678772 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4502 train loss: 1.07646644115448 train acc: 0.9666666388511658 test loss: 1.363767147064209 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4503 train loss: 1.0764539241790771 train acc: 0.9666666388511658 test loss: 1.3444041013717651 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4504 train loss: 1.076452374458313 train acc: 0.9666666388511658 test loss: 1.346065878868103 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4505 train loss: 1.0764895677566528 train acc: 0.9666666388511658 test loss: 1.3547195196151733 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4506 train loss: 1.0764108896255493 train acc: 0.9666666388511658 test loss: 1.351296305656433 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4507 train loss: 1.076499342918396 train acc: 0.9666666388511658 test loss: 1.3512725830078125 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4508 train loss: 1.0764950513839722 train acc: 0.9666666388511658 test loss: 1.3481324911117554 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4509 train loss: 1.0765776634216309 train acc: 0.9666666388511658 test loss: 1.3345848321914673 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4510 train loss: 1.0764079093933105 train acc: 0.9666666388511658 test loss: 1.3345463275909424 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4511 train loss: 1.076548457145691 train acc: 0.9666666388511658 test loss: 1.3432377576828003 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4512 train loss: 1.0763920545578003 train acc: 0.9666666388511658 test loss: 1.336272954940796 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4513 train loss: 1.076499342918396 train acc: 0.9666666388511658 test loss: 1.3372482061386108 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4514 train loss: 1.0764281749725342 train acc: 0.9666666388511658 test loss: 1.3573750257492065 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4515 train loss: 1.0764942169189453 train acc: 0.9666666388511658 test loss: 1.3487204313278198 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4516 train loss: 1.0765949487686157 train acc: 0.9666666388511658 test loss: 1.366249680519104 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4517 train loss: 1.0764073133468628 train acc: 0.9666666388511658 test loss: 1.3469116687774658 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4518 train loss: 1.076414704322815 train acc: 0.9666666388511658 test loss: 1.3539096117019653 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4519 train loss: 1.0762799978256226 train acc: 0.9666666388511658 test loss: 1.3585855960845947 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4520 train loss: 1.0764178037643433 train acc: 0.9666666388511658 test loss: 1.3616902828216553 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4521 train loss: 1.0765726566314697 train acc: 0.9666666388511658 test loss: 1.3543025255203247 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4522 train loss: 1.0765159130096436 train acc: 0.9666666388511658 test loss: 1.3503928184509277 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4523 train loss: 1.076433777809143 train acc: 0.9666666388511658 test loss: 1.3594499826431274 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4524 train loss: 1.0763543844223022 train acc: 0.9666666388511658 test loss: 1.33971107006073 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4525 train loss: 1.076460361480713 train acc: 0.9666666388511658 test loss: 1.3542338609695435 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4526 train loss: 1.0764124393463135 train acc: 0.9666666388511658 test loss: 1.3462544679641724 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4527 train loss: 1.0765130519866943 train acc: 0.9666666388511658 test loss: 1.3661166429519653 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4528 train loss: 1.0766369104385376 train acc: 0.9666666388511658 test loss: 1.3579093217849731 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4529 train loss: 1.0765113830566406 train acc: 0.9666666388511658 test loss: 1.3556301593780518 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4530 train loss: 1.0764819383621216 train acc: 0.9666666388511658 test loss: 1.3478196859359741 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4531 train loss: 1.0764762163162231 train acc: 0.9666666388511658 test loss: 1.3434122800827026 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4532 train loss: 1.0765023231506348 train acc: 0.9666666388511658 test loss: 1.3567787408828735 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4533 train loss: 1.0765002965927124 train acc: 0.9666666388511658 test loss: 1.362210988998413 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4534 train loss: 1.076330542564392 train acc: 0.9666666388511658 test loss: 1.3290150165557861 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4535 train loss: 1.0765206813812256 train acc: 0.9666666388511658 test loss: 1.3316842317581177 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4536 train loss: 1.0764187574386597 train acc: 0.9666666388511658 test loss: 1.3425084352493286 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4537 train loss: 1.076500415802002 train acc: 0.9666666388511658 test loss: 1.3497265577316284 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4538 train loss: 1.0765451192855835 train acc: 0.9666666388511658 test loss: 1.3478679656982422 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4539 train loss: 1.0764786005020142 train acc: 0.9666666388511658 test loss: 1.351945400238037 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4540 train loss: 1.0765202045440674 train acc: 0.9666666388511658 test loss: 1.349942684173584 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4541 train loss: 1.0763986110687256 train acc: 0.9666666388511658 test loss: 1.3478597402572632 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4542 train loss: 1.0765060186386108 train acc: 0.9666666388511658 test loss: 1.3434232473373413 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4543 train loss: 1.0765141248703003 train acc: 0.9666666388511658 test loss: 1.3615461587905884 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4544 train loss: 1.0764427185058594 train acc: 0.9666666388511658 test loss: 1.3408336639404297 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4545 train loss: 1.0765737295150757 train acc: 0.9666666388511658 test loss: 1.3524997234344482 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4546 train loss: 1.0764657258987427 train acc: 0.9666666388511658 test loss: 1.3491209745407104 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4547 train loss: 1.0765392780303955 train acc: 0.9666666388511658 test loss: 1.354473352432251 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4548 train loss: 1.0765708684921265 train acc: 0.9666666388511658 test loss: 1.3709666728973389 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4549 train loss: 1.0763700008392334 train acc: 0.9666666388511658 test loss: 1.366066575050354 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4550 train loss: 1.0764306783676147 train acc: 0.9666666388511658 test loss: 1.3417149782180786 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4551 train loss: 1.0764033794403076 train acc: 0.9666666388511658 test loss: 1.35194730758667 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4552 train loss: 1.0763633251190186 train acc: 0.9666666388511658 test loss: 1.3463255167007446 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4553 train loss: 1.0764727592468262 train acc: 0.9666666388511658 test loss: 1.3686702251434326 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4554 train loss: 1.076358675956726 train acc: 0.9666666388511658 test loss: 1.3434066772460938 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4555 train loss: 1.0764654874801636 train acc: 0.9666666388511658 test loss: 1.3577624559402466 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4556 train loss: 1.076558232307434 train acc: 0.9666666388511658 test loss: 1.3541903495788574 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4557 train loss: 1.0764696598052979 train acc: 0.9666666388511658 test loss: 1.364911437034607 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4558 train loss: 1.076377511024475 train acc: 0.9666666388511658 test loss: 1.3638570308685303 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4559 train loss: 1.076427936553955 train acc: 0.9666666388511658 test loss: 1.3565341234207153 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4560 train loss: 1.0765666961669922 train acc: 0.9666666388511658 test loss: 1.3535430431365967 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4561 train loss: 1.0764206647872925 train acc: 0.9666666388511658 test loss: 1.3600566387176514 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4562 train loss: 1.0765079259872437 train acc: 0.9666666388511658 test loss: 1.3557826280593872 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4563 train loss: 1.0767689943313599 train acc: 0.9666666388511658 test loss: 1.3391162157058716 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4564 train loss: 1.076440691947937 train acc: 0.9666666388511658 test loss: 1.340286135673523 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4565 train loss: 1.076511025428772 train acc: 0.9666666388511658 test loss: 1.354331612586975 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4566 train loss: 1.0763667821884155 train acc: 0.9666666388511658 test loss: 1.3573551177978516 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4567 train loss: 1.0765255689620972 train acc: 0.9666666388511658 test loss: 1.3460853099822998 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4568 train loss: 1.0766254663467407 train acc: 0.9666666388511658 test loss: 1.3665906190872192 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4569 train loss: 1.0764729976654053 train acc: 0.9666666388511658 test loss: 1.3588573932647705 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4570 train loss: 1.0765175819396973 train acc: 0.9666666388511658 test loss: 1.351021409034729 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4571 train loss: 1.0763447284698486 train acc: 0.9666666388511658 test loss: 1.3455026149749756 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4572 train loss: 1.076438069343567 train acc: 0.9666666388511658 test loss: 1.3461416959762573 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4573 train loss: 1.0765506029129028 train acc: 0.9666666388511658 test loss: 1.3599406480789185 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4574 train loss: 1.0764706134796143 train acc: 0.9666666388511658 test loss: 1.3444969654083252 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4575 train loss: 1.0750805139541626 train acc: 0.96875 test loss: 1.344675898551941 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4576 train loss: 1.0764164924621582 train acc: 0.9666666388511658 test loss: 1.3549039363861084 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4577 train loss: 1.0762724876403809 train acc: 0.9666666388511658 test loss: 1.3496800661087036 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4578 train loss: 1.0765353441238403 train acc: 0.9666666388511658 test loss: 1.346641182899475 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4579 train loss: 1.0765126943588257 train acc: 0.9666666388511658 test loss: 1.354322075843811 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4580 train loss: 1.0762999057769775 train acc: 0.9666666388511658 test loss: 1.359911561012268 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4581 train loss: 1.076425313949585 train acc: 0.9666666388511658 test loss: 1.368582844734192 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4582 train loss: 1.0766899585723877 train acc: 0.9666666388511658 test loss: 1.3628008365631104 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4583 train loss: 1.0761160850524902 train acc: 0.9666666388511658 test loss: 1.3583838939666748 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4584 train loss: 1.0769927501678467 train acc: 0.9666666388511658 test loss: 1.3622685670852661 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4585 train loss: 1.0764943361282349 train acc: 0.9666666388511658 test loss: 1.357825756072998 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4586 train loss: 1.0773955583572388 train acc: 0.9666666388511658 test loss: 1.3407714366912842 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4587 train loss: 1.0764267444610596 train acc: 0.9666666388511658 test loss: 1.3613382577896118 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4588 train loss: 1.0765559673309326 train acc: 0.9666666388511658 test loss: 1.3527048826217651 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4589 train loss: 1.0766302347183228 train acc: 0.9666666388511658 test loss: 1.3456535339355469 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4590 train loss: 1.0761964321136475 train acc: 0.9666666388511658 test loss: 1.3573548793792725 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4591 train loss: 1.0765990018844604 train acc: 0.9666666388511658 test loss: 1.360538363456726 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4592 train loss: 1.076408863067627 train acc: 0.9666666388511658 test loss: 1.3642261028289795 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4593 train loss: 1.0762786865234375 train acc: 0.9666666388511658 test loss: 1.349288821220398 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4594 train loss: 1.076578974723816 train acc: 0.9666666388511658 test loss: 1.3462649583816528 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4595 train loss: 1.075434923171997 train acc: 0.96875 test loss: 1.3416234254837036 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4596 train loss: 1.0766985416412354 train acc: 0.9666666388511658 test loss: 1.3345898389816284 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4597 train loss: 1.0750974416732788 train acc: 0.96875 test loss: 1.3556571006774902 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4598 train loss: 1.0747770071029663 train acc: 0.96875 test loss: 1.3615589141845703 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4599 train loss: 1.076860785484314 train acc: 0.9666666388511658 test loss: 1.3450723886489868 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4600 train loss: 1.075809121131897 train acc: 0.96875 test loss: 1.3413450717926025 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4601 train loss: 1.0752723217010498 train acc: 0.96875 test loss: 1.3613379001617432 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4602 train loss: 1.074633240699768 train acc: 0.96875 test loss: 1.3438743352890015 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4603 train loss: 1.0745244026184082 train acc: 0.96875 test loss: 1.3457452058792114 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4604 train loss: 1.0743145942687988 train acc: 0.96875 test loss: 1.3394968509674072 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4605 train loss: 1.0751808881759644 train acc: 0.96875 test loss: 1.345588207244873 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4606 train loss: 1.074512004852295 train acc: 0.96875 test loss: 1.3433985710144043 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4607 train loss: 1.0745521783828735 train acc: 0.96875 test loss: 1.3403202295303345 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4608 train loss: 1.0745080709457397 train acc: 0.96875 test loss: 1.338216781616211 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4609 train loss: 1.0745511054992676 train acc: 0.96875 test loss: 1.345993161201477 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4610 train loss: 1.0744000673294067 train acc: 0.96875 test loss: 1.3353432416915894 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4611 train loss: 1.074489712715149 train acc: 0.96875 test loss: 1.3422369956970215 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4612 train loss: 1.074228286743164 train acc: 0.96875 test loss: 1.3486696481704712 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4613 train loss: 1.074378252029419 train acc: 0.96875 test loss: 1.3471819162368774 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4614 train loss: 1.0743132829666138 train acc: 0.96875 test loss: 1.3484643697738647 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4615 train loss: 1.0745731592178345 train acc: 0.96875 test loss: 1.340509057044983 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4616 train loss: 1.0744872093200684 train acc: 0.96875 test loss: 1.354494571685791 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4617 train loss: 1.0744990110397339 train acc: 0.96875 test loss: 1.3589861392974854 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4618 train loss: 1.0746796131134033 train acc: 0.96875 test loss: 1.3438032865524292 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4619 train loss: 1.0743606090545654 train acc: 0.96875 test loss: 1.350293517112732 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4620 train loss: 1.0744990110397339 train acc: 0.96875 test loss: 1.3511940240859985 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4621 train loss: 1.0744311809539795 train acc: 0.96875 test loss: 1.3695508241653442 best test loss: 1.283445954322815 test acc: 0.6583333611488342\n",
      "Epoch 4622 train loss: 1.0744856595993042 train acc: 0.96875 test loss: 1.3527870178222656 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4623 train loss: 1.0745502710342407 train acc: 0.96875 test loss: 1.3605376482009888 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4624 train loss: 1.074574589729309 train acc: 0.96875 test loss: 1.3539263010025024 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4625 train loss: 1.0744770765304565 train acc: 0.96875 test loss: 1.346320629119873 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4626 train loss: 1.07450532913208 train acc: 0.96875 test loss: 1.3550307750701904 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4627 train loss: 1.0745131969451904 train acc: 0.96875 test loss: 1.3391077518463135 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4628 train loss: 1.0743712186813354 train acc: 0.96875 test loss: 1.3525348901748657 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4629 train loss: 1.0744868516921997 train acc: 0.96875 test loss: 1.361304759979248 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4630 train loss: 1.0743740797042847 train acc: 0.96875 test loss: 1.3546429872512817 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4631 train loss: 1.0743998289108276 train acc: 0.96875 test loss: 1.3615368604660034 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4632 train loss: 1.0743716955184937 train acc: 0.96875 test loss: 1.3580002784729004 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4633 train loss: 1.07438325881958 train acc: 0.96875 test loss: 1.3271558284759521 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4634 train loss: 1.0743095874786377 train acc: 0.96875 test loss: 1.3331612348556519 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4635 train loss: 1.0743545293807983 train acc: 0.96875 test loss: 1.3599746227264404 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4636 train loss: 1.0745429992675781 train acc: 0.96875 test loss: 1.347658634185791 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4637 train loss: 1.074407696723938 train acc: 0.96875 test loss: 1.341932773590088 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4638 train loss: 1.0746440887451172 train acc: 0.96875 test loss: 1.3378524780273438 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4639 train loss: 1.0744682550430298 train acc: 0.96875 test loss: 1.3467326164245605 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4640 train loss: 1.0744855403900146 train acc: 0.96875 test loss: 1.353787899017334 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4641 train loss: 1.0743346214294434 train acc: 0.96875 test loss: 1.3613145351409912 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4642 train loss: 1.0743364095687866 train acc: 0.96875 test loss: 1.3443399667739868 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4643 train loss: 1.0743939876556396 train acc: 0.96875 test loss: 1.356005072593689 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4644 train loss: 1.074350357055664 train acc: 0.96875 test loss: 1.3501516580581665 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4645 train loss: 1.074323058128357 train acc: 0.96875 test loss: 1.3595901727676392 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4646 train loss: 1.0743868350982666 train acc: 0.96875 test loss: 1.362322211265564 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4647 train loss: 1.0744850635528564 train acc: 0.96875 test loss: 1.3629624843597412 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4648 train loss: 1.0745165348052979 train acc: 0.96875 test loss: 1.359904408454895 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4649 train loss: 1.0743721723556519 train acc: 0.96875 test loss: 1.3510640859603882 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4650 train loss: 1.0745750665664673 train acc: 0.96875 test loss: 1.3490822315216064 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4651 train loss: 1.0743370056152344 train acc: 0.96875 test loss: 1.363314151763916 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4652 train loss: 1.074442982673645 train acc: 0.96875 test loss: 1.3396586179733276 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4653 train loss: 1.0744290351867676 train acc: 0.96875 test loss: 1.3690627813339233 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4654 train loss: 1.0745866298675537 train acc: 0.96875 test loss: 1.3635140657424927 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4655 train loss: 1.07440185546875 train acc: 0.96875 test loss: 1.3634507656097412 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4656 train loss: 1.0743842124938965 train acc: 0.96875 test loss: 1.3472208976745605 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4657 train loss: 1.0744338035583496 train acc: 0.96875 test loss: 1.3638819456100464 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4658 train loss: 1.0742928981781006 train acc: 0.96875 test loss: 1.3644593954086304 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4659 train loss: 1.0744282007217407 train acc: 0.96875 test loss: 1.3423691987991333 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4660 train loss: 1.0744231939315796 train acc: 0.96875 test loss: 1.3577675819396973 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4661 train loss: 1.0743827819824219 train acc: 0.96875 test loss: 1.334479570388794 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4662 train loss: 1.074442982673645 train acc: 0.96875 test loss: 1.3487496376037598 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4663 train loss: 1.0742526054382324 train acc: 0.96875 test loss: 1.3511403799057007 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4664 train loss: 1.074190378189087 train acc: 0.96875 test loss: 1.3511768579483032 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4665 train loss: 1.074428677558899 train acc: 0.96875 test loss: 1.380461573600769 best test loss: 1.283445954322815 test acc: 0.6499999761581421\n",
      "Epoch 4666 train loss: 1.0743200778961182 train acc: 0.96875 test loss: 1.335370421409607 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4667 train loss: 1.0744107961654663 train acc: 0.96875 test loss: 1.3549798727035522 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4668 train loss: 1.074399471282959 train acc: 0.96875 test loss: 1.3567975759506226 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4669 train loss: 1.07453191280365 train acc: 0.96875 test loss: 1.3540674448013306 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4670 train loss: 1.0743777751922607 train acc: 0.96875 test loss: 1.361049771308899 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4671 train loss: 1.07449209690094 train acc: 0.96875 test loss: 1.3541104793548584 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4672 train loss: 1.0743635892868042 train acc: 0.96875 test loss: 1.3484885692596436 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4673 train loss: 1.0744282007217407 train acc: 0.96875 test loss: 1.3416860103607178 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4674 train loss: 1.0742653608322144 train acc: 0.96875 test loss: 1.3500449657440186 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4675 train loss: 1.0745130777359009 train acc: 0.96875 test loss: 1.3497984409332275 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4676 train loss: 1.0743826627731323 train acc: 0.96875 test loss: 1.3496003150939941 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4677 train loss: 1.0742849111557007 train acc: 0.96875 test loss: 1.3466328382492065 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4678 train loss: 1.0742111206054688 train acc: 0.96875 test loss: 1.3394322395324707 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4679 train loss: 1.074586033821106 train acc: 0.96875 test loss: 1.3460348844528198 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4680 train loss: 1.0745667219161987 train acc: 0.96875 test loss: 1.3397819995880127 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4681 train loss: 1.0744614601135254 train acc: 0.96875 test loss: 1.3512085676193237 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4682 train loss: 1.0745097398757935 train acc: 0.96875 test loss: 1.3558192253112793 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4683 train loss: 1.0744479894638062 train acc: 0.96875 test loss: 1.3553963899612427 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4684 train loss: 1.0744678974151611 train acc: 0.96875 test loss: 1.3533132076263428 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4685 train loss: 1.0745365619659424 train acc: 0.96875 test loss: 1.360488772392273 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4686 train loss: 1.07442045211792 train acc: 0.96875 test loss: 1.3516420125961304 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4687 train loss: 1.0742875337600708 train acc: 0.96875 test loss: 1.3297927379608154 best test loss: 1.283445954322815 test acc: 0.7250000238418579\n",
      "Epoch 4688 train loss: 1.0742207765579224 train acc: 0.96875 test loss: 1.354333519935608 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4689 train loss: 1.0742982625961304 train acc: 0.96875 test loss: 1.3538223505020142 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4690 train loss: 1.0743223428726196 train acc: 0.96875 test loss: 1.359025478363037 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4691 train loss: 1.0745984315872192 train acc: 0.96875 test loss: 1.3426640033721924 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4692 train loss: 1.0744132995605469 train acc: 0.96875 test loss: 1.3480721712112427 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4693 train loss: 1.074608325958252 train acc: 0.96875 test loss: 1.3580365180969238 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4694 train loss: 1.0745493173599243 train acc: 0.96875 test loss: 1.346744418144226 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4695 train loss: 1.0745177268981934 train acc: 0.96875 test loss: 1.3626792430877686 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4696 train loss: 1.0743281841278076 train acc: 0.96875 test loss: 1.3390387296676636 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4697 train loss: 1.0743626356124878 train acc: 0.96875 test loss: 1.3378013372421265 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4698 train loss: 1.0744003057479858 train acc: 0.96875 test loss: 1.360512375831604 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4699 train loss: 1.0744779109954834 train acc: 0.96875 test loss: 1.359976053237915 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4700 train loss: 1.074317216873169 train acc: 0.96875 test loss: 1.3462951183319092 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4701 train loss: 1.074371099472046 train acc: 0.96875 test loss: 1.3537788391113281 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4702 train loss: 1.0743685960769653 train acc: 0.96875 test loss: 1.3674484491348267 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4703 train loss: 1.0744049549102783 train acc: 0.96875 test loss: 1.3569610118865967 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4704 train loss: 1.074377417564392 train acc: 0.96875 test loss: 1.3369542360305786 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4705 train loss: 1.0743072032928467 train acc: 0.96875 test loss: 1.3625434637069702 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4706 train loss: 1.074442982673645 train acc: 0.96875 test loss: 1.3463456630706787 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4707 train loss: 1.0743560791015625 train acc: 0.96875 test loss: 1.3515874147415161 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4708 train loss: 1.0743900537490845 train acc: 0.96875 test loss: 1.3555928468704224 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4709 train loss: 1.0744706392288208 train acc: 0.96875 test loss: 1.3488198518753052 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4710 train loss: 1.0743991136550903 train acc: 0.96875 test loss: 1.3430315256118774 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4711 train loss: 1.074431300163269 train acc: 0.96875 test loss: 1.3580878973007202 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4712 train loss: 1.0744181871414185 train acc: 0.96875 test loss: 1.3653626441955566 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4713 train loss: 1.0743151903152466 train acc: 0.96875 test loss: 1.3490262031555176 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4714 train loss: 1.0743820667266846 train acc: 0.96875 test loss: 1.3635218143463135 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4715 train loss: 1.0742977857589722 train acc: 0.96875 test loss: 1.340790033340454 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4716 train loss: 1.0743783712387085 train acc: 0.96875 test loss: 1.3587173223495483 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4717 train loss: 1.0745446681976318 train acc: 0.96875 test loss: 1.3557037115097046 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4718 train loss: 1.0743184089660645 train acc: 0.96875 test loss: 1.3670461177825928 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4719 train loss: 1.0743457078933716 train acc: 0.96875 test loss: 1.3497354984283447 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4720 train loss: 1.074458360671997 train acc: 0.96875 test loss: 1.3533356189727783 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4721 train loss: 1.0742896795272827 train acc: 0.96875 test loss: 1.3598824739456177 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4722 train loss: 1.0744354724884033 train acc: 0.96875 test loss: 1.362488865852356 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4723 train loss: 1.074387788772583 train acc: 0.96875 test loss: 1.3414660692214966 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4724 train loss: 1.0744744539260864 train acc: 0.96875 test loss: 1.3529808521270752 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4725 train loss: 1.0745216608047485 train acc: 0.96875 test loss: 1.3570131063461304 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4726 train loss: 1.0743876695632935 train acc: 0.96875 test loss: 1.3605787754058838 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4727 train loss: 1.0744293928146362 train acc: 0.96875 test loss: 1.35481595993042 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4728 train loss: 1.074381947517395 train acc: 0.96875 test loss: 1.3464996814727783 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4729 train loss: 1.0744290351867676 train acc: 0.96875 test loss: 1.352077841758728 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4730 train loss: 1.0743072032928467 train acc: 0.96875 test loss: 1.330484390258789 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4731 train loss: 1.074218511581421 train acc: 0.96875 test loss: 1.3538991212844849 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4732 train loss: 1.074208378791809 train acc: 0.96875 test loss: 1.3517953157424927 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4733 train loss: 1.0744032859802246 train acc: 0.96875 test loss: 1.3509442806243896 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4734 train loss: 1.0743051767349243 train acc: 0.96875 test loss: 1.358539342880249 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4735 train loss: 1.0743579864501953 train acc: 0.96875 test loss: 1.3425313234329224 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4736 train loss: 1.074442744255066 train acc: 0.96875 test loss: 1.352947473526001 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4737 train loss: 1.0742937326431274 train acc: 0.96875 test loss: 1.338655948638916 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4738 train loss: 1.0743765830993652 train acc: 0.96875 test loss: 1.3582016229629517 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4739 train loss: 1.0743861198425293 train acc: 0.96875 test loss: 1.3480145931243896 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4740 train loss: 1.0743663311004639 train acc: 0.96875 test loss: 1.3622699975967407 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4741 train loss: 1.074493408203125 train acc: 0.96875 test loss: 1.3380850553512573 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4742 train loss: 1.0742504596710205 train acc: 0.96875 test loss: 1.3609650135040283 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4743 train loss: 1.0743805170059204 train acc: 0.96875 test loss: 1.3535417318344116 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4744 train loss: 1.0744324922561646 train acc: 0.96875 test loss: 1.3464924097061157 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4745 train loss: 1.074503779411316 train acc: 0.96875 test loss: 1.3587119579315186 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4746 train loss: 1.074462652206421 train acc: 0.96875 test loss: 1.3527933359146118 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4747 train loss: 1.074334740638733 train acc: 0.96875 test loss: 1.3494369983673096 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4748 train loss: 1.0742863416671753 train acc: 0.96875 test loss: 1.3592597246170044 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4749 train loss: 1.0743486881256104 train acc: 0.96875 test loss: 1.3578578233718872 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4750 train loss: 1.0743604898452759 train acc: 0.96875 test loss: 1.357279658317566 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4751 train loss: 1.074411392211914 train acc: 0.96875 test loss: 1.3512934446334839 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4752 train loss: 1.0745422840118408 train acc: 0.96875 test loss: 1.354149341583252 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4753 train loss: 1.0744084119796753 train acc: 0.96875 test loss: 1.3478683233261108 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4754 train loss: 1.0744097232818604 train acc: 0.96875 test loss: 1.3567739725112915 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4755 train loss: 1.074389100074768 train acc: 0.96875 test loss: 1.3484241962432861 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4756 train loss: 1.0743999481201172 train acc: 0.96875 test loss: 1.3567503690719604 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4757 train loss: 1.074325442314148 train acc: 0.96875 test loss: 1.3564203977584839 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4758 train loss: 1.0743465423583984 train acc: 0.96875 test loss: 1.3558651208877563 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4759 train loss: 1.0744092464447021 train acc: 0.96875 test loss: 1.361867904663086 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4760 train loss: 1.07439386844635 train acc: 0.96875 test loss: 1.3457082509994507 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4761 train loss: 1.0742366313934326 train acc: 0.96875 test loss: 1.3515938520431519 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4762 train loss: 1.0743635892868042 train acc: 0.96875 test loss: 1.3486063480377197 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4763 train loss: 1.0743021965026855 train acc: 0.96875 test loss: 1.3387483358383179 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4764 train loss: 1.074398398399353 train acc: 0.96875 test loss: 1.3503667116165161 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4765 train loss: 1.0744363069534302 train acc: 0.96875 test loss: 1.341774582862854 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4766 train loss: 1.0742840766906738 train acc: 0.96875 test loss: 1.353090763092041 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4767 train loss: 1.0742897987365723 train acc: 0.96875 test loss: 1.3473907709121704 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4768 train loss: 1.0742985010147095 train acc: 0.96875 test loss: 1.3395705223083496 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4769 train loss: 1.074465036392212 train acc: 0.96875 test loss: 1.3382697105407715 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4770 train loss: 1.07433021068573 train acc: 0.96875 test loss: 1.3447169065475464 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4771 train loss: 1.0742820501327515 train acc: 0.96875 test loss: 1.3409467935562134 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4772 train loss: 1.0744563341140747 train acc: 0.96875 test loss: 1.3487210273742676 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4773 train loss: 1.0744205713272095 train acc: 0.96875 test loss: 1.356459140777588 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4774 train loss: 1.074488878250122 train acc: 0.96875 test loss: 1.3565226793289185 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4775 train loss: 1.074257731437683 train acc: 0.96875 test loss: 1.3622798919677734 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4776 train loss: 1.0743964910507202 train acc: 0.96875 test loss: 1.3460955619812012 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4777 train loss: 1.0742496252059937 train acc: 0.96875 test loss: 1.3564563989639282 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4778 train loss: 1.07427978515625 train acc: 0.96875 test loss: 1.354601502418518 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4779 train loss: 1.074241280555725 train acc: 0.96875 test loss: 1.3452057838439941 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4780 train loss: 1.0743118524551392 train acc: 0.96875 test loss: 1.3360329866409302 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4781 train loss: 1.0744668245315552 train acc: 0.96875 test loss: 1.3481539487838745 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4782 train loss: 1.0743602514266968 train acc: 0.96875 test loss: 1.3377705812454224 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4783 train loss: 1.0742104053497314 train acc: 0.96875 test loss: 1.3383113145828247 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4784 train loss: 1.0744379758834839 train acc: 0.96875 test loss: 1.3378986120224 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4785 train loss: 1.074461579322815 train acc: 0.96875 test loss: 1.3606646060943604 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4786 train loss: 1.074399471282959 train acc: 0.96875 test loss: 1.3604779243469238 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4787 train loss: 1.0744493007659912 train acc: 0.96875 test loss: 1.3447099924087524 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4788 train loss: 1.074342131614685 train acc: 0.96875 test loss: 1.3554294109344482 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4789 train loss: 1.0742741823196411 train acc: 0.96875 test loss: 1.349861979484558 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4790 train loss: 1.0744132995605469 train acc: 0.96875 test loss: 1.3446762561798096 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4791 train loss: 1.0743454694747925 train acc: 0.96875 test loss: 1.3512400388717651 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4792 train loss: 1.0744112730026245 train acc: 0.96875 test loss: 1.3521075248718262 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4793 train loss: 1.0743359327316284 train acc: 0.96875 test loss: 1.3432179689407349 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4794 train loss: 1.074474573135376 train acc: 0.96875 test loss: 1.3533746004104614 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4795 train loss: 1.0743170976638794 train acc: 0.96875 test loss: 1.3460558652877808 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4796 train loss: 1.0743796825408936 train acc: 0.96875 test loss: 1.3578178882598877 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4797 train loss: 1.0742543935775757 train acc: 0.96875 test loss: 1.344408631324768 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4798 train loss: 1.0743637084960938 train acc: 0.96875 test loss: 1.3294309377670288 best test loss: 1.283445954322815 test acc: 0.7166666388511658\n",
      "Epoch 4799 train loss: 1.0743893384933472 train acc: 0.96875 test loss: 1.3548710346221924 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4800 train loss: 1.0742378234863281 train acc: 0.96875 test loss: 1.3461450338363647 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4801 train loss: 1.074365258216858 train acc: 0.96875 test loss: 1.349380373954773 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4802 train loss: 1.0743039846420288 train acc: 0.96875 test loss: 1.3480559587478638 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4803 train loss: 1.0743635892868042 train acc: 0.96875 test loss: 1.3547954559326172 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4804 train loss: 1.07433021068573 train acc: 0.96875 test loss: 1.352194905281067 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4805 train loss: 1.0742838382720947 train acc: 0.96875 test loss: 1.3419020175933838 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4806 train loss: 1.0744171142578125 train acc: 0.96875 test loss: 1.3498806953430176 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4807 train loss: 1.0742802619934082 train acc: 0.96875 test loss: 1.3478296995162964 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4808 train loss: 1.0742576122283936 train acc: 0.96875 test loss: 1.3511879444122314 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4809 train loss: 1.0742744207382202 train acc: 0.96875 test loss: 1.3501826524734497 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4810 train loss: 1.074168086051941 train acc: 0.96875 test loss: 1.3525575399398804 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4811 train loss: 1.0742076635360718 train acc: 0.96875 test loss: 1.3508071899414062 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4812 train loss: 1.0742096900939941 train acc: 0.96875 test loss: 1.3424931764602661 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4813 train loss: 1.0743792057037354 train acc: 0.96875 test loss: 1.3620494604110718 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4814 train loss: 1.074255108833313 train acc: 0.96875 test loss: 1.347730040550232 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4815 train loss: 1.0743319988250732 train acc: 0.96875 test loss: 1.3538973331451416 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4816 train loss: 1.074346661567688 train acc: 0.96875 test loss: 1.3479384183883667 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4817 train loss: 1.0743317604064941 train acc: 0.96875 test loss: 1.347284197807312 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4818 train loss: 1.0745123624801636 train acc: 0.96875 test loss: 1.3526530265808105 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4819 train loss: 1.0741630792617798 train acc: 0.96875 test loss: 1.3547662496566772 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4820 train loss: 1.0744527578353882 train acc: 0.96875 test loss: 1.3585846424102783 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4821 train loss: 1.0743690729141235 train acc: 0.96875 test loss: 1.3533577919006348 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4822 train loss: 1.0743122100830078 train acc: 0.96875 test loss: 1.3542643785476685 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4823 train loss: 1.0741606950759888 train acc: 0.96875 test loss: 1.357771396636963 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4824 train loss: 1.0742815732955933 train acc: 0.96875 test loss: 1.3573334217071533 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4825 train loss: 1.0743176937103271 train acc: 0.96875 test loss: 1.352315068244934 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4826 train loss: 1.0742871761322021 train acc: 0.96875 test loss: 1.3412374258041382 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4827 train loss: 1.0743825435638428 train acc: 0.96875 test loss: 1.347039818763733 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4828 train loss: 1.0743240118026733 train acc: 0.96875 test loss: 1.3598228693008423 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4829 train loss: 1.0742987394332886 train acc: 0.96875 test loss: 1.3488705158233643 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4830 train loss: 1.0742604732513428 train acc: 0.96875 test loss: 1.3467485904693604 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4831 train loss: 1.0743829011917114 train acc: 0.96875 test loss: 1.3498226404190063 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4832 train loss: 1.074340581893921 train acc: 0.96875 test loss: 1.3514412641525269 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4833 train loss: 1.0743051767349243 train acc: 0.96875 test loss: 1.3428778648376465 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4834 train loss: 1.074168086051941 train acc: 0.96875 test loss: 1.3584760427474976 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4835 train loss: 1.074223279953003 train acc: 0.96875 test loss: 1.3393661975860596 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4836 train loss: 1.0742672681808472 train acc: 0.96875 test loss: 1.347941279411316 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4837 train loss: 1.0742772817611694 train acc: 0.96875 test loss: 1.3398535251617432 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4838 train loss: 1.0742437839508057 train acc: 0.96875 test loss: 1.3526501655578613 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4839 train loss: 1.0742199420928955 train acc: 0.96875 test loss: 1.3532052040100098 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4840 train loss: 1.074388861656189 train acc: 0.96875 test loss: 1.3647602796554565 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4841 train loss: 1.0742452144622803 train acc: 0.96875 test loss: 1.3460804224014282 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4842 train loss: 1.074267029762268 train acc: 0.96875 test loss: 1.3433188199996948 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4843 train loss: 1.0743508338928223 train acc: 0.96875 test loss: 1.3520984649658203 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4844 train loss: 1.074216365814209 train acc: 0.96875 test loss: 1.3611515760421753 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4845 train loss: 1.0742989778518677 train acc: 0.96875 test loss: 1.3514784574508667 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4846 train loss: 1.0743111371994019 train acc: 0.96875 test loss: 1.3431899547576904 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4847 train loss: 1.0742249488830566 train acc: 0.96875 test loss: 1.3517115116119385 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4848 train loss: 1.0742650032043457 train acc: 0.96875 test loss: 1.346405029296875 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4849 train loss: 1.0744527578353882 train acc: 0.96875 test loss: 1.354040503501892 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4850 train loss: 1.074180006980896 train acc: 0.96875 test loss: 1.3514147996902466 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4851 train loss: 1.0742021799087524 train acc: 0.96875 test loss: 1.3507921695709229 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4852 train loss: 1.0743941068649292 train acc: 0.96875 test loss: 1.3688217401504517 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4853 train loss: 1.0742347240447998 train acc: 0.96875 test loss: 1.3477786779403687 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4854 train loss: 1.0741591453552246 train acc: 0.96875 test loss: 1.3451839685440063 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4855 train loss: 1.074266791343689 train acc: 0.96875 test loss: 1.3530552387237549 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4856 train loss: 1.0744166374206543 train acc: 0.96875 test loss: 1.3517563343048096 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4857 train loss: 1.074327826499939 train acc: 0.96875 test loss: 1.348284363746643 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4858 train loss: 1.074278712272644 train acc: 0.96875 test loss: 1.349818468093872 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4859 train loss: 1.074368953704834 train acc: 0.96875 test loss: 1.349111795425415 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4860 train loss: 1.0743391513824463 train acc: 0.96875 test loss: 1.3502802848815918 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4861 train loss: 1.0741963386535645 train acc: 0.96875 test loss: 1.3514713048934937 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4862 train loss: 1.0743356943130493 train acc: 0.96875 test loss: 1.3548990488052368 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4863 train loss: 1.0742454528808594 train acc: 0.96875 test loss: 1.3635883331298828 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4864 train loss: 1.0741621255874634 train acc: 0.96875 test loss: 1.3514025211334229 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4865 train loss: 1.0743035078048706 train acc: 0.96875 test loss: 1.3586726188659668 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4866 train loss: 1.0743128061294556 train acc: 0.96875 test loss: 1.3638321161270142 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4867 train loss: 1.0744837522506714 train acc: 0.96875 test loss: 1.3496447801589966 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4868 train loss: 1.0742294788360596 train acc: 0.96875 test loss: 1.3392436504364014 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4869 train loss: 1.0743600130081177 train acc: 0.96875 test loss: 1.3546717166900635 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4870 train loss: 1.0742262601852417 train acc: 0.96875 test loss: 1.3497974872589111 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4871 train loss: 1.0741227865219116 train acc: 0.96875 test loss: 1.3354530334472656 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4872 train loss: 1.0743675231933594 train acc: 0.96875 test loss: 1.3468159437179565 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4873 train loss: 1.0743716955184937 train acc: 0.96875 test loss: 1.3474076986312866 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4874 train loss: 1.0741522312164307 train acc: 0.96875 test loss: 1.358316421508789 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4875 train loss: 1.0741981267929077 train acc: 0.96875 test loss: 1.344706416130066 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4876 train loss: 1.0741753578186035 train acc: 0.96875 test loss: 1.3482520580291748 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4877 train loss: 1.0744514465332031 train acc: 0.96875 test loss: 1.3633562326431274 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4878 train loss: 1.0743980407714844 train acc: 0.96875 test loss: 1.3515950441360474 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4879 train loss: 1.0742274522781372 train acc: 0.96875 test loss: 1.3489125967025757 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4880 train loss: 1.0744209289550781 train acc: 0.96875 test loss: 1.3458529710769653 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4881 train loss: 1.074231743812561 train acc: 0.96875 test loss: 1.3548753261566162 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4882 train loss: 1.0742992162704468 train acc: 0.96875 test loss: 1.3512320518493652 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4883 train loss: 1.0742640495300293 train acc: 0.96875 test loss: 1.3464865684509277 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4884 train loss: 1.074222445487976 train acc: 0.96875 test loss: 1.365816593170166 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4885 train loss: 1.0742056369781494 train acc: 0.96875 test loss: 1.3673014640808105 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4886 train loss: 1.0743329524993896 train acc: 0.96875 test loss: 1.3462008237838745 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4887 train loss: 1.07412588596344 train acc: 0.96875 test loss: 1.3570345640182495 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4888 train loss: 1.0742449760437012 train acc: 0.96875 test loss: 1.359246850013733 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4889 train loss: 1.0743939876556396 train acc: 0.96875 test loss: 1.3488932847976685 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4890 train loss: 1.0744510889053345 train acc: 0.96875 test loss: 1.3535946607589722 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4891 train loss: 1.0743584632873535 train acc: 0.96875 test loss: 1.3404299020767212 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4892 train loss: 1.0743087530136108 train acc: 0.96875 test loss: 1.3412266969680786 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4893 train loss: 1.0743461847305298 train acc: 0.96875 test loss: 1.362407922744751 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4894 train loss: 1.0743482112884521 train acc: 0.96875 test loss: 1.3465896844863892 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4895 train loss: 1.0741831064224243 train acc: 0.96875 test loss: 1.3510100841522217 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4896 train loss: 1.0743440389633179 train acc: 0.96875 test loss: 1.3393043279647827 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4897 train loss: 1.0743181705474854 train acc: 0.96875 test loss: 1.3498890399932861 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4898 train loss: 1.074408769607544 train acc: 0.96875 test loss: 1.3602582216262817 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4899 train loss: 1.0743876695632935 train acc: 0.96875 test loss: 1.3419654369354248 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4900 train loss: 1.0743898153305054 train acc: 0.96875 test loss: 1.3626213073730469 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4901 train loss: 1.074347734451294 train acc: 0.96875 test loss: 1.35164213180542 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4902 train loss: 1.0743311643600464 train acc: 0.96875 test loss: 1.3576966524124146 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4903 train loss: 1.0744011402130127 train acc: 0.96875 test loss: 1.3419181108474731 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4904 train loss: 1.0744339227676392 train acc: 0.96875 test loss: 1.3570481538772583 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4905 train loss: 1.0743063688278198 train acc: 0.96875 test loss: 1.33646559715271 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4906 train loss: 1.0742814540863037 train acc: 0.96875 test loss: 1.3487491607666016 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4907 train loss: 1.0742793083190918 train acc: 0.96875 test loss: 1.3543086051940918 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4908 train loss: 1.0742697715759277 train acc: 0.96875 test loss: 1.3534328937530518 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4909 train loss: 1.0743672847747803 train acc: 0.96875 test loss: 1.363158941268921 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4910 train loss: 1.0742255449295044 train acc: 0.96875 test loss: 1.3523355722427368 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4911 train loss: 1.0741729736328125 train acc: 0.96875 test loss: 1.3455384969711304 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4912 train loss: 1.0743789672851562 train acc: 0.96875 test loss: 1.3565071821212769 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4913 train loss: 1.0744068622589111 train acc: 0.96875 test loss: 1.340496301651001 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4914 train loss: 1.0743372440338135 train acc: 0.96875 test loss: 1.3340117931365967 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4915 train loss: 1.0743011236190796 train acc: 0.96875 test loss: 1.3618460893630981 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4916 train loss: 1.0744655132293701 train acc: 0.96875 test loss: 1.3560330867767334 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4917 train loss: 1.0743612051010132 train acc: 0.96875 test loss: 1.3572849035263062 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4918 train loss: 1.0744037628173828 train acc: 0.96875 test loss: 1.339965581893921 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4919 train loss: 1.0742958784103394 train acc: 0.96875 test loss: 1.3567556142807007 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4920 train loss: 1.0744602680206299 train acc: 0.96875 test loss: 1.3438318967819214 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4921 train loss: 1.0743557214736938 train acc: 0.96875 test loss: 1.3574869632720947 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4922 train loss: 1.0743838548660278 train acc: 0.96875 test loss: 1.3531399965286255 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4923 train loss: 1.0742467641830444 train acc: 0.96875 test loss: 1.3405901193618774 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4924 train loss: 1.0743430852890015 train acc: 0.96875 test loss: 1.3484364748001099 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4925 train loss: 1.0742225646972656 train acc: 0.96875 test loss: 1.3554047346115112 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4926 train loss: 1.074222207069397 train acc: 0.96875 test loss: 1.3508213758468628 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4927 train loss: 1.0742361545562744 train acc: 0.96875 test loss: 1.353383183479309 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4928 train loss: 1.0743868350982666 train acc: 0.96875 test loss: 1.344385027885437 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4929 train loss: 1.0743001699447632 train acc: 0.96875 test loss: 1.3530508279800415 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4930 train loss: 1.0742700099945068 train acc: 0.96875 test loss: 1.3680521249771118 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4931 train loss: 1.07438325881958 train acc: 0.96875 test loss: 1.3541679382324219 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4932 train loss: 1.0743346214294434 train acc: 0.96875 test loss: 1.3461830615997314 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4933 train loss: 1.0743080377578735 train acc: 0.96875 test loss: 1.360310673713684 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4934 train loss: 1.0744038820266724 train acc: 0.96875 test loss: 1.3525875806808472 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4935 train loss: 1.0743252038955688 train acc: 0.96875 test loss: 1.356947660446167 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4936 train loss: 1.074288010597229 train acc: 0.96875 test loss: 1.366382360458374 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4937 train loss: 1.0742501020431519 train acc: 0.96875 test loss: 1.3605763912200928 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4938 train loss: 1.0743130445480347 train acc: 0.96875 test loss: 1.3488163948059082 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4939 train loss: 1.0742005109786987 train acc: 0.96875 test loss: 1.346997618675232 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4940 train loss: 1.074366807937622 train acc: 0.96875 test loss: 1.3499720096588135 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4941 train loss: 1.0742498636245728 train acc: 0.96875 test loss: 1.365259051322937 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4942 train loss: 1.0742095708847046 train acc: 0.96875 test loss: 1.358016848564148 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4943 train loss: 1.0742721557617188 train acc: 0.96875 test loss: 1.3409055471420288 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4944 train loss: 1.0742994546890259 train acc: 0.96875 test loss: 1.3490715026855469 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4945 train loss: 1.0741113424301147 train acc: 0.96875 test loss: 1.3498413562774658 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4946 train loss: 1.0742896795272827 train acc: 0.96875 test loss: 1.3626576662063599 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4947 train loss: 1.074155330657959 train acc: 0.96875 test loss: 1.3537522554397583 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4948 train loss: 1.0742524862289429 train acc: 0.96875 test loss: 1.353786587715149 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4949 train loss: 1.074248194694519 train acc: 0.96875 test loss: 1.3609851598739624 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4950 train loss: 1.074301838874817 train acc: 0.96875 test loss: 1.3601454496383667 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4951 train loss: 1.074218511581421 train acc: 0.96875 test loss: 1.3383958339691162 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4952 train loss: 1.0742027759552002 train acc: 0.96875 test loss: 1.3466600179672241 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4953 train loss: 1.074063777923584 train acc: 0.96875 test loss: 1.3420811891555786 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4954 train loss: 1.0741753578186035 train acc: 0.96875 test loss: 1.3459398746490479 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4955 train loss: 1.07435142993927 train acc: 0.96875 test loss: 1.3544832468032837 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4956 train loss: 1.0742757320404053 train acc: 0.96875 test loss: 1.360404133796692 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4957 train loss: 1.0742393732070923 train acc: 0.96875 test loss: 1.3445428609848022 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4958 train loss: 1.0743348598480225 train acc: 0.96875 test loss: 1.3560682535171509 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4959 train loss: 1.0744190216064453 train acc: 0.96875 test loss: 1.3456141948699951 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4960 train loss: 1.0744754076004028 train acc: 0.96875 test loss: 1.351324439048767 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4961 train loss: 1.0741342306137085 train acc: 0.96875 test loss: 1.3485393524169922 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4962 train loss: 1.0743008852005005 train acc: 0.96875 test loss: 1.3613168001174927 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4963 train loss: 1.0741922855377197 train acc: 0.96875 test loss: 1.3463311195373535 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4964 train loss: 1.0742186307907104 train acc: 0.96875 test loss: 1.3594856262207031 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4965 train loss: 1.0742706060409546 train acc: 0.96875 test loss: 1.3606064319610596 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4966 train loss: 1.0743783712387085 train acc: 0.96875 test loss: 1.3507139682769775 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4967 train loss: 1.0743051767349243 train acc: 0.96875 test loss: 1.3514628410339355 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4968 train loss: 1.0741914510726929 train acc: 0.96875 test loss: 1.3552292585372925 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4969 train loss: 1.0741379261016846 train acc: 0.96875 test loss: 1.3453537225723267 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4970 train loss: 1.074181318283081 train acc: 0.96875 test loss: 1.3456714153289795 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4971 train loss: 1.0742262601852417 train acc: 0.96875 test loss: 1.35599684715271 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4972 train loss: 1.0742530822753906 train acc: 0.96875 test loss: 1.357715368270874 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4973 train loss: 1.0742331743240356 train acc: 0.96875 test loss: 1.3509899377822876 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4974 train loss: 1.0741740465164185 train acc: 0.96875 test loss: 1.3529382944107056 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4975 train loss: 1.0742543935775757 train acc: 0.96875 test loss: 1.3619818687438965 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4976 train loss: 1.0743937492370605 train acc: 0.96875 test loss: 1.3495237827301025 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4977 train loss: 1.0744826793670654 train acc: 0.96875 test loss: 1.3617103099822998 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4978 train loss: 1.0741914510726929 train acc: 0.96875 test loss: 1.3540433645248413 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4979 train loss: 1.07432222366333 train acc: 0.96875 test loss: 1.3502497673034668 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4980 train loss: 1.0741983652114868 train acc: 0.96875 test loss: 1.334226369857788 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4981 train loss: 1.0742393732070923 train acc: 0.96875 test loss: 1.3627573251724243 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4982 train loss: 1.0742582082748413 train acc: 0.96875 test loss: 1.3531852960586548 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4983 train loss: 1.0742530822753906 train acc: 0.96875 test loss: 1.3375526666641235 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4984 train loss: 1.0747050046920776 train acc: 0.96875 test loss: 1.355985164642334 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4985 train loss: 1.0743014812469482 train acc: 0.96875 test loss: 1.3483830690383911 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4986 train loss: 1.0742363929748535 train acc: 0.96875 test loss: 1.3447339534759521 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4987 train loss: 1.074266791343689 train acc: 0.96875 test loss: 1.3632344007492065 best test loss: 1.283445954322815 test acc: 0.675000011920929\n",
      "Epoch 4988 train loss: 1.0742640495300293 train acc: 0.96875 test loss: 1.3487156629562378 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4989 train loss: 1.0743237733840942 train acc: 0.96875 test loss: 1.3490617275238037 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4990 train loss: 1.0741424560546875 train acc: 0.96875 test loss: 1.3554863929748535 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4991 train loss: 1.0744904279708862 train acc: 0.96875 test loss: 1.3515269756317139 best test loss: 1.283445954322815 test acc: 0.699999988079071\n",
      "Epoch 4992 train loss: 1.0741157531738281 train acc: 0.96875 test loss: 1.3576874732971191 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4993 train loss: 1.074324369430542 train acc: 0.96875 test loss: 1.355850338935852 best test loss: 1.283445954322815 test acc: 0.6833333373069763\n",
      "Epoch 4994 train loss: 1.0743746757507324 train acc: 0.96875 test loss: 1.3440589904785156 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4995 train loss: 1.0743273496627808 train acc: 0.96875 test loss: 1.3478459119796753 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4996 train loss: 1.07420015335083 train acc: 0.96875 test loss: 1.3640543222427368 best test loss: 1.283445954322815 test acc: 0.6666666865348816\n",
      "Epoch 4997 train loss: 1.074189305305481 train acc: 0.96875 test loss: 1.3436318635940552 best test loss: 1.283445954322815 test acc: 0.7083333134651184\n",
      "Epoch 4998 train loss: 1.0743567943572998 train acc: 0.96875 test loss: 1.3586983680725098 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n",
      "Epoch 4999 train loss: 1.0742369890213013 train acc: 0.96875 test loss: 1.3510507345199585 best test loss: 1.283445954322815 test acc: 0.6916666626930237\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model(inputs_train)\n",
    "    train_loss = criterion(preds_train, outputs_train)\n",
    "    train_loss.backward()\n",
    "    train_acc = (preds_train.argmax(1) == outputs_train.argmax(1)).float().mean()\n",
    "    optimizer.step()\n",
    "\n",
    "    preds_test = model(inputs_test)\n",
    "    test_loss = criterion(preds_test, outputs_test)\n",
    "    test_acc = (preds_test.argmax(1) == outputs_test.argmax(1)).float().mean()\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'action_classifier.pt')\n",
    "\n",
    "    print('Epoch {} train loss: {} train acc: {} test loss: {} best test loss: {} test acc: {}'.format(epoch, train_loss, train_acc, test_loss, best_test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActionClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (fc4): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout4): Dropout(p=0.2, inplace=False)\n",
       "  (fc5): Linear(in_features=384, out_features=6, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_classifier = ActionClassifier()\n",
    "action_classifier.load_state_dict(torch.load('action_classifier.pt'))\n",
    "action_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    input_ = torch.Tensor(encoder.encode([text]))\n",
    "    output = action_classifier(input_)\n",
    "    return classes[output.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treehacks24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
