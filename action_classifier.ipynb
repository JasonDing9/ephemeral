{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arvind/miniconda3/envs/treehacks24/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"clarify\", \"email\", \"link\", \"schedule\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 768])\n"
     ]
    }
   ],
   "source": [
    "# Walk through all files in a directory\n",
    "import os\n",
    "lines = []\n",
    "for class_ in classes:\n",
    "    # Read contents of file\n",
    "    with open('data/action_classification/' + class_ + '.txt', 'r') as f:\n",
    "        lines += f.readlines()\n",
    "\n",
    "inputs = encoder.encode(lines)\n",
    "inputs = torch.Tensor(inputs)\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 5])\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "for class_ in classes:\n",
    "    output_1he = torch.Tensor([0] * len(classes))\n",
    "    output_1he[classes.index(class_)] = 1\n",
    "    for i in range(100):\n",
    "        outputs.append(output_1he)\n",
    "\n",
    "outputs = torch.stack(outputs)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "inputs_train, inputs_test, outputs_train, outputs_test = train_test_split(inputs, outputs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(768, 384)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(384, 384)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(384, 384)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.fc4 = nn.Linear(384, 384)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.fc5 = nn.Linear(384, len(classes))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.gelu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = F.gelu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = F.gelu(self.fc5(x))\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = ActionClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train loss: 1.6094944477081299 train acc: 0.18000000715255737 test loss: 1.6091312170028687 best test loss: 1.6091312170028687 test acc: 0.2800000011920929\n",
      "Epoch 1 train loss: 1.6095086336135864 train acc: 0.18000000715255737 test loss: 1.609305739402771 best test loss: 1.6091312170028687 test acc: 0.2800000011920929\n",
      "Epoch 2 train loss: 1.6094298362731934 train acc: 0.18000000715255737 test loss: 1.6093095541000366 best test loss: 1.6091312170028687 test acc: 0.2800000011920929\n",
      "Epoch 3 train loss: 1.6094456911087036 train acc: 0.18000000715255737 test loss: 1.609419345855713 best test loss: 1.6091312170028687 test acc: 0.25999999046325684\n",
      "Epoch 4 train loss: 1.6093816757202148 train acc: 0.18000000715255737 test loss: 1.609339714050293 best test loss: 1.6091312170028687 test acc: 0.27000001072883606\n",
      "Epoch 5 train loss: 1.609422206878662 train acc: 0.1875 test loss: 1.6094322204589844 best test loss: 1.6091312170028687 test acc: 0.27000001072883606\n",
      "Epoch 6 train loss: 1.60941743850708 train acc: 0.17749999463558197 test loss: 1.6095435619354248 best test loss: 1.6091312170028687 test acc: 0.27000001072883606\n",
      "Epoch 7 train loss: 1.609366774559021 train acc: 0.19249999523162842 test loss: 1.6095625162124634 best test loss: 1.6091312170028687 test acc: 0.25999999046325684\n",
      "Epoch 8 train loss: 1.6093759536743164 train acc: 0.19249999523162842 test loss: 1.609679102897644 best test loss: 1.6091312170028687 test acc: 0.2199999988079071\n",
      "Epoch 9 train loss: 1.6093357801437378 train acc: 0.19750000536441803 test loss: 1.609650731086731 best test loss: 1.6091312170028687 test acc: 0.23000000417232513\n",
      "Epoch 10 train loss: 1.609276294708252 train acc: 0.23749999701976776 test loss: 1.6097345352172852 best test loss: 1.6091312170028687 test acc: 0.17000000178813934\n",
      "Epoch 11 train loss: 1.6092983484268188 train acc: 0.23250000178813934 test loss: 1.6097722053527832 best test loss: 1.6091312170028687 test acc: 0.1899999976158142\n",
      "Epoch 12 train loss: 1.6092416048049927 train acc: 0.22499999403953552 test loss: 1.609861135482788 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 13 train loss: 1.609256386756897 train acc: 0.2150000035762787 test loss: 1.609857439994812 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 14 train loss: 1.6091835498809814 train acc: 0.22750000655651093 test loss: 1.6098878383636475 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 15 train loss: 1.6091742515563965 train acc: 0.23499999940395355 test loss: 1.6099859476089478 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 16 train loss: 1.609139084815979 train acc: 0.2524999976158142 test loss: 1.6099504232406616 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 17 train loss: 1.6091444492340088 train acc: 0.2475000023841858 test loss: 1.6100893020629883 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 18 train loss: 1.6091176271438599 train acc: 0.23999999463558197 test loss: 1.610032081604004 best test loss: 1.6091312170028687 test acc: 0.17000000178813934\n",
      "Epoch 19 train loss: 1.6090487241744995 train acc: 0.24500000476837158 test loss: 1.6100174188613892 best test loss: 1.6091312170028687 test acc: 0.18000000715255737\n",
      "Epoch 20 train loss: 1.609006404876709 train acc: 0.25999999046325684 test loss: 1.6102089881896973 best test loss: 1.6091312170028687 test acc: 0.15000000596046448\n",
      "Epoch 21 train loss: 1.609036922454834 train acc: 0.20999999344348907 test loss: 1.6103198528289795 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 22 train loss: 1.608931303024292 train acc: 0.2475000023841858 test loss: 1.6102603673934937 best test loss: 1.6091312170028687 test acc: 0.18000000715255737\n",
      "Epoch 23 train loss: 1.608921766281128 train acc: 0.2800000011920929 test loss: 1.6103224754333496 best test loss: 1.6091312170028687 test acc: 0.10000000149011612\n",
      "Epoch 24 train loss: 1.6089004278182983 train acc: 0.22750000655651093 test loss: 1.610419750213623 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 25 train loss: 1.6088131666183472 train acc: 0.2524999976158142 test loss: 1.610377550125122 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 26 train loss: 1.6087809801101685 train acc: 0.27000001072883606 test loss: 1.6104556322097778 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 27 train loss: 1.6087106466293335 train acc: 0.26249998807907104 test loss: 1.6105422973632812 best test loss: 1.6091312170028687 test acc: 0.17000000178813934\n",
      "Epoch 28 train loss: 1.6086974143981934 train acc: 0.26249998807907104 test loss: 1.6105995178222656 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 29 train loss: 1.6085937023162842 train acc: 0.26249998807907104 test loss: 1.6106772422790527 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 30 train loss: 1.6085443496704102 train acc: 0.2524999976158142 test loss: 1.61063551902771 best test loss: 1.6091312170028687 test acc: 0.15000000596046448\n",
      "Epoch 31 train loss: 1.6084283590316772 train acc: 0.26249998807907104 test loss: 1.6107449531555176 best test loss: 1.6091312170028687 test acc: 0.10999999940395355\n",
      "Epoch 32 train loss: 1.608360767364502 train acc: 0.27250000834465027 test loss: 1.6108006238937378 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 33 train loss: 1.608293890953064 train acc: 0.25999999046325684 test loss: 1.6107699871063232 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 34 train loss: 1.608171820640564 train acc: 0.25999999046325684 test loss: 1.6109100580215454 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 35 train loss: 1.6080524921417236 train acc: 0.25 test loss: 1.6108691692352295 best test loss: 1.6091312170028687 test acc: 0.15000000596046448\n",
      "Epoch 36 train loss: 1.6080071926116943 train acc: 0.27250000834465027 test loss: 1.6108590364456177 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 37 train loss: 1.6078683137893677 train acc: 0.26499998569488525 test loss: 1.6109646558761597 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 38 train loss: 1.6076560020446777 train acc: 0.27000001072883606 test loss: 1.6110026836395264 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 39 train loss: 1.6075432300567627 train acc: 0.26499998569488525 test loss: 1.6112589836120605 best test loss: 1.6091312170028687 test acc: 0.10999999940395355\n",
      "Epoch 40 train loss: 1.6074527502059937 train acc: 0.2475000023841858 test loss: 1.6110420227050781 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 41 train loss: 1.6072381734848022 train acc: 0.25 test loss: 1.6111358404159546 best test loss: 1.6091312170028687 test acc: 0.10999999940395355\n",
      "Epoch 42 train loss: 1.607038974761963 train acc: 0.26249998807907104 test loss: 1.6111915111541748 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 43 train loss: 1.6068307161331177 train acc: 0.26750001311302185 test loss: 1.6110912561416626 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 44 train loss: 1.606562614440918 train acc: 0.2750000059604645 test loss: 1.6112347841262817 best test loss: 1.6091312170028687 test acc: 0.10000000149011612\n",
      "Epoch 45 train loss: 1.6063308715820312 train acc: 0.25999999046325684 test loss: 1.6112273931503296 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 46 train loss: 1.6060004234313965 train acc: 0.25999999046325684 test loss: 1.6111514568328857 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 47 train loss: 1.6056746244430542 train acc: 0.25999999046325684 test loss: 1.6111630201339722 best test loss: 1.6091312170028687 test acc: 0.10000000149011612\n",
      "Epoch 48 train loss: 1.605293869972229 train acc: 0.26750001311302185 test loss: 1.6114435195922852 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 49 train loss: 1.6049554347991943 train acc: 0.28999999165534973 test loss: 1.610935926437378 best test loss: 1.6091312170028687 test acc: 0.15000000596046448\n",
      "Epoch 50 train loss: 1.604493260383606 train acc: 0.27250000834465027 test loss: 1.6114189624786377 best test loss: 1.6091312170028687 test acc: 0.11999999731779099\n",
      "Epoch 51 train loss: 1.6039948463439941 train acc: 0.2849999964237213 test loss: 1.6112922430038452 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 52 train loss: 1.603324294090271 train acc: 0.29249998927116394 test loss: 1.6113821268081665 best test loss: 1.6091312170028687 test acc: 0.12999999523162842\n",
      "Epoch 53 train loss: 1.6027637720108032 train acc: 0.3075000047683716 test loss: 1.611296534538269 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 54 train loss: 1.6022030115127563 train acc: 0.3125 test loss: 1.6111642122268677 best test loss: 1.6091312170028687 test acc: 0.17000000178813934\n",
      "Epoch 55 train loss: 1.6014355421066284 train acc: 0.28999999165534973 test loss: 1.61153244972229 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 56 train loss: 1.6004432439804077 train acc: 0.32499998807907104 test loss: 1.611444354057312 best test loss: 1.6091312170028687 test acc: 0.15000000596046448\n",
      "Epoch 57 train loss: 1.5997412204742432 train acc: 0.3125 test loss: 1.6108890771865845 best test loss: 1.6091312170028687 test acc: 0.18000000715255737\n",
      "Epoch 58 train loss: 1.5986241102218628 train acc: 0.32249999046325684 test loss: 1.6116604804992676 best test loss: 1.6091312170028687 test acc: 0.14000000059604645\n",
      "Epoch 59 train loss: 1.5972164869308472 train acc: 0.3400000035762787 test loss: 1.6110843420028687 best test loss: 1.6091312170028687 test acc: 0.1599999964237213\n",
      "Epoch 60 train loss: 1.596043586730957 train acc: 0.33000001311302185 test loss: 1.61063551902771 best test loss: 1.6091312170028687 test acc: 0.17000000178813934\n",
      "Epoch 61 train loss: 1.5947293043136597 train acc: 0.3425000011920929 test loss: 1.6109172105789185 best test loss: 1.6091312170028687 test acc: 0.1899999976158142\n",
      "Epoch 62 train loss: 1.593198299407959 train acc: 0.3425000011920929 test loss: 1.6103978157043457 best test loss: 1.6091312170028687 test acc: 0.17000000178813934\n",
      "Epoch 63 train loss: 1.5915066003799438 train acc: 0.36500000953674316 test loss: 1.610498070716858 best test loss: 1.6091312170028687 test acc: 0.20999999344348907\n",
      "Epoch 64 train loss: 1.5892767906188965 train acc: 0.36250001192092896 test loss: 1.6101489067077637 best test loss: 1.6091312170028687 test acc: 0.20000000298023224\n",
      "Epoch 65 train loss: 1.5871095657348633 train acc: 0.35499998927116394 test loss: 1.6099122762680054 best test loss: 1.6091312170028687 test acc: 0.20999999344348907\n",
      "Epoch 66 train loss: 1.584936261177063 train acc: 0.35249999165534973 test loss: 1.610375165939331 best test loss: 1.6091312170028687 test acc: 0.1899999976158142\n",
      "Epoch 67 train loss: 1.58286452293396 train acc: 0.3425000011920929 test loss: 1.6095757484436035 best test loss: 1.6091312170028687 test acc: 0.23000000417232513\n",
      "Epoch 68 train loss: 1.5795689821243286 train acc: 0.32499998807907104 test loss: 1.6087219715118408 best test loss: 1.6087219715118408 test acc: 0.23000000417232513\n",
      "Epoch 69 train loss: 1.5765349864959717 train acc: 0.32249999046325684 test loss: 1.6084438562393188 best test loss: 1.6084438562393188 test acc: 0.20999999344348907\n",
      "Epoch 70 train loss: 1.574556589126587 train acc: 0.3149999976158142 test loss: 1.6067181825637817 best test loss: 1.6067181825637817 test acc: 0.1899999976158142\n",
      "Epoch 71 train loss: 1.5703883171081543 train acc: 0.3075000047683716 test loss: 1.6065287590026855 best test loss: 1.6065287590026855 test acc: 0.20000000298023224\n",
      "Epoch 72 train loss: 1.5678448677062988 train acc: 0.29499998688697815 test loss: 1.607498288154602 best test loss: 1.6065287590026855 test acc: 0.1599999964237213\n",
      "Epoch 73 train loss: 1.564283847808838 train acc: 0.29249998927116394 test loss: 1.60626220703125 best test loss: 1.60626220703125 test acc: 0.18000000715255737\n",
      "Epoch 74 train loss: 1.5597398281097412 train acc: 0.27250000834465027 test loss: 1.607922077178955 best test loss: 1.60626220703125 test acc: 0.15000000596046448\n",
      "Epoch 75 train loss: 1.557234764099121 train acc: 0.26750001311302185 test loss: 1.6039400100708008 best test loss: 1.6039400100708008 test acc: 0.1599999964237213\n",
      "Epoch 76 train loss: 1.5550616979599 train acc: 0.27250000834465027 test loss: 1.6040111780166626 best test loss: 1.6039400100708008 test acc: 0.17000000178813934\n",
      "Epoch 77 train loss: 1.551674246788025 train acc: 0.2524999976158142 test loss: 1.6045161485671997 best test loss: 1.6039400100708008 test acc: 0.15000000596046448\n",
      "Epoch 78 train loss: 1.5489020347595215 train acc: 0.23749999701976776 test loss: 1.6036999225616455 best test loss: 1.6036999225616455 test acc: 0.1599999964237213\n",
      "Epoch 79 train loss: 1.5467109680175781 train acc: 0.24500000476837158 test loss: 1.601656198501587 best test loss: 1.601656198501587 test acc: 0.15000000596046448\n",
      "Epoch 80 train loss: 1.5435705184936523 train acc: 0.22499999403953552 test loss: 1.5996202230453491 best test loss: 1.5996202230453491 test acc: 0.1599999964237213\n",
      "Epoch 81 train loss: 1.5392550230026245 train acc: 0.25 test loss: 1.6003504991531372 best test loss: 1.5996202230453491 test acc: 0.1599999964237213\n",
      "Epoch 82 train loss: 1.5370521545410156 train acc: 0.23749999701976776 test loss: 1.5961618423461914 best test loss: 1.5961618423461914 test acc: 0.18000000715255737\n",
      "Epoch 83 train loss: 1.5345771312713623 train acc: 0.2475000023841858 test loss: 1.5944161415100098 best test loss: 1.5944161415100098 test acc: 0.18000000715255737\n",
      "Epoch 84 train loss: 1.5325417518615723 train acc: 0.25 test loss: 1.5915091037750244 best test loss: 1.5915091037750244 test acc: 0.18000000715255737\n",
      "Epoch 85 train loss: 1.5280261039733887 train acc: 0.25 test loss: 1.5874775648117065 best test loss: 1.5874775648117065 test acc: 0.25\n",
      "Epoch 86 train loss: 1.524624228477478 train acc: 0.2824999988079071 test loss: 1.5854452848434448 best test loss: 1.5854452848434448 test acc: 0.2199999988079071\n",
      "Epoch 87 train loss: 1.5193730592727661 train acc: 0.29249998927116394 test loss: 1.5813307762145996 best test loss: 1.5813307762145996 test acc: 0.2199999988079071\n",
      "Epoch 88 train loss: 1.519773006439209 train acc: 0.2849999964237213 test loss: 1.5801669359207153 best test loss: 1.5801669359207153 test acc: 0.23999999463558197\n",
      "Epoch 89 train loss: 1.511603832244873 train acc: 0.3199999928474426 test loss: 1.5765020847320557 best test loss: 1.5765020847320557 test acc: 0.25\n",
      "Epoch 90 train loss: 1.5067434310913086 train acc: 0.33500000834465027 test loss: 1.5684711933135986 best test loss: 1.5684711933135986 test acc: 0.2800000011920929\n",
      "Epoch 91 train loss: 1.5020183324813843 train acc: 0.36000001430511475 test loss: 1.5654067993164062 best test loss: 1.5654067993164062 test acc: 0.2800000011920929\n",
      "Epoch 92 train loss: 1.4979652166366577 train acc: 0.3824999928474426 test loss: 1.5618889331817627 best test loss: 1.5618889331817627 test acc: 0.30000001192092896\n",
      "Epoch 93 train loss: 1.4936288595199585 train acc: 0.4050000011920929 test loss: 1.5545217990875244 best test loss: 1.5545217990875244 test acc: 0.33000001311302185\n",
      "Epoch 94 train loss: 1.487918734550476 train acc: 0.4375 test loss: 1.55144464969635 best test loss: 1.55144464969635 test acc: 0.3499999940395355\n",
      "Epoch 95 train loss: 1.4794387817382812 train acc: 0.45500001311302185 test loss: 1.5441184043884277 best test loss: 1.5441184043884277 test acc: 0.3499999940395355\n",
      "Epoch 96 train loss: 1.4749352931976318 train acc: 0.4650000035762787 test loss: 1.5402636528015137 best test loss: 1.5402636528015137 test acc: 0.36000001430511475\n",
      "Epoch 97 train loss: 1.4686901569366455 train acc: 0.48500001430511475 test loss: 1.5334514379501343 best test loss: 1.5334514379501343 test acc: 0.36000001430511475\n",
      "Epoch 98 train loss: 1.4628082513809204 train acc: 0.5174999833106995 test loss: 1.5279724597930908 best test loss: 1.5279724597930908 test acc: 0.3799999952316284\n",
      "Epoch 99 train loss: 1.4560619592666626 train acc: 0.5299999713897705 test loss: 1.5196386575698853 best test loss: 1.5196386575698853 test acc: 0.3799999952316284\n",
      "Epoch 100 train loss: 1.4497092962265015 train acc: 0.5299999713897705 test loss: 1.5144445896148682 best test loss: 1.5144445896148682 test acc: 0.38999998569488525\n",
      "Epoch 101 train loss: 1.4409884214401245 train acc: 0.5525000095367432 test loss: 1.5051161050796509 best test loss: 1.5051161050796509 test acc: 0.38999998569488525\n",
      "Epoch 102 train loss: 1.429598331451416 train acc: 0.5475000143051147 test loss: 1.5015325546264648 best test loss: 1.5015325546264648 test acc: 0.4099999964237213\n",
      "Epoch 103 train loss: 1.4233496189117432 train acc: 0.5600000023841858 test loss: 1.4921808242797852 best test loss: 1.4921808242797852 test acc: 0.4000000059604645\n",
      "Epoch 104 train loss: 1.4124058485031128 train acc: 0.5699999928474426 test loss: 1.4780694246292114 best test loss: 1.4780694246292114 test acc: 0.4399999976158142\n",
      "Epoch 105 train loss: 1.4012759923934937 train acc: 0.5724999904632568 test loss: 1.4751951694488525 best test loss: 1.4751951694488525 test acc: 0.44999998807907104\n",
      "Epoch 106 train loss: 1.3923529386520386 train acc: 0.5849999785423279 test loss: 1.4618072509765625 best test loss: 1.4618072509765625 test acc: 0.44999998807907104\n",
      "Epoch 107 train loss: 1.3827221393585205 train acc: 0.5774999856948853 test loss: 1.4513667821884155 best test loss: 1.4513667821884155 test acc: 0.4699999988079071\n",
      "Epoch 108 train loss: 1.3716228008270264 train acc: 0.5975000262260437 test loss: 1.4440505504608154 best test loss: 1.4440505504608154 test acc: 0.46000000834465027\n",
      "Epoch 109 train loss: 1.3601443767547607 train acc: 0.6000000238418579 test loss: 1.4351451396942139 best test loss: 1.4351451396942139 test acc: 0.47999998927116394\n",
      "Epoch 110 train loss: 1.3464771509170532 train acc: 0.6175000071525574 test loss: 1.426481008529663 best test loss: 1.426481008529663 test acc: 0.5\n",
      "Epoch 111 train loss: 1.341100811958313 train acc: 0.6100000143051147 test loss: 1.4121041297912598 best test loss: 1.4121041297912598 test acc: 0.49000000953674316\n",
      "Epoch 112 train loss: 1.333601951599121 train acc: 0.6200000047683716 test loss: 1.409918189048767 best test loss: 1.409918189048767 test acc: 0.49000000953674316\n",
      "Epoch 113 train loss: 1.3221755027770996 train acc: 0.6299999952316284 test loss: 1.3998032808303833 best test loss: 1.3998032808303833 test acc: 0.47999998927116394\n",
      "Epoch 114 train loss: 1.315824031829834 train acc: 0.625 test loss: 1.3920350074768066 best test loss: 1.3920350074768066 test acc: 0.5\n",
      "Epoch 115 train loss: 1.3076024055480957 train acc: 0.6150000095367432 test loss: 1.3905954360961914 best test loss: 1.3905954360961914 test acc: 0.5\n",
      "Epoch 116 train loss: 1.2970154285430908 train acc: 0.6349999904632568 test loss: 1.3871862888336182 best test loss: 1.3871862888336182 test acc: 0.5\n",
      "Epoch 117 train loss: 1.293957233428955 train acc: 0.6299999952316284 test loss: 1.3855772018432617 best test loss: 1.3855772018432617 test acc: 0.5\n",
      "Epoch 118 train loss: 1.290195107460022 train acc: 0.6274999976158142 test loss: 1.3774226903915405 best test loss: 1.3774226903915405 test acc: 0.5\n",
      "Epoch 119 train loss: 1.2826679944992065 train acc: 0.6324999928474426 test loss: 1.377151370048523 best test loss: 1.377151370048523 test acc: 0.5\n",
      "Epoch 120 train loss: 1.2815732955932617 train acc: 0.6349999904632568 test loss: 1.370033860206604 best test loss: 1.370033860206604 test acc: 0.5199999809265137\n",
      "Epoch 121 train loss: 1.2763386964797974 train acc: 0.6424999833106995 test loss: 1.365638256072998 best test loss: 1.365638256072998 test acc: 0.5299999713897705\n",
      "Epoch 122 train loss: 1.2705578804016113 train acc: 0.6399999856948853 test loss: 1.3654143810272217 best test loss: 1.3654143810272217 test acc: 0.5\n",
      "Epoch 123 train loss: 1.2701818943023682 train acc: 0.6349999904632568 test loss: 1.3641401529312134 best test loss: 1.3641401529312134 test acc: 0.5199999809265137\n",
      "Epoch 124 train loss: 1.2633755207061768 train acc: 0.6399999856948853 test loss: 1.3628827333450317 best test loss: 1.3628827333450317 test acc: 0.5299999713897705\n",
      "Epoch 125 train loss: 1.2625523805618286 train acc: 0.6424999833106995 test loss: 1.3597465753555298 best test loss: 1.3597465753555298 test acc: 0.5400000214576721\n",
      "Epoch 126 train loss: 1.2556896209716797 train acc: 0.6499999761581421 test loss: 1.3584809303283691 best test loss: 1.3584809303283691 test acc: 0.5299999713897705\n",
      "Epoch 127 train loss: 1.2595603466033936 train acc: 0.6349999904632568 test loss: 1.360275387763977 best test loss: 1.3584809303283691 test acc: 0.5299999713897705\n",
      "Epoch 128 train loss: 1.251844048500061 train acc: 0.6424999833106995 test loss: 1.3545726537704468 best test loss: 1.3545726537704468 test acc: 0.5400000214576721\n",
      "Epoch 129 train loss: 1.2509537935256958 train acc: 0.6575000286102295 test loss: 1.3472479581832886 best test loss: 1.3472479581832886 test acc: 0.5400000214576721\n",
      "Epoch 130 train loss: 1.2477993965148926 train acc: 0.6499999761581421 test loss: 1.3458553552627563 best test loss: 1.3458553552627563 test acc: 0.550000011920929\n",
      "Epoch 131 train loss: 1.2472074031829834 train acc: 0.6524999737739563 test loss: 1.3473756313323975 best test loss: 1.3458553552627563 test acc: 0.5400000214576721\n",
      "Epoch 132 train loss: 1.244107961654663 train acc: 0.6575000286102295 test loss: 1.3452668190002441 best test loss: 1.3452668190002441 test acc: 0.5299999713897705\n",
      "Epoch 133 train loss: 1.2414742708206177 train acc: 0.6625000238418579 test loss: 1.3411794900894165 best test loss: 1.3411794900894165 test acc: 0.5400000214576721\n",
      "Epoch 134 train loss: 1.2410138845443726 train acc: 0.6524999737739563 test loss: 1.343113899230957 best test loss: 1.3411794900894165 test acc: 0.5299999713897705\n",
      "Epoch 135 train loss: 1.236724853515625 train acc: 0.6675000190734863 test loss: 1.3404011726379395 best test loss: 1.3404011726379395 test acc: 0.5400000214576721\n",
      "Epoch 136 train loss: 1.2333372831344604 train acc: 0.6675000190734863 test loss: 1.3363643884658813 best test loss: 1.3363643884658813 test acc: 0.5400000214576721\n",
      "Epoch 137 train loss: 1.2289820909500122 train acc: 0.6675000190734863 test loss: 1.3331531286239624 best test loss: 1.3331531286239624 test acc: 0.550000011920929\n",
      "Epoch 138 train loss: 1.2263394594192505 train acc: 0.6775000095367432 test loss: 1.331648349761963 best test loss: 1.331648349761963 test acc: 0.5799999833106995\n",
      "Epoch 139 train loss: 1.2271467447280884 train acc: 0.675000011920929 test loss: 1.3295812606811523 best test loss: 1.3295812606811523 test acc: 0.5699999928474426\n",
      "Epoch 140 train loss: 1.2215936183929443 train acc: 0.6899999976158142 test loss: 1.3249706029891968 best test loss: 1.3249706029891968 test acc: 0.5799999833106995\n",
      "Epoch 141 train loss: 1.2196658849716187 train acc: 0.6924999952316284 test loss: 1.32316255569458 best test loss: 1.32316255569458 test acc: 0.5899999737739563\n",
      "Epoch 142 train loss: 1.2168927192687988 train acc: 0.6924999952316284 test loss: 1.321327805519104 best test loss: 1.321327805519104 test acc: 0.6000000238418579\n",
      "Epoch 143 train loss: 1.2171297073364258 train acc: 0.6974999904632568 test loss: 1.3156788349151611 best test loss: 1.3156788349151611 test acc: 0.6100000143051147\n",
      "Epoch 144 train loss: 1.2092441320419312 train acc: 0.7149999737739563 test loss: 1.3124611377716064 best test loss: 1.3124611377716064 test acc: 0.5899999737739563\n",
      "Epoch 145 train loss: 1.2083371877670288 train acc: 0.7250000238418579 test loss: 1.3058907985687256 best test loss: 1.3058907985687256 test acc: 0.6100000143051147\n",
      "Epoch 146 train loss: 1.2049493789672852 train acc: 0.7225000262260437 test loss: 1.3048781156539917 best test loss: 1.3048781156539917 test acc: 0.6100000143051147\n",
      "Epoch 147 train loss: 1.1993966102600098 train acc: 0.7300000190734863 test loss: 1.299345850944519 best test loss: 1.299345850944519 test acc: 0.6299999952316284\n",
      "Epoch 148 train loss: 1.1959278583526611 train acc: 0.7325000166893005 test loss: 1.2990820407867432 best test loss: 1.2990820407867432 test acc: 0.6399999856948853\n",
      "Epoch 149 train loss: 1.1974292993545532 train acc: 0.7300000190734863 test loss: 1.2912955284118652 best test loss: 1.2912955284118652 test acc: 0.6399999856948853\n",
      "Epoch 150 train loss: 1.1927173137664795 train acc: 0.737500011920929 test loss: 1.2838255167007446 best test loss: 1.2838255167007446 test acc: 0.6600000262260437\n",
      "Epoch 151 train loss: 1.1866273880004883 train acc: 0.737500011920929 test loss: 1.281520128250122 best test loss: 1.281520128250122 test acc: 0.6700000166893005\n",
      "Epoch 152 train loss: 1.1853299140930176 train acc: 0.7425000071525574 test loss: 1.273861289024353 best test loss: 1.273861289024353 test acc: 0.6600000262260437\n",
      "Epoch 153 train loss: 1.1821322441101074 train acc: 0.7400000095367432 test loss: 1.2728931903839111 best test loss: 1.2728931903839111 test acc: 0.6600000262260437\n",
      "Epoch 154 train loss: 1.175614356994629 train acc: 0.7400000095367432 test loss: 1.2666221857070923 best test loss: 1.2666221857070923 test acc: 0.6800000071525574\n",
      "Epoch 155 train loss: 1.1728909015655518 train acc: 0.7450000047683716 test loss: 1.2661265134811401 best test loss: 1.2661265134811401 test acc: 0.6700000166893005\n",
      "Epoch 156 train loss: 1.16933012008667 train acc: 0.7524999976158142 test loss: 1.2596632242202759 best test loss: 1.2596632242202759 test acc: 0.6700000166893005\n",
      "Epoch 157 train loss: 1.1678463220596313 train acc: 0.7475000023841858 test loss: 1.2520415782928467 best test loss: 1.2520415782928467 test acc: 0.6899999976158142\n",
      "Epoch 158 train loss: 1.1606773138046265 train acc: 0.7549999952316284 test loss: 1.2484437227249146 best test loss: 1.2484437227249146 test acc: 0.6800000071525574\n",
      "Epoch 159 train loss: 1.1608445644378662 train acc: 0.7574999928474426 test loss: 1.2397249937057495 best test loss: 1.2397249937057495 test acc: 0.7099999785423279\n",
      "Epoch 160 train loss: 1.1550261974334717 train acc: 0.7549999952316284 test loss: 1.2407888174057007 best test loss: 1.2397249937057495 test acc: 0.699999988079071\n",
      "Epoch 161 train loss: 1.1527976989746094 train acc: 0.762499988079071 test loss: 1.2385729551315308 best test loss: 1.2385729551315308 test acc: 0.6899999976158142\n",
      "Epoch 162 train loss: 1.1527653932571411 train acc: 0.7599999904632568 test loss: 1.2331254482269287 best test loss: 1.2331254482269287 test acc: 0.7099999785423279\n",
      "Epoch 163 train loss: 1.1475783586502075 train acc: 0.7599999904632568 test loss: 1.2257980108261108 best test loss: 1.2257980108261108 test acc: 0.7099999785423279\n",
      "Epoch 164 train loss: 1.1447501182556152 train acc: 0.7699999809265137 test loss: 1.223659634590149 best test loss: 1.223659634590149 test acc: 0.699999988079071\n",
      "Epoch 165 train loss: 1.1418887376785278 train acc: 0.7649999856948853 test loss: 1.222124457359314 best test loss: 1.222124457359314 test acc: 0.6899999976158142\n",
      "Epoch 166 train loss: 1.138755202293396 train acc: 0.7674999833106995 test loss: 1.2186359167099 best test loss: 1.2186359167099 test acc: 0.6800000071525574\n",
      "Epoch 167 train loss: 1.1323838233947754 train acc: 0.7749999761581421 test loss: 1.212855577468872 best test loss: 1.212855577468872 test acc: 0.6800000071525574\n",
      "Epoch 168 train loss: 1.1342145204544067 train acc: 0.7749999761581421 test loss: 1.2142961025238037 best test loss: 1.212855577468872 test acc: 0.6800000071525574\n",
      "Epoch 169 train loss: 1.1329237222671509 train acc: 0.7724999785423279 test loss: 1.2075945138931274 best test loss: 1.2075945138931274 test acc: 0.699999988079071\n",
      "Epoch 170 train loss: 1.1271429061889648 train acc: 0.7774999737739563 test loss: 1.209328055381775 best test loss: 1.2075945138931274 test acc: 0.699999988079071\n",
      "Epoch 171 train loss: 1.128082036972046 train acc: 0.7724999785423279 test loss: 1.2061941623687744 best test loss: 1.2061941623687744 test acc: 0.6899999976158142\n",
      "Epoch 172 train loss: 1.1252105236053467 train acc: 0.7825000286102295 test loss: 1.2041192054748535 best test loss: 1.2041192054748535 test acc: 0.7300000190734863\n",
      "Epoch 173 train loss: 1.1203938722610474 train acc: 0.7850000262260437 test loss: 1.2043474912643433 best test loss: 1.2041192054748535 test acc: 0.7099999785423279\n",
      "Epoch 174 train loss: 1.1215299367904663 train acc: 0.7875000238418579 test loss: 1.1978405714035034 best test loss: 1.1978405714035034 test acc: 0.7200000286102295\n",
      "Epoch 175 train loss: 1.1165721416473389 train acc: 0.7900000214576721 test loss: 1.1916342973709106 best test loss: 1.1916342973709106 test acc: 0.7400000095367432\n",
      "Epoch 176 train loss: 1.1098042726516724 train acc: 0.7975000143051147 test loss: 1.1941735744476318 best test loss: 1.1916342973709106 test acc: 0.7400000095367432\n",
      "Epoch 177 train loss: 1.1110546588897705 train acc: 0.800000011920929 test loss: 1.193518877029419 best test loss: 1.1916342973709106 test acc: 0.7400000095367432\n",
      "Epoch 178 train loss: 1.109920620918274 train acc: 0.8025000095367432 test loss: 1.1839698553085327 best test loss: 1.1839698553085327 test acc: 0.75\n",
      "Epoch 179 train loss: 1.1050286293029785 train acc: 0.8050000071525574 test loss: 1.1898531913757324 best test loss: 1.1839698553085327 test acc: 0.7400000095367432\n",
      "Epoch 180 train loss: 1.103885531425476 train acc: 0.8050000071525574 test loss: 1.1880254745483398 best test loss: 1.1839698553085327 test acc: 0.7300000190734863\n",
      "Epoch 181 train loss: 1.100947618484497 train acc: 0.8050000071525574 test loss: 1.1834510564804077 best test loss: 1.1834510564804077 test acc: 0.7200000286102295\n",
      "Epoch 182 train loss: 1.0979024171829224 train acc: 0.8199999928474426 test loss: 1.1825636625289917 best test loss: 1.1825636625289917 test acc: 0.7400000095367432\n",
      "Epoch 183 train loss: 1.0941842794418335 train acc: 0.824999988079071 test loss: 1.1825677156448364 best test loss: 1.1825636625289917 test acc: 0.7400000095367432\n",
      "Epoch 184 train loss: 1.0926258563995361 train acc: 0.8274999856948853 test loss: 1.1737476587295532 best test loss: 1.1737476587295532 test acc: 0.7400000095367432\n",
      "Epoch 185 train loss: 1.0896552801132202 train acc: 0.8349999785423279 test loss: 1.173689603805542 best test loss: 1.173689603805542 test acc: 0.7599999904632568\n",
      "Epoch 186 train loss: 1.0838024616241455 train acc: 0.8475000262260437 test loss: 1.1734257936477661 best test loss: 1.1734257936477661 test acc: 0.7699999809265137\n",
      "Epoch 187 train loss: 1.0800201892852783 train acc: 0.8575000166893005 test loss: 1.1668670177459717 best test loss: 1.1668670177459717 test acc: 0.7699999809265137\n",
      "Epoch 188 train loss: 1.0787972211837769 train acc: 0.8525000214576721 test loss: 1.1702388525009155 best test loss: 1.1668670177459717 test acc: 0.7799999713897705\n",
      "Epoch 189 train loss: 1.0780587196350098 train acc: 0.8575000166893005 test loss: 1.1610201597213745 best test loss: 1.1610201597213745 test acc: 0.7799999713897705\n",
      "Epoch 190 train loss: 1.0696170330047607 train acc: 0.8700000047683716 test loss: 1.1585856676101685 best test loss: 1.1585856676101685 test acc: 0.800000011920929\n",
      "Epoch 191 train loss: 1.0640532970428467 train acc: 0.8824999928474426 test loss: 1.153576374053955 best test loss: 1.153576374053955 test acc: 0.800000011920929\n",
      "Epoch 192 train loss: 1.0628618001937866 train acc: 0.8849999904632568 test loss: 1.1492522954940796 best test loss: 1.1492522954940796 test acc: 0.800000011920929\n",
      "Epoch 193 train loss: 1.061992883682251 train acc: 0.8799999952316284 test loss: 1.1494413614273071 best test loss: 1.1492522954940796 test acc: 0.7900000214576721\n",
      "Epoch 194 train loss: 1.0541054010391235 train acc: 0.8899999856948853 test loss: 1.1454139947891235 best test loss: 1.1454139947891235 test acc: 0.800000011920929\n",
      "Epoch 195 train loss: 1.0538631677627563 train acc: 0.8899999856948853 test loss: 1.136367678642273 best test loss: 1.136367678642273 test acc: 0.8100000023841858\n",
      "Epoch 196 train loss: 1.0492186546325684 train acc: 0.8924999833106995 test loss: 1.1297941207885742 best test loss: 1.1297941207885742 test acc: 0.8100000023841858\n",
      "Epoch 197 train loss: 1.0451244115829468 train acc: 0.8949999809265137 test loss: 1.1294232606887817 best test loss: 1.1294232606887817 test acc: 0.8199999928474426\n",
      "Epoch 198 train loss: 1.0422383546829224 train acc: 0.8999999761581421 test loss: 1.1244581937789917 best test loss: 1.1244581937789917 test acc: 0.8100000023841858\n",
      "Epoch 199 train loss: 1.0394514799118042 train acc: 0.9024999737739563 test loss: 1.1234077215194702 best test loss: 1.1234077215194702 test acc: 0.8100000023841858\n",
      "Epoch 200 train loss: 1.034199833869934 train acc: 0.9049999713897705 test loss: 1.11686110496521 best test loss: 1.11686110496521 test acc: 0.8199999928474426\n",
      "Epoch 201 train loss: 1.03200101852417 train acc: 0.8999999761581421 test loss: 1.1163671016693115 best test loss: 1.1163671016693115 test acc: 0.8100000023841858\n",
      "Epoch 202 train loss: 1.0271916389465332 train acc: 0.9075000286102295 test loss: 1.1121150255203247 best test loss: 1.1121150255203247 test acc: 0.7900000214576721\n",
      "Epoch 203 train loss: 1.0249779224395752 train acc: 0.9075000286102295 test loss: 1.1129573583602905 best test loss: 1.1121150255203247 test acc: 0.7900000214576721\n",
      "Epoch 204 train loss: 1.0210378170013428 train acc: 0.9024999737739563 test loss: 1.1069042682647705 best test loss: 1.1069042682647705 test acc: 0.8299999833106995\n",
      "Epoch 205 train loss: 1.0186238288879395 train acc: 0.9100000262260437 test loss: 1.0972439050674438 best test loss: 1.0972439050674438 test acc: 0.8399999737739563\n",
      "Epoch 206 train loss: 1.0162005424499512 train acc: 0.9200000166893005 test loss: 1.0987141132354736 best test loss: 1.0972439050674438 test acc: 0.8600000143051147\n",
      "Epoch 207 train loss: 1.0132818222045898 train acc: 0.9150000214576721 test loss: 1.0896140336990356 best test loss: 1.0896140336990356 test acc: 0.8500000238418579\n",
      "Epoch 208 train loss: 1.0085787773132324 train acc: 0.9225000143051147 test loss: 1.0972802639007568 best test loss: 1.0896140336990356 test acc: 0.8199999928474426\n",
      "Epoch 209 train loss: 1.0050681829452515 train acc: 0.925000011920929 test loss: 1.0834611654281616 best test loss: 1.0834611654281616 test acc: 0.8500000238418579\n",
      "Epoch 210 train loss: 1.006690502166748 train acc: 0.925000011920929 test loss: 1.0886573791503906 best test loss: 1.0834611654281616 test acc: 0.8399999737739563\n",
      "Epoch 211 train loss: 1.0021612644195557 train acc: 0.9275000095367432 test loss: 1.080182671546936 best test loss: 1.080182671546936 test acc: 0.8600000143051147\n",
      "Epoch 212 train loss: 0.9974450469017029 train acc: 0.9275000095367432 test loss: 1.0840489864349365 best test loss: 1.080182671546936 test acc: 0.8399999737739563\n",
      "Epoch 213 train loss: 0.9980910420417786 train acc: 0.9300000071525574 test loss: 1.0772755146026611 best test loss: 1.0772755146026611 test acc: 0.8500000238418579\n",
      "Epoch 214 train loss: 0.9959731101989746 train acc: 0.9225000143051147 test loss: 1.0845624208450317 best test loss: 1.0772755146026611 test acc: 0.8399999737739563\n",
      "Epoch 215 train loss: 0.99151611328125 train acc: 0.9300000071525574 test loss: 1.077698826789856 best test loss: 1.0772755146026611 test acc: 0.8399999737739563\n",
      "Epoch 216 train loss: 0.9887662529945374 train acc: 0.9325000047683716 test loss: 1.0760952234268188 best test loss: 1.0760952234268188 test acc: 0.8399999737739563\n",
      "Epoch 217 train loss: 0.9885684251785278 train acc: 0.9300000071525574 test loss: 1.0712881088256836 best test loss: 1.0712881088256836 test acc: 0.8399999737739563\n",
      "Epoch 218 train loss: 0.9817171692848206 train acc: 0.9449999928474426 test loss: 1.0775914192199707 best test loss: 1.0712881088256836 test acc: 0.8399999737739563\n",
      "Epoch 219 train loss: 0.9833739995956421 train acc: 0.9375 test loss: 1.0642449855804443 best test loss: 1.0642449855804443 test acc: 0.8500000238418579\n",
      "Epoch 220 train loss: 0.9815537333488464 train acc: 0.9424999952316284 test loss: 1.0617674589157104 best test loss: 1.0617674589157104 test acc: 0.8600000143051147\n",
      "Epoch 221 train loss: 0.9797943234443665 train acc: 0.9424999952316284 test loss: 1.0759292840957642 best test loss: 1.0617674589157104 test acc: 0.8199999928474426\n",
      "Epoch 222 train loss: 0.9770107865333557 train acc: 0.9424999952316284 test loss: 1.0648772716522217 best test loss: 1.0617674589157104 test acc: 0.8600000143051147\n",
      "Epoch 223 train loss: 0.9767401218414307 train acc: 0.9474999904632568 test loss: 1.0651518106460571 best test loss: 1.0617674589157104 test acc: 0.8500000238418579\n",
      "Epoch 224 train loss: 0.972402036190033 train acc: 0.9449999928474426 test loss: 1.0663734674453735 best test loss: 1.0617674589157104 test acc: 0.8399999737739563\n",
      "Epoch 225 train loss: 0.9714717864990234 train acc: 0.9449999928474426 test loss: 1.061504602432251 best test loss: 1.061504602432251 test acc: 0.8600000143051147\n",
      "Epoch 226 train loss: 0.9747517108917236 train acc: 0.9449999928474426 test loss: 1.0611469745635986 best test loss: 1.0611469745635986 test acc: 0.8700000047683716\n",
      "Epoch 227 train loss: 0.9714491367340088 train acc: 0.9449999928474426 test loss: 1.0566550493240356 best test loss: 1.0566550493240356 test acc: 0.8500000238418579\n",
      "Epoch 228 train loss: 0.9670153260231018 train acc: 0.949999988079071 test loss: 1.0532574653625488 best test loss: 1.0532574653625488 test acc: 0.8600000143051147\n",
      "Epoch 229 train loss: 0.9680073261260986 train acc: 0.949999988079071 test loss: 1.0600194931030273 best test loss: 1.0532574653625488 test acc: 0.8500000238418579\n",
      "Epoch 230 train loss: 0.9663451313972473 train acc: 0.949999988079071 test loss: 1.057735562324524 best test loss: 1.0532574653625488 test acc: 0.8299999833106995\n",
      "Epoch 231 train loss: 0.9662789106369019 train acc: 0.949999988079071 test loss: 1.0538920164108276 best test loss: 1.0532574653625488 test acc: 0.8700000047683716\n",
      "Epoch 232 train loss: 0.9656184911727905 train acc: 0.949999988079071 test loss: 1.0582116842269897 best test loss: 1.0532574653625488 test acc: 0.8500000238418579\n",
      "Epoch 233 train loss: 0.9649571776390076 train acc: 0.9474999904632568 test loss: 1.051073431968689 best test loss: 1.051073431968689 test acc: 0.8799999952316284\n",
      "Epoch 234 train loss: 0.9640809893608093 train acc: 0.9474999904632568 test loss: 1.0523667335510254 best test loss: 1.051073431968689 test acc: 0.8500000238418579\n",
      "Epoch 235 train loss: 0.9621988534927368 train acc: 0.949999988079071 test loss: 1.0587161779403687 best test loss: 1.051073431968689 test acc: 0.8500000238418579\n",
      "Epoch 236 train loss: 0.9608135223388672 train acc: 0.949999988079071 test loss: 1.0486963987350464 best test loss: 1.0486963987350464 test acc: 0.8799999952316284\n",
      "Epoch 237 train loss: 0.9604198336601257 train acc: 0.949999988079071 test loss: 1.0615928173065186 best test loss: 1.0486963987350464 test acc: 0.8299999833106995\n",
      "Epoch 238 train loss: 0.9609903693199158 train acc: 0.949999988079071 test loss: 1.051068902015686 best test loss: 1.0486963987350464 test acc: 0.8700000047683716\n",
      "Epoch 239 train loss: 0.9602119326591492 train acc: 0.9524999856948853 test loss: 1.0458928346633911 best test loss: 1.0458928346633911 test acc: 0.8600000143051147\n",
      "Epoch 240 train loss: 0.9601565003395081 train acc: 0.9524999856948853 test loss: 1.0457650423049927 best test loss: 1.0457650423049927 test acc: 0.8600000143051147\n",
      "Epoch 241 train loss: 0.9585059285163879 train acc: 0.9524999856948853 test loss: 1.0515871047973633 best test loss: 1.0457650423049927 test acc: 0.8600000143051147\n",
      "Epoch 242 train loss: 0.9607792496681213 train acc: 0.9524999856948853 test loss: 1.0423961877822876 best test loss: 1.0423961877822876 test acc: 0.8700000047683716\n",
      "Epoch 243 train loss: 0.9592498540878296 train acc: 0.949999988079071 test loss: 1.046951174736023 best test loss: 1.0423961877822876 test acc: 0.8700000047683716\n",
      "Epoch 244 train loss: 0.9573104977607727 train acc: 0.9549999833106995 test loss: 1.0425727367401123 best test loss: 1.0423961877822876 test acc: 0.8700000047683716\n",
      "Epoch 245 train loss: 0.9594146013259888 train acc: 0.9549999833106995 test loss: 1.0513883829116821 best test loss: 1.0423961877822876 test acc: 0.8600000143051147\n",
      "Epoch 246 train loss: 0.9562641382217407 train acc: 0.9549999833106995 test loss: 1.0542901754379272 best test loss: 1.0423961877822876 test acc: 0.8600000143051147\n",
      "Epoch 247 train loss: 0.9565414190292358 train acc: 0.9549999833106995 test loss: 1.0496493577957153 best test loss: 1.0423961877822876 test acc: 0.8500000238418579\n",
      "Epoch 248 train loss: 0.9566712975502014 train acc: 0.9549999833106995 test loss: 1.03923761844635 best test loss: 1.03923761844635 test acc: 0.8799999952316284\n",
      "Epoch 249 train loss: 0.9569045901298523 train acc: 0.9549999833106995 test loss: 1.0492159128189087 best test loss: 1.03923761844635 test acc: 0.8600000143051147\n",
      "Epoch 250 train loss: 0.9553037881851196 train acc: 0.9549999833106995 test loss: 1.0415900945663452 best test loss: 1.03923761844635 test acc: 0.8799999952316284\n",
      "Epoch 251 train loss: 0.9555713534355164 train acc: 0.9549999833106995 test loss: 1.04353666305542 best test loss: 1.03923761844635 test acc: 0.8799999952316284\n",
      "Epoch 252 train loss: 0.9551672339439392 train acc: 0.9549999833106995 test loss: 1.04213285446167 best test loss: 1.03923761844635 test acc: 0.8700000047683716\n",
      "Epoch 253 train loss: 0.9541601538658142 train acc: 0.9549999833106995 test loss: 1.0463579893112183 best test loss: 1.03923761844635 test acc: 0.8700000047683716\n",
      "Epoch 254 train loss: 0.9558958411216736 train acc: 0.9549999833106995 test loss: 1.0376296043395996 best test loss: 1.0376296043395996 test acc: 0.8799999952316284\n",
      "Epoch 255 train loss: 0.9546951055526733 train acc: 0.9549999833106995 test loss: 1.0515120029449463 best test loss: 1.0376296043395996 test acc: 0.8500000238418579\n",
      "Epoch 256 train loss: 0.9534928798675537 train acc: 0.9549999833106995 test loss: 1.0431734323501587 best test loss: 1.0376296043395996 test acc: 0.8700000047683716\n",
      "Epoch 257 train loss: 0.9539587497711182 train acc: 0.9549999833106995 test loss: 1.0420775413513184 best test loss: 1.0376296043395996 test acc: 0.8700000047683716\n",
      "Epoch 258 train loss: 0.9545229077339172 train acc: 0.9549999833106995 test loss: 1.0467556715011597 best test loss: 1.0376296043395996 test acc: 0.8600000143051147\n",
      "Epoch 259 train loss: 0.9529886841773987 train acc: 0.9549999833106995 test loss: 1.0410481691360474 best test loss: 1.0376296043395996 test acc: 0.8600000143051147\n",
      "Epoch 260 train loss: 0.9545795321464539 train acc: 0.9549999833106995 test loss: 1.0446052551269531 best test loss: 1.0376296043395996 test acc: 0.8600000143051147\n",
      "Epoch 261 train loss: 0.9536170363426208 train acc: 0.9549999833106995 test loss: 1.0362892150878906 best test loss: 1.0362892150878906 test acc: 0.8799999952316284\n",
      "Epoch 262 train loss: 0.9532520771026611 train acc: 0.9549999833106995 test loss: 1.0455577373504639 best test loss: 1.0362892150878906 test acc: 0.8600000143051147\n",
      "Epoch 263 train loss: 0.9526528716087341 train acc: 0.9549999833106995 test loss: 1.0477155447006226 best test loss: 1.0362892150878906 test acc: 0.8600000143051147\n",
      "Epoch 264 train loss: 0.9527269601821899 train acc: 0.9549999833106995 test loss: 1.0405298471450806 best test loss: 1.0362892150878906 test acc: 0.8700000047683716\n",
      "Epoch 265 train loss: 0.9529313445091248 train acc: 0.9549999833106995 test loss: 1.0378109216690063 best test loss: 1.0362892150878906 test acc: 0.8799999952316284\n",
      "Epoch 266 train loss: 0.9524863958358765 train acc: 0.9549999833106995 test loss: 1.0451470613479614 best test loss: 1.0362892150878906 test acc: 0.8700000047683716\n",
      "Epoch 267 train loss: 0.9527067542076111 train acc: 0.9549999833106995 test loss: 1.0514798164367676 best test loss: 1.0362892150878906 test acc: 0.8500000238418579\n",
      "Epoch 268 train loss: 0.9547002911567688 train acc: 0.9524999856948853 test loss: 1.0338118076324463 best test loss: 1.0338118076324463 test acc: 0.8799999952316284\n",
      "Epoch 269 train loss: 0.9518074989318848 train acc: 0.9549999833106995 test loss: 1.0448726415634155 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 270 train loss: 0.9512193202972412 train acc: 0.9549999833106995 test loss: 1.0366504192352295 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 271 train loss: 0.9518895745277405 train acc: 0.9549999833106995 test loss: 1.0383620262145996 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 272 train loss: 0.9514399766921997 train acc: 0.9549999833106995 test loss: 1.0561470985412598 best test loss: 1.0338118076324463 test acc: 0.8399999737739563\n",
      "Epoch 273 train loss: 0.9511677026748657 train acc: 0.9549999833106995 test loss: 1.047580599784851 best test loss: 1.0338118076324463 test acc: 0.8500000238418579\n",
      "Epoch 274 train loss: 0.9520102143287659 train acc: 0.9549999833106995 test loss: 1.0435824394226074 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 275 train loss: 0.9512162804603577 train acc: 0.9549999833106995 test loss: 1.0565539598464966 best test loss: 1.0338118076324463 test acc: 0.8399999737739563\n",
      "Epoch 276 train loss: 0.9525167942047119 train acc: 0.9524999856948853 test loss: 1.036650538444519 best test loss: 1.0338118076324463 test acc: 0.8799999952316284\n",
      "Epoch 277 train loss: 0.9513599276542664 train acc: 0.9549999833106995 test loss: 1.0376389026641846 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 278 train loss: 0.9509937763214111 train acc: 0.9549999833106995 test loss: 1.0382000207901 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 279 train loss: 0.9508367776870728 train acc: 0.9549999833106995 test loss: 1.0379672050476074 best test loss: 1.0338118076324463 test acc: 0.8700000047683716\n",
      "Epoch 280 train loss: 0.9520977735519409 train acc: 0.9549999833106995 test loss: 1.050538182258606 best test loss: 1.0338118076324463 test acc: 0.8600000143051147\n",
      "Epoch 281 train loss: 0.9504454135894775 train acc: 0.9549999833106995 test loss: 1.0333893299102783 best test loss: 1.0333893299102783 test acc: 0.8799999952316284\n",
      "Epoch 282 train loss: 0.9512379169464111 train acc: 0.9549999833106995 test loss: 1.047712802886963 best test loss: 1.0333893299102783 test acc: 0.8500000238418579\n",
      "Epoch 283 train loss: 0.9512925744056702 train acc: 0.9549999833106995 test loss: 1.0393813848495483 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 284 train loss: 0.950700581073761 train acc: 0.9549999833106995 test loss: 1.045331597328186 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 285 train loss: 0.9512247443199158 train acc: 0.9549999833106995 test loss: 1.0520579814910889 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 286 train loss: 0.9500483870506287 train acc: 0.9549999833106995 test loss: 1.040399432182312 best test loss: 1.0333893299102783 test acc: 0.8899999856948853\n",
      "Epoch 287 train loss: 0.9503355622291565 train acc: 0.9549999833106995 test loss: 1.0409626960754395 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 288 train loss: 0.950359046459198 train acc: 0.9549999833106995 test loss: 1.04432213306427 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 289 train loss: 0.9499377608299255 train acc: 0.9549999833106995 test loss: 1.038290023803711 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 290 train loss: 0.9498858451843262 train acc: 0.9549999833106995 test loss: 1.0476243495941162 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 291 train loss: 0.9492166638374329 train acc: 0.9574999809265137 test loss: 1.0488872528076172 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 292 train loss: 0.9494079351425171 train acc: 0.9549999833106995 test loss: 1.0454360246658325 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 293 train loss: 0.9493753910064697 train acc: 0.9574999809265137 test loss: 1.0391168594360352 best test loss: 1.0333893299102783 test acc: 0.8799999952316284\n",
      "Epoch 294 train loss: 0.9494163990020752 train acc: 0.9574999809265137 test loss: 1.0473705530166626 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 295 train loss: 0.9500203132629395 train acc: 0.9549999833106995 test loss: 1.0436538457870483 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 296 train loss: 0.9488125443458557 train acc: 0.9574999809265137 test loss: 1.0455422401428223 best test loss: 1.0333893299102783 test acc: 0.8500000238418579\n",
      "Epoch 297 train loss: 0.9485841393470764 train acc: 0.9574999809265137 test loss: 1.044925570487976 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 298 train loss: 0.9483776688575745 train acc: 0.9574999809265137 test loss: 1.0416408777236938 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 299 train loss: 0.9490182995796204 train acc: 0.9574999809265137 test loss: 1.0464297533035278 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 300 train loss: 0.9483203887939453 train acc: 0.9574999809265137 test loss: 1.0530034303665161 best test loss: 1.0333893299102783 test acc: 0.8500000238418579\n",
      "Epoch 301 train loss: 0.9481484889984131 train acc: 0.9574999809265137 test loss: 1.0340561866760254 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 302 train loss: 0.948432445526123 train acc: 0.9574999809265137 test loss: 1.0388712882995605 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 303 train loss: 0.9485895037651062 train acc: 0.9574999809265137 test loss: 1.0428483486175537 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 304 train loss: 0.9475146532058716 train acc: 0.9574999809265137 test loss: 1.0434200763702393 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 305 train loss: 0.9472603797912598 train acc: 0.9574999809265137 test loss: 1.0414211750030518 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 306 train loss: 0.9477888345718384 train acc: 0.9574999809265137 test loss: 1.0379620790481567 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 307 train loss: 0.9472386837005615 train acc: 0.9599999785423279 test loss: 1.0432045459747314 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 308 train loss: 0.9469074010848999 train acc: 0.9599999785423279 test loss: 1.0345557928085327 best test loss: 1.0333893299102783 test acc: 0.8899999856948853\n",
      "Epoch 309 train loss: 0.94722580909729 train acc: 0.9574999809265137 test loss: 1.0439276695251465 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 310 train loss: 0.9480751752853394 train acc: 0.9574999809265137 test loss: 1.0472099781036377 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 311 train loss: 0.9478294253349304 train acc: 0.9574999809265137 test loss: 1.0389248132705688 best test loss: 1.0333893299102783 test acc: 0.8799999952316284\n",
      "Epoch 312 train loss: 0.9483289122581482 train acc: 0.9574999809265137 test loss: 1.0359032154083252 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 313 train loss: 0.9472702741622925 train acc: 0.9599999785423279 test loss: 1.0434783697128296 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 314 train loss: 0.9475064277648926 train acc: 0.9599999785423279 test loss: 1.0455292463302612 best test loss: 1.0333893299102783 test acc: 0.8600000143051147\n",
      "Epoch 315 train loss: 0.9458640217781067 train acc: 0.9624999761581421 test loss: 1.0428063869476318 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 316 train loss: 0.9477500915527344 train acc: 0.9599999785423279 test loss: 1.0460213422775269 best test loss: 1.0333893299102783 test acc: 0.8500000238418579\n",
      "Epoch 317 train loss: 0.945921003818512 train acc: 0.9624999761581421 test loss: 1.0433132648468018 best test loss: 1.0333893299102783 test acc: 0.8700000047683716\n",
      "Epoch 318 train loss: 0.9475046396255493 train acc: 0.9574999809265137 test loss: 1.0327513217926025 best test loss: 1.0327513217926025 test acc: 0.8799999952316284\n",
      "Epoch 319 train loss: 0.9470704793930054 train acc: 0.9599999785423279 test loss: 1.0361050367355347 best test loss: 1.0327513217926025 test acc: 0.8700000047683716\n",
      "Epoch 320 train loss: 0.9467208981513977 train acc: 0.9599999785423279 test loss: 1.0446006059646606 best test loss: 1.0327513217926025 test acc: 0.8600000143051147\n",
      "Epoch 321 train loss: 0.9464418888092041 train acc: 0.9599999785423279 test loss: 1.0461325645446777 best test loss: 1.0327513217926025 test acc: 0.8600000143051147\n",
      "Epoch 322 train loss: 0.9475515484809875 train acc: 0.9599999785423279 test loss: 1.0458792448043823 best test loss: 1.0327513217926025 test acc: 0.8500000238418579\n",
      "Epoch 323 train loss: 0.9471847414970398 train acc: 0.9574999809265137 test loss: 1.039077639579773 best test loss: 1.0327513217926025 test acc: 0.8700000047683716\n",
      "Epoch 324 train loss: 0.9448933601379395 train acc: 0.9624999761581421 test loss: 1.033303141593933 best test loss: 1.0327513217926025 test acc: 0.8799999952316284\n",
      "Epoch 325 train loss: 0.946577250957489 train acc: 0.9599999785423279 test loss: 1.032260775566101 best test loss: 1.032260775566101 test acc: 0.8700000047683716\n",
      "Epoch 326 train loss: 0.9451538324356079 train acc: 0.9624999761581421 test loss: 1.053381323814392 best test loss: 1.032260775566101 test acc: 0.8299999833106995\n",
      "Epoch 327 train loss: 0.9467256665229797 train acc: 0.9574999809265137 test loss: 1.0385746955871582 best test loss: 1.032260775566101 test acc: 0.8700000047683716\n",
      "Epoch 328 train loss: 0.9464530348777771 train acc: 0.9599999785423279 test loss: 1.0479576587677002 best test loss: 1.032260775566101 test acc: 0.8600000143051147\n",
      "Epoch 329 train loss: 0.9469397664070129 train acc: 0.9574999809265137 test loss: 1.0452743768692017 best test loss: 1.032260775566101 test acc: 0.8600000143051147\n",
      "Epoch 330 train loss: 0.9459691643714905 train acc: 0.9599999785423279 test loss: 1.0315055847167969 best test loss: 1.0315055847167969 test acc: 0.8799999952316284\n",
      "Epoch 331 train loss: 0.9444206953048706 train acc: 0.9624999761581421 test loss: 1.0484435558319092 best test loss: 1.0315055847167969 test acc: 0.8399999737739563\n",
      "Epoch 332 train loss: 0.9480372071266174 train acc: 0.9599999785423279 test loss: 1.033413290977478 best test loss: 1.0315055847167969 test acc: 0.8799999952316284\n",
      "Epoch 333 train loss: 0.9442616105079651 train acc: 0.9624999761581421 test loss: 1.0424411296844482 best test loss: 1.0315055847167969 test acc: 0.8799999952316284\n",
      "Epoch 334 train loss: 0.947130024433136 train acc: 0.9599999785423279 test loss: 1.0445142984390259 best test loss: 1.0315055847167969 test acc: 0.8600000143051147\n",
      "Epoch 335 train loss: 0.9441108703613281 train acc: 0.9624999761581421 test loss: 1.0477051734924316 best test loss: 1.0315055847167969 test acc: 0.8500000238418579\n",
      "Epoch 336 train loss: 0.9445183277130127 train acc: 0.9624999761581421 test loss: 1.0399452447891235 best test loss: 1.0315055847167969 test acc: 0.8799999952316284\n",
      "Epoch 337 train loss: 0.9442102313041687 train acc: 0.9624999761581421 test loss: 1.041947364807129 best test loss: 1.0315055847167969 test acc: 0.8600000143051147\n",
      "Epoch 338 train loss: 0.9472165107727051 train acc: 0.9574999809265137 test loss: 1.0356298685073853 best test loss: 1.0315055847167969 test acc: 0.8799999952316284\n",
      "Epoch 339 train loss: 0.9438410401344299 train acc: 0.9624999761581421 test loss: 1.0362613201141357 best test loss: 1.0315055847167969 test acc: 0.8700000047683716\n",
      "Epoch 340 train loss: 0.9447912573814392 train acc: 0.9624999761581421 test loss: 1.039010763168335 best test loss: 1.0315055847167969 test acc: 0.8700000047683716\n",
      "Epoch 341 train loss: 0.9456566572189331 train acc: 0.9599999785423279 test loss: 1.0424424409866333 best test loss: 1.0315055847167969 test acc: 0.8600000143051147\n",
      "Epoch 342 train loss: 0.9433577060699463 train acc: 0.9624999761581421 test loss: 1.04023015499115 best test loss: 1.0315055847167969 test acc: 0.8600000143051147\n",
      "Epoch 343 train loss: 0.9437897205352783 train acc: 0.9624999761581421 test loss: 1.0390658378601074 best test loss: 1.0315055847167969 test acc: 0.8600000143051147\n",
      "Epoch 344 train loss: 0.9436206817626953 train acc: 0.9624999761581421 test loss: 1.0374271869659424 best test loss: 1.0315055847167969 test acc: 0.8700000047683716\n",
      "Epoch 345 train loss: 0.9438104033470154 train acc: 0.9624999761581421 test loss: 1.0470530986785889 best test loss: 1.0315055847167969 test acc: 0.8500000238418579\n",
      "Epoch 346 train loss: 0.946613609790802 train acc: 0.9599999785423279 test loss: 1.0425901412963867 best test loss: 1.0315055847167969 test acc: 0.8600000143051147\n",
      "Epoch 347 train loss: 0.9440217018127441 train acc: 0.9624999761581421 test loss: 1.030086874961853 best test loss: 1.030086874961853 test acc: 0.8799999952316284\n",
      "Epoch 348 train loss: 0.9442012310028076 train acc: 0.9599999785423279 test loss: 1.0331722497940063 best test loss: 1.030086874961853 test acc: 0.8799999952316284\n",
      "Epoch 349 train loss: 0.9436808824539185 train acc: 0.9624999761581421 test loss: 1.0371317863464355 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 350 train loss: 0.9432995319366455 train acc: 0.9624999761581421 test loss: 1.0412181615829468 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 351 train loss: 0.943246603012085 train acc: 0.9624999761581421 test loss: 1.047457218170166 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 352 train loss: 0.9437034726142883 train acc: 0.9624999761581421 test loss: 1.042165994644165 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 353 train loss: 0.9453294277191162 train acc: 0.9624999761581421 test loss: 1.0432138442993164 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 354 train loss: 0.9451822638511658 train acc: 0.9599999785423279 test loss: 1.0355658531188965 best test loss: 1.030086874961853 test acc: 0.8799999952316284\n",
      "Epoch 355 train loss: 0.9438596367835999 train acc: 0.9624999761581421 test loss: 1.0322716236114502 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 356 train loss: 0.9440370202064514 train acc: 0.9624999761581421 test loss: 1.0389405488967896 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 357 train loss: 0.9429952502250671 train acc: 0.9624999761581421 test loss: 1.0371497869491577 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 358 train loss: 0.9426515102386475 train acc: 0.9624999761581421 test loss: 1.0378813743591309 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 359 train loss: 0.9448186755180359 train acc: 0.9599999785423279 test loss: 1.0412318706512451 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 360 train loss: 0.9426285624504089 train acc: 0.9624999761581421 test loss: 1.043531060218811 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 361 train loss: 0.9430568218231201 train acc: 0.9624999761581421 test loss: 1.0385810136795044 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 362 train loss: 0.9421519637107849 train acc: 0.9649999737739563 test loss: 1.0355496406555176 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 363 train loss: 0.9429035782814026 train acc: 0.9624999761581421 test loss: 1.0428459644317627 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 364 train loss: 0.9421201944351196 train acc: 0.9624999761581421 test loss: 1.0302302837371826 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 365 train loss: 0.9416705369949341 train acc: 0.9649999737739563 test loss: 1.03402841091156 best test loss: 1.030086874961853 test acc: 0.8799999952316284\n",
      "Epoch 366 train loss: 0.9420130848884583 train acc: 0.9624999761581421 test loss: 1.0356552600860596 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 367 train loss: 0.9432401061058044 train acc: 0.9624999761581421 test loss: 1.0398842096328735 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 368 train loss: 0.9427849054336548 train acc: 0.9624999761581421 test loss: 1.0399508476257324 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 369 train loss: 0.942577064037323 train acc: 0.9624999761581421 test loss: 1.037023901939392 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 370 train loss: 0.9423457980155945 train acc: 0.9624999761581421 test loss: 1.0366634130477905 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 371 train loss: 0.9421629905700684 train acc: 0.9649999737739563 test loss: 1.033463716506958 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 372 train loss: 0.9417737722396851 train acc: 0.9624999761581421 test loss: 1.0431008338928223 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 373 train loss: 0.9428579807281494 train acc: 0.9649999737739563 test loss: 1.035132884979248 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 374 train loss: 0.9417972564697266 train acc: 0.9649999737739563 test loss: 1.0379661321640015 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 375 train loss: 0.9417688250541687 train acc: 0.9649999737739563 test loss: 1.0319935083389282 best test loss: 1.030086874961853 test acc: 0.8500000238418579\n",
      "Epoch 376 train loss: 0.9414465427398682 train acc: 0.9649999737739563 test loss: 1.0346418619155884 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 377 train loss: 0.9421132802963257 train acc: 0.9624999761581421 test loss: 1.0440243482589722 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 378 train loss: 0.9434119462966919 train acc: 0.9624999761581421 test loss: 1.041509747505188 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 379 train loss: 0.9405661821365356 train acc: 0.9649999737739563 test loss: 1.0455143451690674 best test loss: 1.030086874961853 test acc: 0.8500000238418579\n",
      "Epoch 380 train loss: 0.9410755038261414 train acc: 0.9649999737739563 test loss: 1.0395623445510864 best test loss: 1.030086874961853 test acc: 0.8500000238418579\n",
      "Epoch 381 train loss: 0.9408635497093201 train acc: 0.9649999737739563 test loss: 1.0338932275772095 best test loss: 1.030086874961853 test acc: 0.8799999952316284\n",
      "Epoch 382 train loss: 0.941477358341217 train acc: 0.9649999737739563 test loss: 1.0393059253692627 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 383 train loss: 0.9407896399497986 train acc: 0.9649999737739563 test loss: 1.0383143424987793 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 384 train loss: 0.9407253861427307 train acc: 0.9649999737739563 test loss: 1.031042456626892 best test loss: 1.030086874961853 test acc: 0.8799999952316284\n",
      "Epoch 385 train loss: 0.9406310319900513 train acc: 0.9649999737739563 test loss: 1.033219814300537 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 386 train loss: 0.940809965133667 train acc: 0.9649999737739563 test loss: 1.0519405603408813 best test loss: 1.030086874961853 test acc: 0.8299999833106995\n",
      "Epoch 387 train loss: 0.9405450224876404 train acc: 0.9649999737739563 test loss: 1.0383447408676147 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 388 train loss: 0.9424280524253845 train acc: 0.9624999761581421 test loss: 1.0383623838424683 best test loss: 1.030086874961853 test acc: 0.8600000143051147\n",
      "Epoch 389 train loss: 0.9412233829498291 train acc: 0.9649999737739563 test loss: 1.0364116430282593 best test loss: 1.030086874961853 test acc: 0.8700000047683716\n",
      "Epoch 390 train loss: 0.9432323575019836 train acc: 0.9599999785423279 test loss: 1.0487686395645142 best test loss: 1.030086874961853 test acc: 0.8399999737739563\n",
      "Epoch 391 train loss: 0.9404241442680359 train acc: 0.9649999737739563 test loss: 1.029903531074524 best test loss: 1.029903531074524 test acc: 0.8700000047683716\n",
      "Epoch 392 train loss: 0.9402715563774109 train acc: 0.9649999737739563 test loss: 1.0366859436035156 best test loss: 1.029903531074524 test acc: 0.8600000143051147\n",
      "Epoch 393 train loss: 0.9407721161842346 train acc: 0.9649999737739563 test loss: 1.0410327911376953 best test loss: 1.029903531074524 test acc: 0.8500000238418579\n",
      "Epoch 394 train loss: 0.9403029084205627 train acc: 0.9649999737739563 test loss: 1.0463781356811523 best test loss: 1.029903531074524 test acc: 0.8500000238418579\n",
      "Epoch 395 train loss: 0.9403906464576721 train acc: 0.9649999737739563 test loss: 1.0428962707519531 best test loss: 1.029903531074524 test acc: 0.8500000238418579\n",
      "Epoch 396 train loss: 0.9406223893165588 train acc: 0.9649999737739563 test loss: 1.0390393733978271 best test loss: 1.029903531074524 test acc: 0.8600000143051147\n",
      "Epoch 397 train loss: 0.9405324459075928 train acc: 0.9649999737739563 test loss: 1.0401623249053955 best test loss: 1.029903531074524 test acc: 0.8600000143051147\n",
      "Epoch 398 train loss: 0.9401730298995972 train acc: 0.9649999737739563 test loss: 1.0430704355239868 best test loss: 1.029903531074524 test acc: 0.8500000238418579\n",
      "Epoch 399 train loss: 0.9402106404304504 train acc: 0.9649999737739563 test loss: 1.0336755514144897 best test loss: 1.029903531074524 test acc: 0.8700000047683716\n",
      "Epoch 400 train loss: 0.9405994415283203 train acc: 0.9649999737739563 test loss: 1.036096453666687 best test loss: 1.029903531074524 test acc: 0.8700000047683716\n",
      "Epoch 401 train loss: 0.9399476051330566 train acc: 0.9649999737739563 test loss: 1.0394046306610107 best test loss: 1.029903531074524 test acc: 0.8600000143051147\n",
      "Epoch 402 train loss: 0.9405639171600342 train acc: 0.9649999737739563 test loss: 1.037166953086853 best test loss: 1.029903531074524 test acc: 0.8600000143051147\n",
      "Epoch 403 train loss: 0.9401831030845642 train acc: 0.9649999737739563 test loss: 1.0377981662750244 best test loss: 1.029903531074524 test acc: 0.8700000047683716\n",
      "Epoch 404 train loss: 0.9402305483818054 train acc: 0.9649999737739563 test loss: 1.0462089776992798 best test loss: 1.029903531074524 test acc: 0.8500000238418579\n",
      "Epoch 405 train loss: 0.9399640560150146 train acc: 0.9649999737739563 test loss: 1.0294106006622314 best test loss: 1.0294106006622314 test acc: 0.8700000047683716\n",
      "Epoch 406 train loss: 0.940146267414093 train acc: 0.9649999737739563 test loss: 1.0401415824890137 best test loss: 1.0294106006622314 test acc: 0.8500000238418579\n",
      "Epoch 407 train loss: 0.9403568506240845 train acc: 0.9649999737739563 test loss: 1.0369573831558228 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 408 train loss: 0.9405413866043091 train acc: 0.9649999737739563 test loss: 1.0392094850540161 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 409 train loss: 0.9396324157714844 train acc: 0.9649999737739563 test loss: 1.0380696058273315 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 410 train loss: 0.9399259686470032 train acc: 0.9649999737739563 test loss: 1.041865348815918 best test loss: 1.0294106006622314 test acc: 0.8500000238418579\n",
      "Epoch 411 train loss: 0.9398760795593262 train acc: 0.9649999737739563 test loss: 1.0312896966934204 best test loss: 1.0294106006622314 test acc: 0.8700000047683716\n",
      "Epoch 412 train loss: 0.9397977590560913 train acc: 0.9649999737739563 test loss: 1.0394037961959839 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 413 train loss: 0.93971848487854 train acc: 0.9649999737739563 test loss: 1.0362685918807983 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 414 train loss: 0.9398903846740723 train acc: 0.9649999737739563 test loss: 1.032108187675476 best test loss: 1.0294106006622314 test acc: 0.8700000047683716\n",
      "Epoch 415 train loss: 0.9399357438087463 train acc: 0.9649999737739563 test loss: 1.035929799079895 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 416 train loss: 0.9411961436271667 train acc: 0.9649999737739563 test loss: 1.040844202041626 best test loss: 1.0294106006622314 test acc: 0.8700000047683716\n",
      "Epoch 417 train loss: 0.9399356245994568 train acc: 0.9649999737739563 test loss: 1.035721778869629 best test loss: 1.0294106006622314 test acc: 0.8700000047683716\n",
      "Epoch 418 train loss: 0.9403643608093262 train acc: 0.9649999737739563 test loss: 1.0389171838760376 best test loss: 1.0294106006622314 test acc: 0.8500000238418579\n",
      "Epoch 419 train loss: 0.939985454082489 train acc: 0.9649999737739563 test loss: 1.0358827114105225 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 420 train loss: 0.9396920800209045 train acc: 0.9649999737739563 test loss: 1.0345443487167358 best test loss: 1.0294106006622314 test acc: 0.8700000047683716\n",
      "Epoch 421 train loss: 0.9398787617683411 train acc: 0.9649999737739563 test loss: 1.0386966466903687 best test loss: 1.0294106006622314 test acc: 0.8500000238418579\n",
      "Epoch 422 train loss: 0.9398860335350037 train acc: 0.9649999737739563 test loss: 1.0338010787963867 best test loss: 1.0294106006622314 test acc: 0.8600000143051147\n",
      "Epoch 423 train loss: 0.9397066235542297 train acc: 0.9649999737739563 test loss: 1.0276508331298828 best test loss: 1.0276508331298828 test acc: 0.8799999952316284\n",
      "Epoch 424 train loss: 0.9398342967033386 train acc: 0.9649999737739563 test loss: 1.0349408388137817 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 425 train loss: 0.9399994015693665 train acc: 0.9649999737739563 test loss: 1.0325976610183716 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 426 train loss: 0.939934253692627 train acc: 0.9649999737739563 test loss: 1.031652569770813 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 427 train loss: 0.9402762055397034 train acc: 0.9649999737739563 test loss: 1.031011700630188 best test loss: 1.0276508331298828 test acc: 0.8600000143051147\n",
      "Epoch 428 train loss: 0.9395254254341125 train acc: 0.9649999737739563 test loss: 1.0391877889633179 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 429 train loss: 0.9395986795425415 train acc: 0.9649999737739563 test loss: 1.0412392616271973 best test loss: 1.0276508331298828 test acc: 0.8500000238418579\n",
      "Epoch 430 train loss: 0.9397907257080078 train acc: 0.9649999737739563 test loss: 1.038028359413147 best test loss: 1.0276508331298828 test acc: 0.8500000238418579\n",
      "Epoch 431 train loss: 0.9400599002838135 train acc: 0.9649999737739563 test loss: 1.0371602773666382 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 432 train loss: 0.9398182034492493 train acc: 0.9649999737739563 test loss: 1.0354657173156738 best test loss: 1.0276508331298828 test acc: 0.8600000143051147\n",
      "Epoch 433 train loss: 0.9396049380302429 train acc: 0.9649999737739563 test loss: 1.035668969154358 best test loss: 1.0276508331298828 test acc: 0.8799999952316284\n",
      "Epoch 434 train loss: 0.9395012855529785 train acc: 0.9649999737739563 test loss: 1.033522129058838 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 435 train loss: 0.9396106600761414 train acc: 0.9649999737739563 test loss: 1.028914451599121 best test loss: 1.0276508331298828 test acc: 0.8899999856948853\n",
      "Epoch 436 train loss: 0.9400718212127686 train acc: 0.9649999737739563 test loss: 1.0395455360412598 best test loss: 1.0276508331298828 test acc: 0.8600000143051147\n",
      "Epoch 437 train loss: 0.9397454857826233 train acc: 0.9649999737739563 test loss: 1.0385936498641968 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 438 train loss: 0.9397699236869812 train acc: 0.9649999737739563 test loss: 1.0370334386825562 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 439 train loss: 0.9397138953208923 train acc: 0.9649999737739563 test loss: 1.0365879535675049 best test loss: 1.0276508331298828 test acc: 0.8700000047683716\n",
      "Epoch 440 train loss: 0.9397261738777161 train acc: 0.9649999737739563 test loss: 1.035024881362915 best test loss: 1.0276508331298828 test acc: 0.8600000143051147\n",
      "Epoch 441 train loss: 0.9397323131561279 train acc: 0.9649999737739563 test loss: 1.0393565893173218 best test loss: 1.0276508331298828 test acc: 0.8600000143051147\n",
      "Epoch 442 train loss: 0.9395857453346252 train acc: 0.9649999737739563 test loss: 1.0348509550094604 best test loss: 1.0276508331298828 test acc: 0.8600000143051147\n",
      "Epoch 443 train loss: 0.9396489858627319 train acc: 0.9649999737739563 test loss: 1.0263077020645142 best test loss: 1.0263077020645142 test acc: 0.8799999952316284\n",
      "Epoch 444 train loss: 0.9392443895339966 train acc: 0.9649999737739563 test loss: 1.0324424505233765 best test loss: 1.0263077020645142 test acc: 0.8700000047683716\n",
      "Epoch 445 train loss: 0.939465343952179 train acc: 0.9649999737739563 test loss: 1.0434281826019287 best test loss: 1.0263077020645142 test acc: 0.8500000238418579\n",
      "Epoch 446 train loss: 0.9396765828132629 train acc: 0.9649999737739563 test loss: 1.0264382362365723 best test loss: 1.0263077020645142 test acc: 0.8799999952316284\n",
      "Epoch 447 train loss: 0.939542293548584 train acc: 0.9649999737739563 test loss: 1.0275599956512451 best test loss: 1.0263077020645142 test acc: 0.8799999952316284\n",
      "Epoch 448 train loss: 0.94061279296875 train acc: 0.9649999737739563 test loss: 1.030901312828064 best test loss: 1.0263077020645142 test acc: 0.8700000047683716\n",
      "Epoch 449 train loss: 0.9398402571678162 train acc: 0.9649999737739563 test loss: 1.0335781574249268 best test loss: 1.0263077020645142 test acc: 0.8600000143051147\n",
      "Epoch 450 train loss: 0.9394394159317017 train acc: 0.9649999737739563 test loss: 1.0352245569229126 best test loss: 1.0263077020645142 test acc: 0.8600000143051147\n",
      "Epoch 451 train loss: 0.9393861889839172 train acc: 0.9649999737739563 test loss: 1.0316922664642334 best test loss: 1.0263077020645142 test acc: 0.8700000047683716\n",
      "Epoch 452 train loss: 0.9397678375244141 train acc: 0.9649999737739563 test loss: 1.0311838388442993 best test loss: 1.0263077020645142 test acc: 0.8700000047683716\n",
      "Epoch 453 train loss: 0.9397854804992676 train acc: 0.9649999737739563 test loss: 1.0341386795043945 best test loss: 1.0263077020645142 test acc: 0.8700000047683716\n",
      "Epoch 454 train loss: 0.9397220611572266 train acc: 0.9649999737739563 test loss: 1.0366582870483398 best test loss: 1.0263077020645142 test acc: 0.8600000143051147\n",
      "Epoch 455 train loss: 0.9397879838943481 train acc: 0.9649999737739563 test loss: 1.0395112037658691 best test loss: 1.0263077020645142 test acc: 0.8600000143051147\n",
      "Epoch 456 train loss: 0.9392971992492676 train acc: 0.9649999737739563 test loss: 1.0373101234436035 best test loss: 1.0263077020645142 test acc: 0.8700000047683716\n",
      "Epoch 457 train loss: 0.9393675923347473 train acc: 0.9649999737739563 test loss: 1.02571439743042 best test loss: 1.02571439743042 test acc: 0.8700000047683716\n",
      "Epoch 458 train loss: 0.9395192861557007 train acc: 0.9649999737739563 test loss: 1.037888526916504 best test loss: 1.02571439743042 test acc: 0.8700000047683716\n",
      "Epoch 459 train loss: 0.9399623870849609 train acc: 0.9649999737739563 test loss: 1.031379222869873 best test loss: 1.02571439743042 test acc: 0.8700000047683716\n",
      "Epoch 460 train loss: 0.9395427107810974 train acc: 0.9649999737739563 test loss: 1.0325558185577393 best test loss: 1.02571439743042 test acc: 0.8700000047683716\n",
      "Epoch 461 train loss: 0.9392910599708557 train acc: 0.9649999737739563 test loss: 1.0395945310592651 best test loss: 1.02571439743042 test acc: 0.8600000143051147\n",
      "Epoch 462 train loss: 0.9394897222518921 train acc: 0.9649999737739563 test loss: 1.0365029573440552 best test loss: 1.02571439743042 test acc: 0.8600000143051147\n",
      "Epoch 463 train loss: 0.9393492341041565 train acc: 0.9649999737739563 test loss: 1.0384669303894043 best test loss: 1.02571439743042 test acc: 0.8600000143051147\n",
      "Epoch 464 train loss: 0.9395502209663391 train acc: 0.9649999737739563 test loss: 1.026151180267334 best test loss: 1.02571439743042 test acc: 0.8799999952316284\n",
      "Epoch 465 train loss: 0.9410965442657471 train acc: 0.9624999761581421 test loss: 1.0321335792541504 best test loss: 1.02571439743042 test acc: 0.8600000143051147\n",
      "Epoch 466 train loss: 0.9393699169158936 train acc: 0.9649999737739563 test loss: 1.0314300060272217 best test loss: 1.02571439743042 test acc: 0.8600000143051147\n",
      "Epoch 467 train loss: 0.9397026896476746 train acc: 0.9649999737739563 test loss: 1.029695749282837 best test loss: 1.02571439743042 test acc: 0.8799999952316284\n",
      "Epoch 468 train loss: 0.9394298791885376 train acc: 0.9649999737739563 test loss: 1.0249460935592651 best test loss: 1.0249460935592651 test acc: 0.8799999952316284\n",
      "Epoch 469 train loss: 0.9393277168273926 train acc: 0.9649999737739563 test loss: 1.0306706428527832 best test loss: 1.0249460935592651 test acc: 0.8799999952316284\n",
      "Epoch 470 train loss: 0.938804030418396 train acc: 0.9674999713897705 test loss: 1.0335711240768433 best test loss: 1.0249460935592651 test acc: 0.8700000047683716\n",
      "Epoch 471 train loss: 0.9392114281654358 train acc: 0.9649999737739563 test loss: 1.0318423509597778 best test loss: 1.0249460935592651 test acc: 0.8700000047683716\n",
      "Epoch 472 train loss: 0.9396097660064697 train acc: 0.9649999737739563 test loss: 1.0289113521575928 best test loss: 1.0249460935592651 test acc: 0.8700000047683716\n",
      "Epoch 473 train loss: 0.939179539680481 train acc: 0.9649999737739563 test loss: 1.0370570421218872 best test loss: 1.0249460935592651 test acc: 0.8600000143051147\n",
      "Epoch 474 train loss: 0.939144492149353 train acc: 0.9649999737739563 test loss: 1.024303913116455 best test loss: 1.024303913116455 test acc: 0.8799999952316284\n",
      "Epoch 475 train loss: 0.939026951789856 train acc: 0.9674999713897705 test loss: 1.0241273641586304 best test loss: 1.0241273641586304 test acc: 0.8799999952316284\n",
      "Epoch 476 train loss: 0.9391376972198486 train acc: 0.9649999737739563 test loss: 1.0368109941482544 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 477 train loss: 0.9391948580741882 train acc: 0.9649999737739563 test loss: 1.0311185121536255 best test loss: 1.0241273641586304 test acc: 0.8799999952316284\n",
      "Epoch 478 train loss: 0.9377270340919495 train acc: 0.9674999713897705 test loss: 1.0387932062149048 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 479 train loss: 0.9391274452209473 train acc: 0.9649999737739563 test loss: 1.031786322593689 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 480 train loss: 0.9379278421401978 train acc: 0.9674999713897705 test loss: 1.0339778661727905 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 481 train loss: 0.9379488229751587 train acc: 0.9674999713897705 test loss: 1.0360593795776367 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 482 train loss: 0.9385443925857544 train acc: 0.9674999713897705 test loss: 1.0340156555175781 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 483 train loss: 0.9381521344184875 train acc: 0.9674999713897705 test loss: 1.033888339996338 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 484 train loss: 0.9377321004867554 train acc: 0.9674999713897705 test loss: 1.0355734825134277 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 485 train loss: 0.9378893971443176 train acc: 0.9674999713897705 test loss: 1.0337790250778198 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 486 train loss: 0.9384011626243591 train acc: 0.9674999713897705 test loss: 1.0272189378738403 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 487 train loss: 0.9377631545066833 train acc: 0.9674999713897705 test loss: 1.0275626182556152 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 488 train loss: 0.9373335242271423 train acc: 0.9674999713897705 test loss: 1.032946228981018 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 489 train loss: 0.9374087452888489 train acc: 0.9674999713897705 test loss: 1.0388636589050293 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 490 train loss: 0.9378067851066589 train acc: 0.9674999713897705 test loss: 1.037814736366272 best test loss: 1.0241273641586304 test acc: 0.8500000238418579\n",
      "Epoch 491 train loss: 0.9377158284187317 train acc: 0.9674999713897705 test loss: 1.0329893827438354 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 492 train loss: 0.9374337196350098 train acc: 0.9674999713897705 test loss: 1.0292664766311646 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 493 train loss: 0.9376339912414551 train acc: 0.9674999713897705 test loss: 1.0351841449737549 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 494 train loss: 0.9374864101409912 train acc: 0.9674999713897705 test loss: 1.0310862064361572 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 495 train loss: 0.937818169593811 train acc: 0.9674999713897705 test loss: 1.0331820249557495 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 496 train loss: 0.9377335906028748 train acc: 0.9674999713897705 test loss: 1.03465735912323 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 497 train loss: 0.9374198913574219 train acc: 0.9674999713897705 test loss: 1.0299607515335083 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 498 train loss: 0.9373727440834045 train acc: 0.9674999713897705 test loss: 1.0346543788909912 best test loss: 1.0241273641586304 test acc: 0.8700000047683716\n",
      "Epoch 499 train loss: 0.9376896023750305 train acc: 0.9674999713897705 test loss: 1.0406253337860107 best test loss: 1.0241273641586304 test acc: 0.8600000143051147\n",
      "Epoch 500 train loss: 0.9376746416091919 train acc: 0.9674999713897705 test loss: 1.0220847129821777 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 501 train loss: 0.9374479055404663 train acc: 0.9674999713897705 test loss: 1.0279754400253296 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 502 train loss: 0.9375315308570862 train acc: 0.9674999713897705 test loss: 1.0348509550094604 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 503 train loss: 0.9376245737075806 train acc: 0.9674999713897705 test loss: 1.0344105958938599 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 504 train loss: 0.9376273155212402 train acc: 0.9674999713897705 test loss: 1.026186466217041 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 505 train loss: 0.937351405620575 train acc: 0.9674999713897705 test loss: 1.027531623840332 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 506 train loss: 0.9373520016670227 train acc: 0.9674999713897705 test loss: 1.0232585668563843 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 507 train loss: 0.9374356269836426 train acc: 0.9674999713897705 test loss: 1.0436609983444214 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 508 train loss: 0.9372268915176392 train acc: 0.9674999713897705 test loss: 1.0269312858581543 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 509 train loss: 0.9376675486564636 train acc: 0.9674999713897705 test loss: 1.02736496925354 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 510 train loss: 0.9373266696929932 train acc: 0.9674999713897705 test loss: 1.0295928716659546 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 511 train loss: 0.93735671043396 train acc: 0.9674999713897705 test loss: 1.0262951850891113 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 512 train loss: 0.9374272227287292 train acc: 0.9674999713897705 test loss: 1.0303568840026855 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 513 train loss: 0.937347412109375 train acc: 0.9674999713897705 test loss: 1.0363351106643677 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 514 train loss: 0.9372925758361816 train acc: 0.9674999713897705 test loss: 1.0306670665740967 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 515 train loss: 0.9374331831932068 train acc: 0.9674999713897705 test loss: 1.0325981378555298 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 516 train loss: 0.9374090433120728 train acc: 0.9674999713897705 test loss: 1.0355110168457031 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 517 train loss: 0.9371678233146667 train acc: 0.9674999713897705 test loss: 1.0289053916931152 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 518 train loss: 0.9375016689300537 train acc: 0.9674999713897705 test loss: 1.0315327644348145 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 519 train loss: 0.9375907182693481 train acc: 0.9674999713897705 test loss: 1.0376973152160645 best test loss: 1.0220847129821777 test acc: 0.8500000238418579\n",
      "Epoch 520 train loss: 0.9371487498283386 train acc: 0.9674999713897705 test loss: 1.0321367979049683 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 521 train loss: 0.9373174905776978 train acc: 0.9674999713897705 test loss: 1.0284403562545776 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 522 train loss: 0.9372116327285767 train acc: 0.9674999713897705 test loss: 1.0386667251586914 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 523 train loss: 0.937239408493042 train acc: 0.9674999713897705 test loss: 1.0298380851745605 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 524 train loss: 0.9372488260269165 train acc: 0.9674999713897705 test loss: 1.0321831703186035 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 525 train loss: 0.937419056892395 train acc: 0.9674999713897705 test loss: 1.0274275541305542 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 526 train loss: 0.9371631145477295 train acc: 0.9674999713897705 test loss: 1.0381451845169067 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 527 train loss: 0.9371188879013062 train acc: 0.9674999713897705 test loss: 1.0330510139465332 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 528 train loss: 0.9374011754989624 train acc: 0.9674999713897705 test loss: 1.0265065431594849 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 529 train loss: 0.9372738003730774 train acc: 0.9674999713897705 test loss: 1.0286846160888672 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 530 train loss: 0.9383847117424011 train acc: 0.9674999713897705 test loss: 1.027086615562439 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 531 train loss: 0.9372749924659729 train acc: 0.9674999713897705 test loss: 1.0287730693817139 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 532 train loss: 0.9371553659439087 train acc: 0.9674999713897705 test loss: 1.0299482345581055 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 533 train loss: 0.9371833801269531 train acc: 0.9674999713897705 test loss: 1.0402774810791016 best test loss: 1.0220847129821777 test acc: 0.8500000238418579\n",
      "Epoch 534 train loss: 0.9374899864196777 train acc: 0.9674999713897705 test loss: 1.0301775932312012 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 535 train loss: 0.9370571970939636 train acc: 0.9674999713897705 test loss: 1.0342587232589722 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 536 train loss: 0.9372430443763733 train acc: 0.9674999713897705 test loss: 1.0293428897857666 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 537 train loss: 0.9371525645256042 train acc: 0.9674999713897705 test loss: 1.0308016538619995 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 538 train loss: 0.9371774196624756 train acc: 0.9674999713897705 test loss: 1.0276257991790771 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 539 train loss: 0.9372876882553101 train acc: 0.9674999713897705 test loss: 1.0417306423187256 best test loss: 1.0220847129821777 test acc: 0.8500000238418579\n",
      "Epoch 540 train loss: 0.9371487498283386 train acc: 0.9674999713897705 test loss: 1.0278289318084717 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 541 train loss: 0.9372969269752502 train acc: 0.9674999713897705 test loss: 1.0348559617996216 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 542 train loss: 0.9372557997703552 train acc: 0.9674999713897705 test loss: 1.028113842010498 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 543 train loss: 0.9376002550125122 train acc: 0.9674999713897705 test loss: 1.0269129276275635 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 544 train loss: 0.9371953010559082 train acc: 0.9674999713897705 test loss: 1.0322625637054443 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 545 train loss: 0.9371563196182251 train acc: 0.9674999713897705 test loss: 1.025006890296936 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 546 train loss: 0.9371523857116699 train acc: 0.9674999713897705 test loss: 1.0253323316574097 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 547 train loss: 0.9373555183410645 train acc: 0.9674999713897705 test loss: 1.0245275497436523 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 548 train loss: 0.9373389482498169 train acc: 0.9674999713897705 test loss: 1.0305284261703491 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 549 train loss: 0.937099814414978 train acc: 0.9674999713897705 test loss: 1.0271979570388794 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 550 train loss: 0.9376105666160583 train acc: 0.9674999713897705 test loss: 1.032351016998291 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 551 train loss: 0.9375056624412537 train acc: 0.9674999713897705 test loss: 1.0293065309524536 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 552 train loss: 0.937103271484375 train acc: 0.9674999713897705 test loss: 1.0321978330612183 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 553 train loss: 0.9372248649597168 train acc: 0.9674999713897705 test loss: 1.039607286453247 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 554 train loss: 0.9375240802764893 train acc: 0.9674999713897705 test loss: 1.0321365594863892 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 555 train loss: 0.9370526671409607 train acc: 0.9674999713897705 test loss: 1.0286104679107666 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 556 train loss: 0.9371351003646851 train acc: 0.9674999713897705 test loss: 1.0310629606246948 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 557 train loss: 0.9371492266654968 train acc: 0.9674999713897705 test loss: 1.0359662771224976 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 558 train loss: 0.9371163845062256 train acc: 0.9674999713897705 test loss: 1.030407190322876 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 559 train loss: 0.9370085000991821 train acc: 0.9674999713897705 test loss: 1.028000831604004 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 560 train loss: 0.9370152354240417 train acc: 0.9674999713897705 test loss: 1.032507061958313 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 561 train loss: 0.9371589422225952 train acc: 0.9674999713897705 test loss: 1.0270540714263916 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 562 train loss: 0.9371904134750366 train acc: 0.9674999713897705 test loss: 1.0260006189346313 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 563 train loss: 0.9371342658996582 train acc: 0.9674999713897705 test loss: 1.0304101705551147 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 564 train loss: 0.9369913339614868 train acc: 0.9674999713897705 test loss: 1.0334395170211792 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 565 train loss: 0.9371301531791687 train acc: 0.9674999713897705 test loss: 1.0329320430755615 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 566 train loss: 0.9379251599311829 train acc: 0.9674999713897705 test loss: 1.0298316478729248 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 567 train loss: 0.9371396899223328 train acc: 0.9674999713897705 test loss: 1.033737063407898 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 568 train loss: 0.9370729923248291 train acc: 0.9674999713897705 test loss: 1.0220954418182373 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 569 train loss: 0.9372430443763733 train acc: 0.9674999713897705 test loss: 1.0278838872909546 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 570 train loss: 0.9370430111885071 train acc: 0.9674999713897705 test loss: 1.033185601234436 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 571 train loss: 0.9372859001159668 train acc: 0.9674999713897705 test loss: 1.0315957069396973 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 572 train loss: 0.9370677471160889 train acc: 0.9674999713897705 test loss: 1.0245041847229004 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 573 train loss: 0.9372243285179138 train acc: 0.9674999713897705 test loss: 1.0291160345077515 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 574 train loss: 0.9370476007461548 train acc: 0.9674999713897705 test loss: 1.033827543258667 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 575 train loss: 0.9371687173843384 train acc: 0.9674999713897705 test loss: 1.030980110168457 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 576 train loss: 0.9372691512107849 train acc: 0.9674999713897705 test loss: 1.0311601161956787 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 577 train loss: 0.9371575713157654 train acc: 0.9674999713897705 test loss: 1.0322394371032715 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 578 train loss: 0.936921238899231 train acc: 0.9674999713897705 test loss: 1.0336692333221436 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 579 train loss: 0.9374377727508545 train acc: 0.9674999713897705 test loss: 1.0343363285064697 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 580 train loss: 0.9371370077133179 train acc: 0.9674999713897705 test loss: 1.0306763648986816 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 581 train loss: 0.937549889087677 train acc: 0.9674999713897705 test loss: 1.0286004543304443 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 582 train loss: 0.9372231364250183 train acc: 0.9674999713897705 test loss: 1.0326839685440063 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 583 train loss: 0.9370974898338318 train acc: 0.9674999713897705 test loss: 1.032381296157837 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 584 train loss: 0.9368842244148254 train acc: 0.9674999713897705 test loss: 1.0325218439102173 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 585 train loss: 0.9371178150177002 train acc: 0.9674999713897705 test loss: 1.0370432138442993 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 586 train loss: 0.9370677471160889 train acc: 0.9674999713897705 test loss: 1.0275301933288574 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 587 train loss: 0.937284529209137 train acc: 0.9674999713897705 test loss: 1.031758427619934 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 588 train loss: 0.9368671178817749 train acc: 0.9674999713897705 test loss: 1.0330452919006348 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 589 train loss: 0.9376326203346252 train acc: 0.9674999713897705 test loss: 1.031002402305603 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 590 train loss: 0.9370608329772949 train acc: 0.9674999713897705 test loss: 1.027380347251892 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 591 train loss: 0.937195360660553 train acc: 0.9674999713897705 test loss: 1.0315600633621216 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 592 train loss: 0.9370743632316589 train acc: 0.9674999713897705 test loss: 1.0249357223510742 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 593 train loss: 0.9370523691177368 train acc: 0.9674999713897705 test loss: 1.0298142433166504 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 594 train loss: 0.9369643330574036 train acc: 0.9674999713897705 test loss: 1.029944658279419 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 595 train loss: 0.9368700981140137 train acc: 0.9674999713897705 test loss: 1.0275577306747437 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 596 train loss: 0.9370248317718506 train acc: 0.9674999713897705 test loss: 1.0311142206192017 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 597 train loss: 0.9370541572570801 train acc: 0.9674999713897705 test loss: 1.0317729711532593 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 598 train loss: 0.9370792508125305 train acc: 0.9674999713897705 test loss: 1.0386438369750977 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 599 train loss: 0.9371029138565063 train acc: 0.9674999713897705 test loss: 1.0289782285690308 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 600 train loss: 0.9368763566017151 train acc: 0.9674999713897705 test loss: 1.028122067451477 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 601 train loss: 0.9370373487472534 train acc: 0.9674999713897705 test loss: 1.028632402420044 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 602 train loss: 0.9370976090431213 train acc: 0.9674999713897705 test loss: 1.0251128673553467 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 603 train loss: 0.9371870160102844 train acc: 0.9674999713897705 test loss: 1.0384612083435059 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 604 train loss: 0.937194287776947 train acc: 0.9674999713897705 test loss: 1.0247445106506348 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 605 train loss: 0.9369827508926392 train acc: 0.9674999713897705 test loss: 1.0278573036193848 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 606 train loss: 0.9370402693748474 train acc: 0.9674999713897705 test loss: 1.0270806550979614 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 607 train loss: 0.9368997812271118 train acc: 0.9674999713897705 test loss: 1.0295826196670532 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 608 train loss: 0.9370558261871338 train acc: 0.9674999713897705 test loss: 1.031640648841858 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 609 train loss: 0.9369060397148132 train acc: 0.9674999713897705 test loss: 1.0332602262496948 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 610 train loss: 0.9369064569473267 train acc: 0.9674999713897705 test loss: 1.023492693901062 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 611 train loss: 0.936977744102478 train acc: 0.9674999713897705 test loss: 1.0367431640625 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 612 train loss: 0.9368829131126404 train acc: 0.9674999713897705 test loss: 1.0271259546279907 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 613 train loss: 0.9372653365135193 train acc: 0.9674999713897705 test loss: 1.0266920328140259 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 614 train loss: 0.9372101426124573 train acc: 0.9674999713897705 test loss: 1.0243316888809204 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 615 train loss: 0.9370142221450806 train acc: 0.9674999713897705 test loss: 1.0375186204910278 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 616 train loss: 0.9370680451393127 train acc: 0.9674999713897705 test loss: 1.036321759223938 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 617 train loss: 0.9371637105941772 train acc: 0.9674999713897705 test loss: 1.0281862020492554 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 618 train loss: 0.9370889067649841 train acc: 0.9674999713897705 test loss: 1.0268348455429077 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 619 train loss: 0.9370927214622498 train acc: 0.9674999713897705 test loss: 1.0231510400772095 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 620 train loss: 0.9369705319404602 train acc: 0.9674999713897705 test loss: 1.0277537107467651 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 621 train loss: 0.9372022151947021 train acc: 0.9674999713897705 test loss: 1.0251705646514893 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 622 train loss: 0.9371992349624634 train acc: 0.9674999713897705 test loss: 1.035463809967041 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 623 train loss: 0.9370263814926147 train acc: 0.9674999713897705 test loss: 1.030133843421936 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 624 train loss: 0.9370274543762207 train acc: 0.9674999713897705 test loss: 1.0271997451782227 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 625 train loss: 0.9370498657226562 train acc: 0.9674999713897705 test loss: 1.022594928741455 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 626 train loss: 0.9369626641273499 train acc: 0.9674999713897705 test loss: 1.034291386604309 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 627 train loss: 0.9370751976966858 train acc: 0.9674999713897705 test loss: 1.0336657762527466 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 628 train loss: 0.936978280544281 train acc: 0.9674999713897705 test loss: 1.0269653797149658 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 629 train loss: 0.9369115233421326 train acc: 0.9674999713897705 test loss: 1.026326298713684 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 630 train loss: 0.9369866251945496 train acc: 0.9674999713897705 test loss: 1.0256832838058472 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 631 train loss: 0.9371103048324585 train acc: 0.9674999713897705 test loss: 1.036099910736084 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 632 train loss: 0.9370357990264893 train acc: 0.9674999713897705 test loss: 1.0273529291152954 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 633 train loss: 0.9370867013931274 train acc: 0.9674999713897705 test loss: 1.0280349254608154 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 634 train loss: 0.937203049659729 train acc: 0.9674999713897705 test loss: 1.0387349128723145 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 635 train loss: 0.9369451999664307 train acc: 0.9674999713897705 test loss: 1.034929633140564 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 636 train loss: 0.9369912147521973 train acc: 0.9674999713897705 test loss: 1.0319002866744995 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 637 train loss: 0.9370879530906677 train acc: 0.9674999713897705 test loss: 1.0274460315704346 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 638 train loss: 0.937003493309021 train acc: 0.9674999713897705 test loss: 1.033963680267334 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 639 train loss: 0.9371351003646851 train acc: 0.9674999713897705 test loss: 1.028766393661499 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 640 train loss: 0.9369491338729858 train acc: 0.9674999713897705 test loss: 1.0316574573516846 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 641 train loss: 0.9370278716087341 train acc: 0.9674999713897705 test loss: 1.031397819519043 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 642 train loss: 0.9370692372322083 train acc: 0.9674999713897705 test loss: 1.028869867324829 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 643 train loss: 0.9368893504142761 train acc: 0.9674999713897705 test loss: 1.0261704921722412 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 644 train loss: 0.9368324279785156 train acc: 0.9674999713897705 test loss: 1.034532070159912 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 645 train loss: 0.9368558526039124 train acc: 0.9674999713897705 test loss: 1.031099796295166 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 646 train loss: 0.9369924664497375 train acc: 0.9674999713897705 test loss: 1.0256848335266113 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 647 train loss: 0.9369869232177734 train acc: 0.9674999713897705 test loss: 1.033647060394287 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 648 train loss: 0.9367830157279968 train acc: 0.9674999713897705 test loss: 1.0259989500045776 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 649 train loss: 0.9371498823165894 train acc: 0.9674999713897705 test loss: 1.0313329696655273 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 650 train loss: 0.9369522333145142 train acc: 0.9674999713897705 test loss: 1.0263270139694214 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 651 train loss: 0.9369016289710999 train acc: 0.9674999713897705 test loss: 1.028486967086792 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 652 train loss: 0.9368101358413696 train acc: 0.9674999713897705 test loss: 1.0410816669464111 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 653 train loss: 0.9369093179702759 train acc: 0.9674999713897705 test loss: 1.0317529439926147 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 654 train loss: 0.9369537234306335 train acc: 0.9674999713897705 test loss: 1.038855791091919 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 655 train loss: 0.9368299841880798 train acc: 0.9674999713897705 test loss: 1.0353012084960938 best test loss: 1.0220847129821777 test acc: 0.8600000143051147\n",
      "Epoch 656 train loss: 0.9370548129081726 train acc: 0.9674999713897705 test loss: 1.0320931673049927 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 657 train loss: 0.9369378089904785 train acc: 0.9674999713897705 test loss: 1.0247362852096558 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 658 train loss: 0.9367292523384094 train acc: 0.9674999713897705 test loss: 1.0252588987350464 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 659 train loss: 0.9369511604309082 train acc: 0.9674999713897705 test loss: 1.0262665748596191 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 660 train loss: 0.9368905425071716 train acc: 0.9674999713897705 test loss: 1.0347325801849365 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 661 train loss: 0.9370221495628357 train acc: 0.9674999713897705 test loss: 1.0312740802764893 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 662 train loss: 0.9369127154350281 train acc: 0.9674999713897705 test loss: 1.0323138236999512 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 663 train loss: 0.9370558261871338 train acc: 0.9674999713897705 test loss: 1.0251485109329224 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 664 train loss: 0.9368711709976196 train acc: 0.9674999713897705 test loss: 1.029414176940918 best test loss: 1.0220847129821777 test acc: 0.8700000047683716\n",
      "Epoch 665 train loss: 0.9369493722915649 train acc: 0.9674999713897705 test loss: 1.0240166187286377 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 666 train loss: 0.9367486834526062 train acc: 0.9674999713897705 test loss: 1.029574990272522 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 667 train loss: 0.9369586706161499 train acc: 0.9674999713897705 test loss: 1.024283528327942 best test loss: 1.0220847129821777 test acc: 0.8899999856948853\n",
      "Epoch 668 train loss: 0.9366011619567871 train acc: 0.9674999713897705 test loss: 1.0245354175567627 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 669 train loss: 0.9369560480117798 train acc: 0.9674999713897705 test loss: 1.0339610576629639 best test loss: 1.0220847129821777 test acc: 0.8799999952316284\n",
      "Epoch 670 train loss: 0.9369678497314453 train acc: 0.9674999713897705 test loss: 1.0218809843063354 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 671 train loss: 0.9368922710418701 train acc: 0.9674999713897705 test loss: 1.0245095491409302 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 672 train loss: 0.9368314146995544 train acc: 0.9674999713897705 test loss: 1.025801658630371 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 673 train loss: 0.936877965927124 train acc: 0.9674999713897705 test loss: 1.0283576250076294 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 674 train loss: 0.9368342757225037 train acc: 0.9674999713897705 test loss: 1.0339024066925049 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 675 train loss: 0.9370717406272888 train acc: 0.9674999713897705 test loss: 1.0276916027069092 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 676 train loss: 0.9368218779563904 train acc: 0.9674999713897705 test loss: 1.0315016508102417 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 677 train loss: 0.9368737936019897 train acc: 0.9674999713897705 test loss: 1.0238871574401855 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 678 train loss: 0.9367853403091431 train acc: 0.9674999713897705 test loss: 1.0314761400222778 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 679 train loss: 0.9368852376937866 train acc: 0.9674999713897705 test loss: 1.0290672779083252 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 680 train loss: 0.9370471835136414 train acc: 0.9674999713897705 test loss: 1.0324283838272095 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 681 train loss: 0.936713457107544 train acc: 0.9674999713897705 test loss: 1.0356383323669434 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 682 train loss: 0.9369295239448547 train acc: 0.9674999713897705 test loss: 1.036689281463623 best test loss: 1.0218809843063354 test acc: 0.8600000143051147\n",
      "Epoch 683 train loss: 0.9368380904197693 train acc: 0.9674999713897705 test loss: 1.0270711183547974 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 684 train loss: 0.936775803565979 train acc: 0.9674999713897705 test loss: 1.0357751846313477 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 685 train loss: 0.9366884827613831 train acc: 0.9674999713897705 test loss: 1.0263416767120361 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 686 train loss: 0.9365789890289307 train acc: 0.9674999713897705 test loss: 1.0279912948608398 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 687 train loss: 0.9366738796234131 train acc: 0.9674999713897705 test loss: 1.0295876264572144 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 688 train loss: 0.9368818402290344 train acc: 0.9674999713897705 test loss: 1.0232892036437988 best test loss: 1.0218809843063354 test acc: 0.8799999952316284\n",
      "Epoch 689 train loss: 0.9367244243621826 train acc: 0.9674999713897705 test loss: 1.0286332368850708 best test loss: 1.0218809843063354 test acc: 0.8700000047683716\n",
      "Epoch 690 train loss: 0.936489462852478 train acc: 0.9674999713897705 test loss: 1.0208343267440796 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 691 train loss: 0.9366796016693115 train acc: 0.9674999713897705 test loss: 1.028106927871704 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 692 train loss: 0.9366974830627441 train acc: 0.9674999713897705 test loss: 1.0306063890457153 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 693 train loss: 0.9366326928138733 train acc: 0.9674999713897705 test loss: 1.0266404151916504 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 694 train loss: 0.9365678429603577 train acc: 0.9674999713897705 test loss: 1.0316691398620605 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 695 train loss: 0.9366274476051331 train acc: 0.9674999713897705 test loss: 1.0308326482772827 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 696 train loss: 0.9363064765930176 train acc: 0.9674999713897705 test loss: 1.0211361646652222 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 697 train loss: 0.9365713596343994 train acc: 0.9674999713897705 test loss: 1.0276741981506348 best test loss: 1.0208343267440796 test acc: 0.8700000047683716\n",
      "Epoch 698 train loss: 0.9366549849510193 train acc: 0.9674999713897705 test loss: 1.0245177745819092 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 699 train loss: 0.9361722469329834 train acc: 0.9700000286102295 test loss: 1.0252814292907715 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 700 train loss: 0.9365054368972778 train acc: 0.9674999713897705 test loss: 1.0250011682510376 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 701 train loss: 0.9363794922828674 train acc: 0.9674999713897705 test loss: 1.0334604978561401 best test loss: 1.0208343267440796 test acc: 0.8700000047683716\n",
      "Epoch 702 train loss: 0.9365769028663635 train acc: 0.9674999713897705 test loss: 1.0294939279556274 best test loss: 1.0208343267440796 test acc: 0.8700000047683716\n",
      "Epoch 703 train loss: 0.9368631839752197 train acc: 0.9674999713897705 test loss: 1.0407798290252686 best test loss: 1.0208343267440796 test acc: 0.8500000238418579\n",
      "Epoch 704 train loss: 0.9360822439193726 train acc: 0.9700000286102295 test loss: 1.0312657356262207 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 705 train loss: 0.9358322024345398 train acc: 0.9700000286102295 test loss: 1.028488039970398 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 706 train loss: 0.9360517859458923 train acc: 0.9700000286102295 test loss: 1.023194432258606 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 707 train loss: 0.9361531138420105 train acc: 0.9700000286102295 test loss: 1.0321321487426758 best test loss: 1.0208343267440796 test acc: 0.8600000143051147\n",
      "Epoch 708 train loss: 0.9356715679168701 train acc: 0.9700000286102295 test loss: 1.0278452634811401 best test loss: 1.0208343267440796 test acc: 0.8799999952316284\n",
      "Epoch 709 train loss: 0.9357961416244507 train acc: 0.9700000286102295 test loss: 1.0281744003295898 best test loss: 1.0208343267440796 test acc: 0.8700000047683716\n",
      "Epoch 710 train loss: 0.9359652996063232 train acc: 0.9700000286102295 test loss: 1.0330499410629272 best test loss: 1.0208343267440796 test acc: 0.8700000047683716\n",
      "Epoch 711 train loss: 0.9353173971176147 train acc: 0.9700000286102295 test loss: 1.0321359634399414 best test loss: 1.0208343267440796 test acc: 0.8700000047683716\n",
      "Epoch 712 train loss: 0.9356942772865295 train acc: 0.9700000286102295 test loss: 1.019209623336792 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 713 train loss: 0.9355677962303162 train acc: 0.9700000286102295 test loss: 1.0230622291564941 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 714 train loss: 0.9351503252983093 train acc: 0.9700000286102295 test loss: 1.0255628824234009 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 715 train loss: 0.9351708292961121 train acc: 0.9700000286102295 test loss: 1.0228310823440552 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 716 train loss: 0.9351872801780701 train acc: 0.9700000286102295 test loss: 1.0285807847976685 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 717 train loss: 0.9348986744880676 train acc: 0.9700000286102295 test loss: 1.026453971862793 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 718 train loss: 0.9350852966308594 train acc: 0.9700000286102295 test loss: 1.028244137763977 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 719 train loss: 0.9356361627578735 train acc: 0.9700000286102295 test loss: 1.0333114862442017 best test loss: 1.019209623336792 test acc: 0.8700000047683716\n",
      "Epoch 720 train loss: 0.934893786907196 train acc: 0.9700000286102295 test loss: 1.0316230058670044 best test loss: 1.019209623336792 test acc: 0.8700000047683716\n",
      "Epoch 721 train loss: 0.9348905682563782 train acc: 0.9700000286102295 test loss: 1.0270782709121704 best test loss: 1.019209623336792 test acc: 0.8799999952316284\n",
      "Epoch 722 train loss: 0.9348362684249878 train acc: 0.9700000286102295 test loss: 1.0315219163894653 best test loss: 1.019209623336792 test acc: 0.8700000047683716\n",
      "Epoch 723 train loss: 0.9352673292160034 train acc: 0.9700000286102295 test loss: 1.0188069343566895 best test loss: 1.0188069343566895 test acc: 0.8899999856948853\n",
      "Epoch 724 train loss: 0.9350379705429077 train acc: 0.9700000286102295 test loss: 1.0286723375320435 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 725 train loss: 0.9348864555358887 train acc: 0.9700000286102295 test loss: 1.0272027254104614 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 726 train loss: 0.9348354935646057 train acc: 0.9700000286102295 test loss: 1.0301580429077148 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 727 train loss: 0.9349002242088318 train acc: 0.9700000286102295 test loss: 1.029226541519165 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 728 train loss: 0.9347708225250244 train acc: 0.9700000286102295 test loss: 1.021918535232544 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 729 train loss: 0.9347847700119019 train acc: 0.9700000286102295 test loss: 1.0206187963485718 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 730 train loss: 0.9348726868629456 train acc: 0.9700000286102295 test loss: 1.0318313837051392 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 731 train loss: 0.9350201487541199 train acc: 0.9700000286102295 test loss: 1.0328880548477173 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 732 train loss: 0.9351350665092468 train acc: 0.9700000286102295 test loss: 1.0214070081710815 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 733 train loss: 0.9348627328872681 train acc: 0.9700000286102295 test loss: 1.0210973024368286 best test loss: 1.0188069343566895 test acc: 0.8899999856948853\n",
      "Epoch 734 train loss: 0.9345844388008118 train acc: 0.9700000286102295 test loss: 1.0257654190063477 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 735 train loss: 0.934776782989502 train acc: 0.9700000286102295 test loss: 1.0312249660491943 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 736 train loss: 0.9346558451652527 train acc: 0.9700000286102295 test loss: 1.0296406745910645 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 737 train loss: 0.9346870183944702 train acc: 0.9700000286102295 test loss: 1.0304591655731201 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 738 train loss: 0.9346057176589966 train acc: 0.9700000286102295 test loss: 1.025220274925232 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 739 train loss: 0.9346120357513428 train acc: 0.9700000286102295 test loss: 1.0247306823730469 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 740 train loss: 0.9350082874298096 train acc: 0.9700000286102295 test loss: 1.0375123023986816 best test loss: 1.0188069343566895 test acc: 0.8600000143051147\n",
      "Epoch 741 train loss: 0.9347579479217529 train acc: 0.9700000286102295 test loss: 1.0259722471237183 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 742 train loss: 0.9348063468933105 train acc: 0.9700000286102295 test loss: 1.026668667793274 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 743 train loss: 0.9347522258758545 train acc: 0.9700000286102295 test loss: 1.0301601886749268 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 744 train loss: 0.9349186420440674 train acc: 0.9700000286102295 test loss: 1.0283030271530151 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 745 train loss: 0.9348257184028625 train acc: 0.9700000286102295 test loss: 1.030051827430725 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 746 train loss: 0.9347988367080688 train acc: 0.9700000286102295 test loss: 1.0274930000305176 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 747 train loss: 0.9347121715545654 train acc: 0.9700000286102295 test loss: 1.0204046964645386 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 748 train loss: 0.9347900152206421 train acc: 0.9700000286102295 test loss: 1.0261752605438232 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 749 train loss: 0.9346717596054077 train acc: 0.9700000286102295 test loss: 1.0234427452087402 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 750 train loss: 0.9347531199455261 train acc: 0.9700000286102295 test loss: 1.0212857723236084 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 751 train loss: 0.934611976146698 train acc: 0.9700000286102295 test loss: 1.0288619995117188 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 752 train loss: 0.9346164464950562 train acc: 0.9700000286102295 test loss: 1.0256329774856567 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 753 train loss: 0.9348229765892029 train acc: 0.9700000286102295 test loss: 1.0231375694274902 best test loss: 1.0188069343566895 test acc: 0.8899999856948853\n",
      "Epoch 754 train loss: 0.9346164464950562 train acc: 0.9700000286102295 test loss: 1.0252071619033813 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 755 train loss: 0.9348787665367126 train acc: 0.9700000286102295 test loss: 1.027967095375061 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 756 train loss: 0.9347373247146606 train acc: 0.9700000286102295 test loss: 1.0313972234725952 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 757 train loss: 0.9346718788146973 train acc: 0.9700000286102295 test loss: 1.0321412086486816 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 758 train loss: 0.9348379373550415 train acc: 0.9700000286102295 test loss: 1.0224651098251343 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 759 train loss: 0.9362019300460815 train acc: 0.9674999713897705 test loss: 1.0287017822265625 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 760 train loss: 0.9350672364234924 train acc: 0.9700000286102295 test loss: 1.0258839130401611 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 761 train loss: 0.9345542788505554 train acc: 0.9700000286102295 test loss: 1.024375081062317 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 762 train loss: 0.9347905516624451 train acc: 0.9700000286102295 test loss: 1.02570378780365 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 763 train loss: 0.9348341226577759 train acc: 0.9700000286102295 test loss: 1.0248104333877563 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 764 train loss: 0.9346442222595215 train acc: 0.9700000286102295 test loss: 1.0197983980178833 best test loss: 1.0188069343566895 test acc: 0.8899999856948853\n",
      "Epoch 765 train loss: 0.9346416592597961 train acc: 0.9700000286102295 test loss: 1.0289024114608765 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 766 train loss: 0.9346989393234253 train acc: 0.9700000286102295 test loss: 1.0316276550292969 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 767 train loss: 0.9347975850105286 train acc: 0.9700000286102295 test loss: 1.0243170261383057 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 768 train loss: 0.9346084594726562 train acc: 0.9700000286102295 test loss: 1.0259552001953125 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 769 train loss: 0.9345008730888367 train acc: 0.9700000286102295 test loss: 1.0314759016036987 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 770 train loss: 0.9347037672996521 train acc: 0.9700000286102295 test loss: 1.0245243310928345 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 771 train loss: 0.9345895648002625 train acc: 0.9700000286102295 test loss: 1.0321850776672363 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 772 train loss: 0.9347615838050842 train acc: 0.9700000286102295 test loss: 1.04788339138031 best test loss: 1.0188069343566895 test acc: 0.8500000238418579\n",
      "Epoch 773 train loss: 0.9347700476646423 train acc: 0.9700000286102295 test loss: 1.0318787097930908 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 774 train loss: 0.9346981048583984 train acc: 0.9700000286102295 test loss: 1.0390665531158447 best test loss: 1.0188069343566895 test acc: 0.8500000238418579\n",
      "Epoch 775 train loss: 0.934715211391449 train acc: 0.9700000286102295 test loss: 1.0235366821289062 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 776 train loss: 0.9349498152732849 train acc: 0.9700000286102295 test loss: 1.0287960767745972 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 777 train loss: 0.9346024394035339 train acc: 0.9700000286102295 test loss: 1.025385856628418 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 778 train loss: 0.9347421526908875 train acc: 0.9700000286102295 test loss: 1.0271059274673462 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 779 train loss: 0.9347435235977173 train acc: 0.9700000286102295 test loss: 1.022027850151062 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 780 train loss: 0.9347516894340515 train acc: 0.9700000286102295 test loss: 1.0354676246643066 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 781 train loss: 0.9346548318862915 train acc: 0.9700000286102295 test loss: 1.0251587629318237 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 782 train loss: 0.9348194599151611 train acc: 0.9700000286102295 test loss: 1.0260040760040283 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 783 train loss: 0.9348040819168091 train acc: 0.9700000286102295 test loss: 1.024091362953186 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 784 train loss: 0.9346098899841309 train acc: 0.9700000286102295 test loss: 1.041870355606079 best test loss: 1.0188069343566895 test acc: 0.8500000238418579\n",
      "Epoch 785 train loss: 0.9346275925636292 train acc: 0.9700000286102295 test loss: 1.0242780447006226 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 786 train loss: 0.9347022771835327 train acc: 0.9700000286102295 test loss: 1.0284950733184814 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 787 train loss: 0.9346421957015991 train acc: 0.9700000286102295 test loss: 1.0323591232299805 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 788 train loss: 0.9346842765808105 train acc: 0.9700000286102295 test loss: 1.0280647277832031 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 789 train loss: 0.9347371459007263 train acc: 0.9700000286102295 test loss: 1.0289721488952637 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 790 train loss: 0.9345628619194031 train acc: 0.9700000286102295 test loss: 1.0270549058914185 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 791 train loss: 0.934538722038269 train acc: 0.9700000286102295 test loss: 1.0267413854599 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 792 train loss: 0.9346570372581482 train acc: 0.9700000286102295 test loss: 1.0280888080596924 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 793 train loss: 0.9346433877944946 train acc: 0.9700000286102295 test loss: 1.0281790494918823 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 794 train loss: 0.9345839023590088 train acc: 0.9700000286102295 test loss: 1.0268702507019043 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 795 train loss: 0.9346670508384705 train acc: 0.9700000286102295 test loss: 1.024399995803833 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 796 train loss: 0.9346660375595093 train acc: 0.9700000286102295 test loss: 1.0323467254638672 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 797 train loss: 0.934516429901123 train acc: 0.9700000286102295 test loss: 1.0251541137695312 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 798 train loss: 0.9346738457679749 train acc: 0.9700000286102295 test loss: 1.0223374366760254 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 799 train loss: 0.93454909324646 train acc: 0.9700000286102295 test loss: 1.0390287637710571 best test loss: 1.0188069343566895 test acc: 0.8500000238418579\n",
      "Epoch 800 train loss: 0.9346827864646912 train acc: 0.9700000286102295 test loss: 1.0290098190307617 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 801 train loss: 0.9346393346786499 train acc: 0.9700000286102295 test loss: 1.0294572114944458 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 802 train loss: 0.9347019195556641 train acc: 0.9700000286102295 test loss: 1.028338074684143 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 803 train loss: 0.9346757531166077 train acc: 0.9700000286102295 test loss: 1.0329253673553467 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 804 train loss: 0.934446394443512 train acc: 0.9700000286102295 test loss: 1.024844765663147 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 805 train loss: 0.9344818592071533 train acc: 0.9700000286102295 test loss: 1.029806137084961 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 806 train loss: 0.9345676898956299 train acc: 0.9700000286102295 test loss: 1.0350956916809082 best test loss: 1.0188069343566895 test acc: 0.8600000143051147\n",
      "Epoch 807 train loss: 0.9346339702606201 train acc: 0.9700000286102295 test loss: 1.0244280099868774 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 808 train loss: 0.9345484972000122 train acc: 0.9700000286102295 test loss: 1.0389331579208374 best test loss: 1.0188069343566895 test acc: 0.8600000143051147\n",
      "Epoch 809 train loss: 0.9348308444023132 train acc: 0.9700000286102295 test loss: 1.0281033515930176 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 810 train loss: 0.9344615340232849 train acc: 0.9700000286102295 test loss: 1.0255905389785767 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 811 train loss: 0.9345512390136719 train acc: 0.9700000286102295 test loss: 1.0272512435913086 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 812 train loss: 0.9344367980957031 train acc: 0.9700000286102295 test loss: 1.0301674604415894 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 813 train loss: 0.9343980550765991 train acc: 0.9700000286102295 test loss: 1.020583152770996 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 814 train loss: 0.9342455863952637 train acc: 0.9700000286102295 test loss: 1.0306915044784546 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 815 train loss: 0.9344633221626282 train acc: 0.9700000286102295 test loss: 1.0312247276306152 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 816 train loss: 0.9346569180488586 train acc: 0.9700000286102295 test loss: 1.0242801904678345 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 817 train loss: 0.9344927072525024 train acc: 0.9700000286102295 test loss: 1.022876501083374 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 818 train loss: 0.9345647692680359 train acc: 0.9700000286102295 test loss: 1.022981882095337 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 819 train loss: 0.934761106967926 train acc: 0.9700000286102295 test loss: 1.0291332006454468 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 820 train loss: 0.9344924688339233 train acc: 0.9700000286102295 test loss: 1.0264098644256592 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 821 train loss: 0.934386134147644 train acc: 0.9700000286102295 test loss: 1.022578239440918 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 822 train loss: 0.9344087243080139 train acc: 0.9700000286102295 test loss: 1.0260921716690063 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 823 train loss: 0.9343757629394531 train acc: 0.9700000286102295 test loss: 1.0317511558532715 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 824 train loss: 0.9341275095939636 train acc: 0.9700000286102295 test loss: 1.032495379447937 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 825 train loss: 0.9340482950210571 train acc: 0.9700000286102295 test loss: 1.0273540019989014 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 826 train loss: 0.9342096447944641 train acc: 0.9700000286102295 test loss: 1.0332963466644287 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 827 train loss: 0.9339263439178467 train acc: 0.9700000286102295 test loss: 1.0333915948867798 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 828 train loss: 0.9335620403289795 train acc: 0.9725000262260437 test loss: 1.0260611772537231 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 829 train loss: 0.9334286451339722 train acc: 0.9725000262260437 test loss: 1.0307966470718384 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 830 train loss: 0.9330427050590515 train acc: 0.9725000262260437 test loss: 1.028546929359436 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 831 train loss: 0.9328793287277222 train acc: 0.9725000262260437 test loss: 1.0252187252044678 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 832 train loss: 0.933592677116394 train acc: 0.9725000262260437 test loss: 1.0315781831741333 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 833 train loss: 0.9330037832260132 train acc: 0.9725000262260437 test loss: 1.0260450839996338 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 834 train loss: 0.9325608611106873 train acc: 0.9725000262260437 test loss: 1.0381900072097778 best test loss: 1.0188069343566895 test acc: 0.8600000143051147\n",
      "Epoch 835 train loss: 0.9330918788909912 train acc: 0.9725000262260437 test loss: 1.0285409688949585 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 836 train loss: 0.9329279065132141 train acc: 0.9725000262260437 test loss: 1.0364608764648438 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 837 train loss: 0.9326826333999634 train acc: 0.9725000262260437 test loss: 1.0327911376953125 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 838 train loss: 0.932859480381012 train acc: 0.9725000262260437 test loss: 1.0250400304794312 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 839 train loss: 0.9324052333831787 train acc: 0.9725000262260437 test loss: 1.033219575881958 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 840 train loss: 0.9326373934745789 train acc: 0.9725000262260437 test loss: 1.038522481918335 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 841 train loss: 0.9323379397392273 train acc: 0.9725000262260437 test loss: 1.024241328239441 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 842 train loss: 0.9327097535133362 train acc: 0.9725000262260437 test loss: 1.0332465171813965 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 843 train loss: 0.9325913786888123 train acc: 0.9725000262260437 test loss: 1.0238652229309082 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 844 train loss: 0.9327417612075806 train acc: 0.9725000262260437 test loss: 1.025683879852295 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 845 train loss: 0.9325519800186157 train acc: 0.9725000262260437 test loss: 1.030687689781189 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 846 train loss: 0.9327988624572754 train acc: 0.9725000262260437 test loss: 1.0264968872070312 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 847 train loss: 0.9324080944061279 train acc: 0.9725000262260437 test loss: 1.02463698387146 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 848 train loss: 0.9326370358467102 train acc: 0.9725000262260437 test loss: 1.029365062713623 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 849 train loss: 0.9332577586174011 train acc: 0.9725000262260437 test loss: 1.0287240743637085 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 850 train loss: 0.9325734972953796 train acc: 0.9725000262260437 test loss: 1.0318909883499146 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 851 train loss: 0.9322742223739624 train acc: 0.9725000262260437 test loss: 1.0257047414779663 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 852 train loss: 0.9323297142982483 train acc: 0.9725000262260437 test loss: 1.0283066034317017 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 853 train loss: 0.932363748550415 train acc: 0.9725000262260437 test loss: 1.0301624536514282 best test loss: 1.0188069343566895 test acc: 0.8700000047683716\n",
      "Epoch 854 train loss: 0.9326117634773254 train acc: 0.9725000262260437 test loss: 1.026344656944275 best test loss: 1.0188069343566895 test acc: 0.8799999952316284\n",
      "Epoch 855 train loss: 0.9323614239692688 train acc: 0.9725000262260437 test loss: 1.0185256004333496 best test loss: 1.0185256004333496 test acc: 0.8899999856948853\n",
      "Epoch 856 train loss: 0.9322822690010071 train acc: 0.9725000262260437 test loss: 1.0294487476348877 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 857 train loss: 0.9324081540107727 train acc: 0.9725000262260437 test loss: 1.0370298624038696 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 858 train loss: 0.9323787689208984 train acc: 0.9725000262260437 test loss: 1.0293221473693848 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 859 train loss: 0.9326735138893127 train acc: 0.9725000262260437 test loss: 1.0413119792938232 best test loss: 1.0185256004333496 test acc: 0.8600000143051147\n",
      "Epoch 860 train loss: 0.9323700666427612 train acc: 0.9725000262260437 test loss: 1.0275276899337769 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 861 train loss: 0.9322253465652466 train acc: 0.9725000262260437 test loss: 1.0240687131881714 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 862 train loss: 0.9326030015945435 train acc: 0.9725000262260437 test loss: 1.0332871675491333 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 863 train loss: 0.9323382377624512 train acc: 0.9725000262260437 test loss: 1.038425326347351 best test loss: 1.0185256004333496 test acc: 0.8500000238418579\n",
      "Epoch 864 train loss: 0.9323621988296509 train acc: 0.9725000262260437 test loss: 1.0225882530212402 best test loss: 1.0185256004333496 test acc: 0.8899999856948853\n",
      "Epoch 865 train loss: 0.9323799014091492 train acc: 0.9725000262260437 test loss: 1.028843641281128 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 866 train loss: 0.932233452796936 train acc: 0.9725000262260437 test loss: 1.0284168720245361 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 867 train loss: 0.9324100613594055 train acc: 0.9725000262260437 test loss: 1.025001049041748 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 868 train loss: 0.9323877096176147 train acc: 0.9725000262260437 test loss: 1.0252935886383057 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 869 train loss: 0.9324101805686951 train acc: 0.9725000262260437 test loss: 1.0227150917053223 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 870 train loss: 0.9322600364685059 train acc: 0.9725000262260437 test loss: 1.0362838506698608 best test loss: 1.0185256004333496 test acc: 0.8600000143051147\n",
      "Epoch 871 train loss: 0.9323319792747498 train acc: 0.9725000262260437 test loss: 1.0245230197906494 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 872 train loss: 0.9323436617851257 train acc: 0.9725000262260437 test loss: 1.024239420890808 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 873 train loss: 0.9322851300239563 train acc: 0.9725000262260437 test loss: 1.0320476293563843 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 874 train loss: 0.932304859161377 train acc: 0.9725000262260437 test loss: 1.0206369161605835 best test loss: 1.0185256004333496 test acc: 0.8899999856948853\n",
      "Epoch 875 train loss: 0.9322862029075623 train acc: 0.9725000262260437 test loss: 1.0313515663146973 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 876 train loss: 0.9323657155036926 train acc: 0.9725000262260437 test loss: 1.0211049318313599 best test loss: 1.0185256004333496 test acc: 0.8899999856948853\n",
      "Epoch 877 train loss: 0.9323800802230835 train acc: 0.9725000262260437 test loss: 1.020274043083191 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 878 train loss: 0.9322210550308228 train acc: 0.9725000262260437 test loss: 1.035786747932434 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 879 train loss: 0.9323046803474426 train acc: 0.9725000262260437 test loss: 1.0255937576293945 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 880 train loss: 0.9322393536567688 train acc: 0.9725000262260437 test loss: 1.0279415845870972 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 881 train loss: 0.932373046875 train acc: 0.9725000262260437 test loss: 1.0236526727676392 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 882 train loss: 0.9322525262832642 train acc: 0.9725000262260437 test loss: 1.0342556238174438 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 883 train loss: 0.9323796629905701 train acc: 0.9725000262260437 test loss: 1.0297515392303467 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 884 train loss: 0.9323559403419495 train acc: 0.9725000262260437 test loss: 1.0232362747192383 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 885 train loss: 0.9321609735488892 train acc: 0.9725000262260437 test loss: 1.0255666971206665 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 886 train loss: 0.9323475360870361 train acc: 0.9725000262260437 test loss: 1.0262211561203003 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 887 train loss: 0.9324529767036438 train acc: 0.9725000262260437 test loss: 1.0222201347351074 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 888 train loss: 0.932191014289856 train acc: 0.9725000262260437 test loss: 1.024155855178833 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 889 train loss: 0.9324420094490051 train acc: 0.9725000262260437 test loss: 1.0283713340759277 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 890 train loss: 0.9323031902313232 train acc: 0.9725000262260437 test loss: 1.029151201248169 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 891 train loss: 0.9322289228439331 train acc: 0.9725000262260437 test loss: 1.0227270126342773 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 892 train loss: 0.9322807788848877 train acc: 0.9725000262260437 test loss: 1.030970573425293 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 893 train loss: 0.9324136972427368 train acc: 0.9725000262260437 test loss: 1.0276951789855957 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 894 train loss: 0.9323703646659851 train acc: 0.9725000262260437 test loss: 1.0234885215759277 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 895 train loss: 0.9322789907455444 train acc: 0.9725000262260437 test loss: 1.0312492847442627 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 896 train loss: 0.9323984384536743 train acc: 0.9725000262260437 test loss: 1.023327112197876 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 897 train loss: 0.9323457479476929 train acc: 0.9725000262260437 test loss: 1.0283435583114624 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 898 train loss: 0.9322435259819031 train acc: 0.9725000262260437 test loss: 1.0276645421981812 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 899 train loss: 0.9323009252548218 train acc: 0.9725000262260437 test loss: 1.0233244895935059 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 900 train loss: 0.9321328997612 train acc: 0.9725000262260437 test loss: 1.027246356010437 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 901 train loss: 0.9321843981742859 train acc: 0.9725000262260437 test loss: 1.0279779434204102 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 902 train loss: 0.9321997165679932 train acc: 0.9725000262260437 test loss: 1.0326342582702637 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 903 train loss: 0.9322828650474548 train acc: 0.9725000262260437 test loss: 1.0259686708450317 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 904 train loss: 0.9323201179504395 train acc: 0.9725000262260437 test loss: 1.0321828126907349 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 905 train loss: 0.9323177933692932 train acc: 0.9725000262260437 test loss: 1.0260076522827148 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 906 train loss: 0.9322010278701782 train acc: 0.9725000262260437 test loss: 1.0276942253112793 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 907 train loss: 0.9322971105575562 train acc: 0.9725000262260437 test loss: 1.0291904211044312 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 908 train loss: 0.9322504997253418 train acc: 0.9725000262260437 test loss: 1.0258101224899292 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 909 train loss: 0.9321795105934143 train acc: 0.9725000262260437 test loss: 1.0237817764282227 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 910 train loss: 0.9321857690811157 train acc: 0.9725000262260437 test loss: 1.0214654207229614 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 911 train loss: 0.9324148297309875 train acc: 0.9725000262260437 test loss: 1.0285147428512573 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 912 train loss: 0.932344377040863 train acc: 0.9725000262260437 test loss: 1.0293654203414917 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 913 train loss: 0.9323478937149048 train acc: 0.9725000262260437 test loss: 1.0282247066497803 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 914 train loss: 0.9323333501815796 train acc: 0.9725000262260437 test loss: 1.0238033533096313 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 915 train loss: 0.9324259161949158 train acc: 0.9725000262260437 test loss: 1.0257736444473267 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 916 train loss: 0.9323179125785828 train acc: 0.9725000262260437 test loss: 1.0249454975128174 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 917 train loss: 0.9321771860122681 train acc: 0.9725000262260437 test loss: 1.0262898206710815 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 918 train loss: 0.932374119758606 train acc: 0.9725000262260437 test loss: 1.0240648984909058 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 919 train loss: 0.9322157502174377 train acc: 0.9725000262260437 test loss: 1.0295798778533936 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 920 train loss: 0.9323397874832153 train acc: 0.9725000262260437 test loss: 1.0246667861938477 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 921 train loss: 0.9324629902839661 train acc: 0.9725000262260437 test loss: 1.0299558639526367 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 922 train loss: 0.9322074055671692 train acc: 0.9725000262260437 test loss: 1.0248762369155884 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 923 train loss: 0.9323357343673706 train acc: 0.9725000262260437 test loss: 1.0342743396759033 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 924 train loss: 0.9327509999275208 train acc: 0.9725000262260437 test loss: 1.027706265449524 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 925 train loss: 0.9322091937065125 train acc: 0.9725000262260437 test loss: 1.0293809175491333 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 926 train loss: 0.9322324395179749 train acc: 0.9725000262260437 test loss: 1.0237981081008911 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 927 train loss: 0.9324116706848145 train acc: 0.9725000262260437 test loss: 1.0250533819198608 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 928 train loss: 0.9321724772453308 train acc: 0.9725000262260437 test loss: 1.0248327255249023 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 929 train loss: 0.9321649670600891 train acc: 0.9725000262260437 test loss: 1.0247950553894043 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 930 train loss: 0.9323728680610657 train acc: 0.9725000262260437 test loss: 1.0279022455215454 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 931 train loss: 0.9322885870933533 train acc: 0.9725000262260437 test loss: 1.0334508419036865 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 932 train loss: 0.9323656558990479 train acc: 0.9725000262260437 test loss: 1.0246580839157104 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 933 train loss: 0.9323874115943909 train acc: 0.9725000262260437 test loss: 1.028663158416748 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 934 train loss: 0.9322697520256042 train acc: 0.9725000262260437 test loss: 1.0216434001922607 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 935 train loss: 0.9323195815086365 train acc: 0.9725000262260437 test loss: 1.0283043384552002 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 936 train loss: 0.9322547912597656 train acc: 0.9725000262260437 test loss: 1.0303542613983154 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 937 train loss: 0.9324571490287781 train acc: 0.9725000262260437 test loss: 1.030495047569275 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 938 train loss: 0.9322425723075867 train acc: 0.9725000262260437 test loss: 1.0276379585266113 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 939 train loss: 0.9322714805603027 train acc: 0.9725000262260437 test loss: 1.0261120796203613 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 940 train loss: 0.932254433631897 train acc: 0.9725000262260437 test loss: 1.027916431427002 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 941 train loss: 0.9323647022247314 train acc: 0.9725000262260437 test loss: 1.0301796197891235 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 942 train loss: 0.9322125315666199 train acc: 0.9725000262260437 test loss: 1.023504614830017 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 943 train loss: 0.9323489665985107 train acc: 0.9725000262260437 test loss: 1.0315816402435303 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 944 train loss: 0.932267427444458 train acc: 0.9725000262260437 test loss: 1.0234434604644775 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 945 train loss: 0.9322829246520996 train acc: 0.9725000262260437 test loss: 1.0253664255142212 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 946 train loss: 0.9321374297142029 train acc: 0.9725000262260437 test loss: 1.0202280282974243 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 947 train loss: 0.932344377040863 train acc: 0.9725000262260437 test loss: 1.025934100151062 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 948 train loss: 0.9321618676185608 train acc: 0.9725000262260437 test loss: 1.0245455503463745 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 949 train loss: 0.932101309299469 train acc: 0.9725000262260437 test loss: 1.0200815200805664 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 950 train loss: 0.9323484301567078 train acc: 0.9725000262260437 test loss: 1.0233855247497559 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 951 train loss: 0.9323150515556335 train acc: 0.9725000262260437 test loss: 1.024903416633606 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 952 train loss: 0.9323095679283142 train acc: 0.9725000262260437 test loss: 1.0272308588027954 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 953 train loss: 0.9321845769882202 train acc: 0.9725000262260437 test loss: 1.024335265159607 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 954 train loss: 0.9322461485862732 train acc: 0.9725000262260437 test loss: 1.0318541526794434 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 955 train loss: 0.9321156144142151 train acc: 0.9725000262260437 test loss: 1.0278409719467163 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 956 train loss: 0.9323214888572693 train acc: 0.9725000262260437 test loss: 1.0218220949172974 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 957 train loss: 0.9323417544364929 train acc: 0.9725000262260437 test loss: 1.0207098722457886 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 958 train loss: 0.9322426319122314 train acc: 0.9725000262260437 test loss: 1.0265597105026245 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 959 train loss: 0.9321861267089844 train acc: 0.9725000262260437 test loss: 1.023712396621704 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 960 train loss: 0.9321542382240295 train acc: 0.9725000262260437 test loss: 1.0262625217437744 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 961 train loss: 0.9321245551109314 train acc: 0.9725000262260437 test loss: 1.0374884605407715 best test loss: 1.0185256004333496 test acc: 0.8600000143051147\n",
      "Epoch 962 train loss: 0.9322036504745483 train acc: 0.9725000262260437 test loss: 1.0261629819869995 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 963 train loss: 0.9323558211326599 train acc: 0.9725000262260437 test loss: 1.0273550748825073 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 964 train loss: 0.9325739145278931 train acc: 0.9725000262260437 test loss: 1.024716854095459 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 965 train loss: 0.932141900062561 train acc: 0.9725000262260437 test loss: 1.0229452848434448 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 966 train loss: 0.9322741627693176 train acc: 0.9725000262260437 test loss: 1.0337133407592773 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 967 train loss: 0.932319700717926 train acc: 0.9725000262260437 test loss: 1.0287182331085205 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 968 train loss: 0.9322643280029297 train acc: 0.9725000262260437 test loss: 1.0262565612792969 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 969 train loss: 0.9321819543838501 train acc: 0.9725000262260437 test loss: 1.0242385864257812 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 970 train loss: 0.932363748550415 train acc: 0.9725000262260437 test loss: 1.0244022607803345 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 971 train loss: 0.9323506355285645 train acc: 0.9725000262260437 test loss: 1.027302622795105 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 972 train loss: 0.9322862029075623 train acc: 0.9725000262260437 test loss: 1.0338414907455444 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 973 train loss: 0.9322009086608887 train acc: 0.9725000262260437 test loss: 1.0315190553665161 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 974 train loss: 0.9323219060897827 train acc: 0.9725000262260437 test loss: 1.0281949043273926 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 975 train loss: 0.9323921799659729 train acc: 0.9725000262260437 test loss: 1.0227211713790894 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 976 train loss: 0.9322243332862854 train acc: 0.9725000262260437 test loss: 1.0319890975952148 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 977 train loss: 0.9322637915611267 train acc: 0.9725000262260437 test loss: 1.0290943384170532 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 978 train loss: 0.9325406551361084 train acc: 0.9725000262260437 test loss: 1.0291827917099 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 979 train loss: 0.9321913123130798 train acc: 0.9725000262260437 test loss: 1.022489309310913 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 980 train loss: 0.9322092533111572 train acc: 0.9725000262260437 test loss: 1.0318621397018433 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 981 train loss: 0.9321849346160889 train acc: 0.9725000262260437 test loss: 1.0233473777770996 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 982 train loss: 0.9321635365486145 train acc: 0.9725000262260437 test loss: 1.0252326726913452 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 983 train loss: 0.9321209788322449 train acc: 0.9725000262260437 test loss: 1.0291656255722046 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 984 train loss: 0.9321921467781067 train acc: 0.9725000262260437 test loss: 1.0192351341247559 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 985 train loss: 0.932300865650177 train acc: 0.9725000262260437 test loss: 1.0286883115768433 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 986 train loss: 0.9323242902755737 train acc: 0.9725000262260437 test loss: 1.0255908966064453 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 987 train loss: 0.9322370886802673 train acc: 0.9725000262260437 test loss: 1.0267982482910156 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 988 train loss: 0.9323170185089111 train acc: 0.9725000262260437 test loss: 1.0284581184387207 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 989 train loss: 0.9324417114257812 train acc: 0.9725000262260437 test loss: 1.028263807296753 best test loss: 1.0185256004333496 test acc: 0.8899999856948853\n",
      "Epoch 990 train loss: 0.9322676658630371 train acc: 0.9725000262260437 test loss: 1.0303443670272827 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 991 train loss: 0.9323441386222839 train acc: 0.9725000262260437 test loss: 1.0316585302352905 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 992 train loss: 0.9323375821113586 train acc: 0.9725000262260437 test loss: 1.0261632204055786 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 993 train loss: 0.9321939945220947 train acc: 0.9725000262260437 test loss: 1.027336597442627 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 994 train loss: 0.9323156476020813 train acc: 0.9725000262260437 test loss: 1.0286749601364136 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 995 train loss: 0.9323635101318359 train acc: 0.9725000262260437 test loss: 1.0233840942382812 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 996 train loss: 0.932273268699646 train acc: 0.9725000262260437 test loss: 1.0263240337371826 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 997 train loss: 0.9322161674499512 train acc: 0.9725000262260437 test loss: 1.0329724550247192 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 998 train loss: 0.9322078824043274 train acc: 0.9725000262260437 test loss: 1.0200281143188477 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 999 train loss: 0.9323614239692688 train acc: 0.9725000262260437 test loss: 1.030218243598938 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 1000 train loss: 0.9323005080223083 train acc: 0.9725000262260437 test loss: 1.0249099731445312 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 1001 train loss: 0.9321537613868713 train acc: 0.9725000262260437 test loss: 1.0311801433563232 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 1002 train loss: 0.9323363304138184 train acc: 0.9725000262260437 test loss: 1.0299731492996216 best test loss: 1.0185256004333496 test acc: 0.8700000047683716\n",
      "Epoch 1003 train loss: 0.9322081208229065 train acc: 0.9725000262260437 test loss: 1.0338815450668335 best test loss: 1.0185256004333496 test acc: 0.8600000143051147\n",
      "Epoch 1004 train loss: 0.9323760271072388 train acc: 0.9725000262260437 test loss: 1.0235567092895508 best test loss: 1.0185256004333496 test acc: 0.8799999952316284\n",
      "Epoch 1005 train loss: 0.9322720170021057 train acc: 0.9725000262260437 test loss: 1.0163825750350952 best test loss: 1.0163825750350952 test acc: 0.8899999856948853\n",
      "Epoch 1006 train loss: 0.9321902394294739 train acc: 0.9725000262260437 test loss: 1.032860517501831 best test loss: 1.0163825750350952 test acc: 0.8600000143051147\n",
      "Epoch 1007 train loss: 0.9322481751441956 train acc: 0.9725000262260437 test loss: 1.0240015983581543 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1008 train loss: 0.9321568012237549 train acc: 0.9725000262260437 test loss: 1.023679494857788 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1009 train loss: 0.9321841597557068 train acc: 0.9725000262260437 test loss: 1.0309464931488037 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1010 train loss: 0.9322429895401001 train acc: 0.9725000262260437 test loss: 1.031212568283081 best test loss: 1.0163825750350952 test acc: 0.8600000143051147\n",
      "Epoch 1011 train loss: 0.93230801820755 train acc: 0.9725000262260437 test loss: 1.0338462591171265 best test loss: 1.0163825750350952 test acc: 0.8600000143051147\n",
      "Epoch 1012 train loss: 0.9322971105575562 train acc: 0.9725000262260437 test loss: 1.023895263671875 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1013 train loss: 0.9321333169937134 train acc: 0.9725000262260437 test loss: 1.0215559005737305 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1014 train loss: 0.9321656227111816 train acc: 0.9725000262260437 test loss: 1.0401469469070435 best test loss: 1.0163825750350952 test acc: 0.8600000143051147\n",
      "Epoch 1015 train loss: 0.9323108196258545 train acc: 0.9725000262260437 test loss: 1.026410698890686 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1016 train loss: 0.9323326349258423 train acc: 0.9725000262260437 test loss: 1.0240075588226318 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1017 train loss: 0.9322406649589539 train acc: 0.9725000262260437 test loss: 1.0199702978134155 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1018 train loss: 0.9323861002922058 train acc: 0.9725000262260437 test loss: 1.0264376401901245 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1019 train loss: 0.9322534203529358 train acc: 0.9725000262260437 test loss: 1.0291556119918823 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1020 train loss: 0.9322711229324341 train acc: 0.9725000262260437 test loss: 1.0231857299804688 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1021 train loss: 0.9322601556777954 train acc: 0.9725000262260437 test loss: 1.021450161933899 best test loss: 1.0163825750350952 test acc: 0.8899999856948853\n",
      "Epoch 1022 train loss: 0.9323433637619019 train acc: 0.9725000262260437 test loss: 1.024705410003662 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1023 train loss: 0.9322670698165894 train acc: 0.9725000262260437 test loss: 1.028171420097351 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1024 train loss: 0.9321868419647217 train acc: 0.9725000262260437 test loss: 1.0296826362609863 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1025 train loss: 0.9323394894599915 train acc: 0.9725000262260437 test loss: 1.025049090385437 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1026 train loss: 0.9321111440658569 train acc: 0.9725000262260437 test loss: 1.0192660093307495 best test loss: 1.0163825750350952 test acc: 0.8899999856948853\n",
      "Epoch 1027 train loss: 0.932250440120697 train acc: 0.9725000262260437 test loss: 1.0230766534805298 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1028 train loss: 0.9321823716163635 train acc: 0.9725000262260437 test loss: 1.0198930501937866 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1029 train loss: 0.9323496222496033 train acc: 0.9725000262260437 test loss: 1.0297409296035767 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1030 train loss: 0.9323187470436096 train acc: 0.9725000262260437 test loss: 1.023593544960022 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1031 train loss: 0.9322199821472168 train acc: 0.9725000262260437 test loss: 1.0279830694198608 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1032 train loss: 0.9322379231452942 train acc: 0.9725000262260437 test loss: 1.0220195055007935 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1033 train loss: 0.9322956204414368 train acc: 0.9725000262260437 test loss: 1.023281455039978 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1034 train loss: 0.932335376739502 train acc: 0.9725000262260437 test loss: 1.0235389471054077 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1035 train loss: 0.9322502613067627 train acc: 0.9725000262260437 test loss: 1.0336544513702393 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1036 train loss: 0.9322664141654968 train acc: 0.9725000262260437 test loss: 1.0276129245758057 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1037 train loss: 0.9321500658988953 train acc: 0.9725000262260437 test loss: 1.0253005027770996 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1038 train loss: 0.9322456121444702 train acc: 0.9725000262260437 test loss: 1.026365876197815 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1039 train loss: 0.932146430015564 train acc: 0.9725000262260437 test loss: 1.032886028289795 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1040 train loss: 0.9322706460952759 train acc: 0.9725000262260437 test loss: 1.0230584144592285 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1041 train loss: 0.9322493076324463 train acc: 0.9725000262260437 test loss: 1.0261149406433105 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1042 train loss: 0.9320712089538574 train acc: 0.9725000262260437 test loss: 1.0233997106552124 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1043 train loss: 0.9321553111076355 train acc: 0.9725000262260437 test loss: 1.0216037034988403 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1044 train loss: 0.9321014285087585 train acc: 0.9725000262260437 test loss: 1.0256657600402832 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1045 train loss: 0.9322730898857117 train acc: 0.9725000262260437 test loss: 1.0282751321792603 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1046 train loss: 0.932155430316925 train acc: 0.9725000262260437 test loss: 1.0229755640029907 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1047 train loss: 0.9320926666259766 train acc: 0.9725000262260437 test loss: 1.0286630392074585 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1048 train loss: 0.9322474002838135 train acc: 0.9725000262260437 test loss: 1.024003267288208 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1049 train loss: 0.9322510361671448 train acc: 0.9725000262260437 test loss: 1.0272555351257324 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1050 train loss: 0.9322364926338196 train acc: 0.9725000262260437 test loss: 1.023667335510254 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1051 train loss: 0.9322752356529236 train acc: 0.9725000262260437 test loss: 1.0276157855987549 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1052 train loss: 0.932344377040863 train acc: 0.9725000262260437 test loss: 1.027714729309082 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1053 train loss: 0.9322736859321594 train acc: 0.9725000262260437 test loss: 1.0273395776748657 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1054 train loss: 0.9323304891586304 train acc: 0.9725000262260437 test loss: 1.0301282405853271 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1055 train loss: 0.9321447610855103 train acc: 0.9725000262260437 test loss: 1.0317237377166748 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1056 train loss: 0.9320861101150513 train acc: 0.9725000262260437 test loss: 1.0243728160858154 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1057 train loss: 0.9322079420089722 train acc: 0.9725000262260437 test loss: 1.0252108573913574 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1058 train loss: 0.9321852326393127 train acc: 0.9725000262260437 test loss: 1.020645022392273 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1059 train loss: 0.9321157932281494 train acc: 0.9725000262260437 test loss: 1.0276658535003662 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1060 train loss: 0.9322431683540344 train acc: 0.9725000262260437 test loss: 1.023707389831543 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1061 train loss: 0.932172417640686 train acc: 0.9725000262260437 test loss: 1.0289198160171509 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1062 train loss: 0.9321250915527344 train acc: 0.9725000262260437 test loss: 1.031222939491272 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1063 train loss: 0.9322794079780579 train acc: 0.9725000262260437 test loss: 1.0246237516403198 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1064 train loss: 0.9321144819259644 train acc: 0.9725000262260437 test loss: 1.0241649150848389 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1065 train loss: 0.932160496711731 train acc: 0.9725000262260437 test loss: 1.0262356996536255 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1066 train loss: 0.9321845173835754 train acc: 0.9725000262260437 test loss: 1.0255802869796753 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1067 train loss: 0.9323148131370544 train acc: 0.9725000262260437 test loss: 1.0330653190612793 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1068 train loss: 0.9322642683982849 train acc: 0.9725000262260437 test loss: 1.022857427597046 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1069 train loss: 0.9322270154953003 train acc: 0.9725000262260437 test loss: 1.0251315832138062 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1070 train loss: 0.9321399927139282 train acc: 0.9725000262260437 test loss: 1.0241196155548096 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1071 train loss: 0.9322288036346436 train acc: 0.9725000262260437 test loss: 1.0245784521102905 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1072 train loss: 0.9322466850280762 train acc: 0.9725000262260437 test loss: 1.0219050645828247 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1073 train loss: 0.9322154521942139 train acc: 0.9725000262260437 test loss: 1.024332880973816 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1074 train loss: 0.932158350944519 train acc: 0.9725000262260437 test loss: 1.0301073789596558 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1075 train loss: 0.9322869181632996 train acc: 0.9725000262260437 test loss: 1.0237689018249512 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1076 train loss: 0.9322363138198853 train acc: 0.9725000262260437 test loss: 1.0201873779296875 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1077 train loss: 0.9321693181991577 train acc: 0.9725000262260437 test loss: 1.0277084112167358 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1078 train loss: 0.9321689009666443 train acc: 0.9725000262260437 test loss: 1.0254392623901367 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1079 train loss: 0.9322658777236938 train acc: 0.9725000262260437 test loss: 1.0244146585464478 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1080 train loss: 0.9322223663330078 train acc: 0.9725000262260437 test loss: 1.0322686433792114 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1081 train loss: 0.9321224689483643 train acc: 0.9725000262260437 test loss: 1.023958444595337 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1082 train loss: 0.9323655962944031 train acc: 0.9725000262260437 test loss: 1.0255690813064575 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1083 train loss: 0.9322943091392517 train acc: 0.9725000262260437 test loss: 1.029538869857788 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1084 train loss: 0.9321984052658081 train acc: 0.9725000262260437 test loss: 1.0261166095733643 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1085 train loss: 0.9322185516357422 train acc: 0.9725000262260437 test loss: 1.0254586935043335 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1086 train loss: 0.9322887659072876 train acc: 0.9725000262260437 test loss: 1.0293517112731934 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1087 train loss: 0.9322792887687683 train acc: 0.9725000262260437 test loss: 1.0218099355697632 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1088 train loss: 0.9321119785308838 train acc: 0.9725000262260437 test loss: 1.0282127857208252 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1089 train loss: 0.9322772026062012 train acc: 0.9725000262260437 test loss: 1.0236371755599976 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1090 train loss: 0.9322079420089722 train acc: 0.9725000262260437 test loss: 1.0240944623947144 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1091 train loss: 0.9322227239608765 train acc: 0.9725000262260437 test loss: 1.026902437210083 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1092 train loss: 0.932416558265686 train acc: 0.9725000262260437 test loss: 1.0191900730133057 best test loss: 1.0163825750350952 test acc: 0.8899999856948853\n",
      "Epoch 1093 train loss: 0.932310938835144 train acc: 0.9725000262260437 test loss: 1.0266098976135254 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1094 train loss: 0.9321100115776062 train acc: 0.9725000262260437 test loss: 1.0250885486602783 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1095 train loss: 0.9322201013565063 train acc: 0.9725000262260437 test loss: 1.0249736309051514 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1096 train loss: 0.9322362542152405 train acc: 0.9725000262260437 test loss: 1.0313255786895752 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1097 train loss: 0.9320881366729736 train acc: 0.9725000262260437 test loss: 1.0229946374893188 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1098 train loss: 0.9321632981300354 train acc: 0.9725000262260437 test loss: 1.0281611680984497 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1099 train loss: 0.9323433041572571 train acc: 0.9725000262260437 test loss: 1.0242347717285156 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1100 train loss: 0.9323775768280029 train acc: 0.9725000262260437 test loss: 1.0348870754241943 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1101 train loss: 0.9323922991752625 train acc: 0.9725000262260437 test loss: 1.0355770587921143 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1102 train loss: 0.9321370124816895 train acc: 0.9725000262260437 test loss: 1.0224556922912598 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1103 train loss: 0.932115912437439 train acc: 0.9725000262260437 test loss: 1.0255787372589111 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1104 train loss: 0.932127833366394 train acc: 0.9725000262260437 test loss: 1.034145474433899 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1105 train loss: 0.9322820901870728 train acc: 0.9725000262260437 test loss: 1.034500241279602 best test loss: 1.0163825750350952 test acc: 0.8799999952316284\n",
      "Epoch 1106 train loss: 0.9322445392608643 train acc: 0.9725000262260437 test loss: 1.0259407758712769 best test loss: 1.0163825750350952 test acc: 0.8700000047683716\n",
      "Epoch 1107 train loss: 0.932273268699646 train acc: 0.9725000262260437 test loss: 1.0130972862243652 best test loss: 1.0130972862243652 test acc: 0.8999999761581421\n",
      "Epoch 1108 train loss: 0.9321006536483765 train acc: 0.9725000262260437 test loss: 1.0322071313858032 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1109 train loss: 0.9322986602783203 train acc: 0.9725000262260437 test loss: 1.0294405221939087 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1110 train loss: 0.9321828484535217 train acc: 0.9725000262260437 test loss: 1.033853530883789 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1111 train loss: 0.9322883486747742 train acc: 0.9725000262260437 test loss: 1.0349234342575073 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1112 train loss: 0.9324139952659607 train acc: 0.9725000262260437 test loss: 1.0258511304855347 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1113 train loss: 0.9321495890617371 train acc: 0.9725000262260437 test loss: 1.0275956392288208 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1114 train loss: 0.9322552680969238 train acc: 0.9725000262260437 test loss: 1.03077232837677 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1115 train loss: 0.9322949051856995 train acc: 0.9725000262260437 test loss: 1.0305284261703491 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1116 train loss: 0.9322277307510376 train acc: 0.9725000262260437 test loss: 1.027540922164917 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1117 train loss: 0.9321742057800293 train acc: 0.9725000262260437 test loss: 1.0281250476837158 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1118 train loss: 0.9322361946105957 train acc: 0.9725000262260437 test loss: 1.0282613039016724 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1119 train loss: 0.932118833065033 train acc: 0.9725000262260437 test loss: 1.0293787717819214 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1120 train loss: 0.9322556853294373 train acc: 0.9725000262260437 test loss: 1.0244896411895752 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1121 train loss: 0.9322153329849243 train acc: 0.9725000262260437 test loss: 1.0292601585388184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1122 train loss: 0.9322060942649841 train acc: 0.9725000262260437 test loss: 1.0326488018035889 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1123 train loss: 0.9322071671485901 train acc: 0.9725000262260437 test loss: 1.029239296913147 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1124 train loss: 0.9321386814117432 train acc: 0.9725000262260437 test loss: 1.0283339023590088 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1125 train loss: 0.9321236610412598 train acc: 0.9725000262260437 test loss: 1.0223591327667236 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1126 train loss: 0.9322201013565063 train acc: 0.9725000262260437 test loss: 1.0258471965789795 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1127 train loss: 0.9322496056556702 train acc: 0.9725000262260437 test loss: 1.0294008255004883 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1128 train loss: 0.9320888519287109 train acc: 0.9725000262260437 test loss: 1.0234384536743164 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1129 train loss: 0.9322030544281006 train acc: 0.9725000262260437 test loss: 1.0225622653961182 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1130 train loss: 0.9322010278701782 train acc: 0.9725000262260437 test loss: 1.0246202945709229 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1131 train loss: 0.9321957230567932 train acc: 0.9725000262260437 test loss: 1.0246341228485107 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1132 train loss: 0.932188093662262 train acc: 0.9725000262260437 test loss: 1.02342689037323 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1133 train loss: 0.9322509765625 train acc: 0.9725000262260437 test loss: 1.034840703010559 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1134 train loss: 0.9321749210357666 train acc: 0.9725000262260437 test loss: 1.0240238904953003 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1135 train loss: 0.9322514533996582 train acc: 0.9725000262260437 test loss: 1.0283485651016235 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1136 train loss: 0.9322487711906433 train acc: 0.9725000262260437 test loss: 1.0272002220153809 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1137 train loss: 0.9322397112846375 train acc: 0.9725000262260437 test loss: 1.0245230197906494 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1138 train loss: 0.9322178363800049 train acc: 0.9725000262260437 test loss: 1.029180884361267 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1139 train loss: 0.9320857524871826 train acc: 0.9725000262260437 test loss: 1.0310343503952026 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1140 train loss: 0.9322651028633118 train acc: 0.9725000262260437 test loss: 1.0245438814163208 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1141 train loss: 0.9323081374168396 train acc: 0.9725000262260437 test loss: 1.024954080581665 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1142 train loss: 0.9321540594100952 train acc: 0.9725000262260437 test loss: 1.0276625156402588 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1143 train loss: 0.9322433471679688 train acc: 0.9725000262260437 test loss: 1.0223133563995361 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1144 train loss: 0.9320825338363647 train acc: 0.9725000262260437 test loss: 1.0315526723861694 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1145 train loss: 0.9321561455726624 train acc: 0.9725000262260437 test loss: 1.028796672821045 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1146 train loss: 0.9322203993797302 train acc: 0.9725000262260437 test loss: 1.0265743732452393 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1147 train loss: 0.9321035742759705 train acc: 0.9725000262260437 test loss: 1.0274596214294434 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1148 train loss: 0.9323614239692688 train acc: 0.9725000262260437 test loss: 1.0281977653503418 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1149 train loss: 0.9322633147239685 train acc: 0.9725000262260437 test loss: 1.0311764478683472 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1150 train loss: 0.9322984218597412 train acc: 0.9725000262260437 test loss: 1.0304780006408691 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1151 train loss: 0.9323327541351318 train acc: 0.9725000262260437 test loss: 1.0281099081039429 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1152 train loss: 0.9320885539054871 train acc: 0.9725000262260437 test loss: 1.0283567905426025 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1153 train loss: 0.9322320818901062 train acc: 0.9725000262260437 test loss: 1.0220813751220703 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1154 train loss: 0.9322994351387024 train acc: 0.9725000262260437 test loss: 1.0240274667739868 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1155 train loss: 0.9323296546936035 train acc: 0.9725000262260437 test loss: 1.0321155786514282 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1156 train loss: 0.932289183139801 train acc: 0.9725000262260437 test loss: 1.0236507654190063 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1157 train loss: 0.9322541952133179 train acc: 0.9725000262260437 test loss: 1.0171847343444824 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1158 train loss: 0.9320946335792542 train acc: 0.9725000262260437 test loss: 1.0291045904159546 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1159 train loss: 0.9321613907814026 train acc: 0.9725000262260437 test loss: 1.019558310508728 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1160 train loss: 0.932328999042511 train acc: 0.9725000262260437 test loss: 1.0307024717330933 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1161 train loss: 0.9322349429130554 train acc: 0.9725000262260437 test loss: 1.0268847942352295 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1162 train loss: 0.9322030544281006 train acc: 0.9725000262260437 test loss: 1.0260493755340576 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1163 train loss: 0.9322928786277771 train acc: 0.9725000262260437 test loss: 1.0241830348968506 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1164 train loss: 0.932172417640686 train acc: 0.9725000262260437 test loss: 1.0231208801269531 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1165 train loss: 0.9321661591529846 train acc: 0.9725000262260437 test loss: 1.0268828868865967 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1166 train loss: 0.9321867227554321 train acc: 0.9725000262260437 test loss: 1.027747631072998 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1167 train loss: 0.9321768283843994 train acc: 0.9725000262260437 test loss: 1.0204768180847168 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1168 train loss: 0.93231201171875 train acc: 0.9725000262260437 test loss: 1.0229796171188354 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1169 train loss: 0.932081937789917 train acc: 0.9725000262260437 test loss: 1.0186113119125366 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1170 train loss: 0.932080864906311 train acc: 0.9725000262260437 test loss: 1.0248422622680664 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1171 train loss: 0.9322642683982849 train acc: 0.9725000262260437 test loss: 1.0281171798706055 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1172 train loss: 0.9321773052215576 train acc: 0.9725000262260437 test loss: 1.0265167951583862 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1173 train loss: 0.9328677654266357 train acc: 0.9725000262260437 test loss: 1.0321656465530396 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1174 train loss: 0.9320794939994812 train acc: 0.9725000262260437 test loss: 1.020913004875183 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1175 train loss: 0.9322307705879211 train acc: 0.9725000262260437 test loss: 1.0203555822372437 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1176 train loss: 0.9322856664657593 train acc: 0.9725000262260437 test loss: 1.02377450466156 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1177 train loss: 0.9323084354400635 train acc: 0.9725000262260437 test loss: 1.0241609811782837 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1178 train loss: 0.9322112798690796 train acc: 0.9725000262260437 test loss: 1.0243107080459595 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1179 train loss: 0.9321268200874329 train acc: 0.9725000262260437 test loss: 1.0272984504699707 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1180 train loss: 0.9322929978370667 train acc: 0.9725000262260437 test loss: 1.0221675634384155 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1181 train loss: 0.9323272705078125 train acc: 0.9725000262260437 test loss: 1.0256904363632202 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1182 train loss: 0.9322183132171631 train acc: 0.9725000262260437 test loss: 1.0259572267532349 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1183 train loss: 0.9322534799575806 train acc: 0.9725000262260437 test loss: 1.0219221115112305 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1184 train loss: 0.9322052597999573 train acc: 0.9725000262260437 test loss: 1.0217431783676147 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1185 train loss: 0.9322120547294617 train acc: 0.9725000262260437 test loss: 1.0232329368591309 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1186 train loss: 0.9323845505714417 train acc: 0.9725000262260437 test loss: 1.025087594985962 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1187 train loss: 0.932166576385498 train acc: 0.9725000262260437 test loss: 1.0265330076217651 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1188 train loss: 0.9322816729545593 train acc: 0.9725000262260437 test loss: 1.0226882696151733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1189 train loss: 0.9321637153625488 train acc: 0.9725000262260437 test loss: 1.0210108757019043 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1190 train loss: 0.9321062564849854 train acc: 0.9725000262260437 test loss: 1.0239894390106201 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1191 train loss: 0.932395875453949 train acc: 0.9725000262260437 test loss: 1.0223277807235718 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1192 train loss: 0.9320898652076721 train acc: 0.9725000262260437 test loss: 1.0248626470565796 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1193 train loss: 0.932213306427002 train acc: 0.9725000262260437 test loss: 1.0260661840438843 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1194 train loss: 0.9322740435600281 train acc: 0.9725000262260437 test loss: 1.0190412998199463 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1195 train loss: 0.932097852230072 train acc: 0.9725000262260437 test loss: 1.0427019596099854 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1196 train loss: 0.9323922991752625 train acc: 0.9725000262260437 test loss: 1.0246706008911133 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1197 train loss: 0.9322882294654846 train acc: 0.9725000262260437 test loss: 1.034263253211975 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1198 train loss: 0.9322190284729004 train acc: 0.9725000262260437 test loss: 1.0207507610321045 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1199 train loss: 0.9322952032089233 train acc: 0.9725000262260437 test loss: 1.0327011346817017 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1200 train loss: 0.9322830438613892 train acc: 0.9725000262260437 test loss: 1.0277986526489258 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1201 train loss: 0.9321302771568298 train acc: 0.9725000262260437 test loss: 1.0327831506729126 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1202 train loss: 0.9321414232254028 train acc: 0.9725000262260437 test loss: 1.0262935161590576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1203 train loss: 0.9321245551109314 train acc: 0.9725000262260437 test loss: 1.023726224899292 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1204 train loss: 0.9321994781494141 train acc: 0.9725000262260437 test loss: 1.0333195924758911 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1205 train loss: 0.9321567416191101 train acc: 0.9725000262260437 test loss: 1.0251376628875732 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1206 train loss: 0.9322683811187744 train acc: 0.9725000262260437 test loss: 1.0236353874206543 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1207 train loss: 0.9321893453598022 train acc: 0.9725000262260437 test loss: 1.0234085321426392 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1208 train loss: 0.9322713613510132 train acc: 0.9725000262260437 test loss: 1.030143141746521 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1209 train loss: 0.9322984218597412 train acc: 0.9725000262260437 test loss: 1.0235023498535156 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1210 train loss: 0.9322358965873718 train acc: 0.9725000262260437 test loss: 1.0240933895111084 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1211 train loss: 0.932114839553833 train acc: 0.9725000262260437 test loss: 1.027660846710205 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1212 train loss: 0.9322183132171631 train acc: 0.9725000262260437 test loss: 1.0278226137161255 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1213 train loss: 0.9322677850723267 train acc: 0.9725000262260437 test loss: 1.0261553525924683 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1214 train loss: 0.9321156144142151 train acc: 0.9725000262260437 test loss: 1.0356571674346924 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1215 train loss: 0.9321062564849854 train acc: 0.9725000262260437 test loss: 1.0249431133270264 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1216 train loss: 0.9320929646492004 train acc: 0.9725000262260437 test loss: 1.026868462562561 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1217 train loss: 0.9322672486305237 train acc: 0.9725000262260437 test loss: 1.0242427587509155 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1218 train loss: 0.9321362972259521 train acc: 0.9725000262260437 test loss: 1.0157818794250488 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1219 train loss: 0.9322327375411987 train acc: 0.9725000262260437 test loss: 1.0234894752502441 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1220 train loss: 0.9321014881134033 train acc: 0.9725000262260437 test loss: 1.0242279767990112 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1221 train loss: 0.9321373105049133 train acc: 0.9725000262260437 test loss: 1.02498459815979 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1222 train loss: 0.932332456111908 train acc: 0.9725000262260437 test loss: 1.026364803314209 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1223 train loss: 0.9321640133857727 train acc: 0.9725000262260437 test loss: 1.037636160850525 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1224 train loss: 0.9321847558021545 train acc: 0.9725000262260437 test loss: 1.021213173866272 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1225 train loss: 0.932197093963623 train acc: 0.9725000262260437 test loss: 1.035636067390442 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1226 train loss: 0.9323171973228455 train acc: 0.9725000262260437 test loss: 1.0232913494110107 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1227 train loss: 0.9323018789291382 train acc: 0.9725000262260437 test loss: 1.024298071861267 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1228 train loss: 0.9322469234466553 train acc: 0.9725000262260437 test loss: 1.0254634618759155 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1229 train loss: 0.9321062564849854 train acc: 0.9725000262260437 test loss: 1.0283229351043701 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1230 train loss: 0.9321547150611877 train acc: 0.9725000262260437 test loss: 1.0158953666687012 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1231 train loss: 0.9320820569992065 train acc: 0.9725000262260437 test loss: 1.0265790224075317 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1232 train loss: 0.932151198387146 train acc: 0.9725000262260437 test loss: 1.0230060815811157 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1233 train loss: 0.9321275353431702 train acc: 0.9725000262260437 test loss: 1.0212162733078003 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1234 train loss: 0.9323458671569824 train acc: 0.9725000262260437 test loss: 1.0302650928497314 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1235 train loss: 0.9321341514587402 train acc: 0.9725000262260437 test loss: 1.0279209613800049 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1236 train loss: 0.932383120059967 train acc: 0.9725000262260437 test loss: 1.0245471000671387 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1237 train loss: 0.932371973991394 train acc: 0.9725000262260437 test loss: 1.0223002433776855 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1238 train loss: 0.9322476387023926 train acc: 0.9725000262260437 test loss: 1.0256820917129517 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1239 train loss: 0.9322060942649841 train acc: 0.9725000262260437 test loss: 1.021240472793579 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1240 train loss: 0.9320651292800903 train acc: 0.9725000262260437 test loss: 1.0201681852340698 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1241 train loss: 0.9321048855781555 train acc: 0.9725000262260437 test loss: 1.0249478816986084 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1242 train loss: 0.9321725368499756 train acc: 0.9725000262260437 test loss: 1.0176854133605957 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1243 train loss: 0.9321897029876709 train acc: 0.9725000262260437 test loss: 1.0265727043151855 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1244 train loss: 0.9321373105049133 train acc: 0.9725000262260437 test loss: 1.0332781076431274 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1245 train loss: 0.9323039054870605 train acc: 0.9725000262260437 test loss: 1.0301473140716553 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1246 train loss: 0.9320796728134155 train acc: 0.9725000262260437 test loss: 1.0228571891784668 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1247 train loss: 0.9323338270187378 train acc: 0.9725000262260437 test loss: 1.0244951248168945 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1248 train loss: 0.9321814179420471 train acc: 0.9725000262260437 test loss: 1.0289417505264282 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1249 train loss: 0.9322522878646851 train acc: 0.9725000262260437 test loss: 1.0223000049591064 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1250 train loss: 0.9322684407234192 train acc: 0.9725000262260437 test loss: 1.0300021171569824 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1251 train loss: 0.9322172403335571 train acc: 0.9725000262260437 test loss: 1.0253950357437134 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1252 train loss: 0.932168185710907 train acc: 0.9725000262260437 test loss: 1.0283352136611938 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1253 train loss: 0.9321138858795166 train acc: 0.9725000262260437 test loss: 1.0419992208480835 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1254 train loss: 0.9323092103004456 train acc: 0.9725000262260437 test loss: 1.024278163909912 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1255 train loss: 0.932090699672699 train acc: 0.9725000262260437 test loss: 1.0238593816757202 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1256 train loss: 0.9322014451026917 train acc: 0.9725000262260437 test loss: 1.0340847969055176 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1257 train loss: 0.9323352575302124 train acc: 0.9725000262260437 test loss: 1.024011254310608 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1258 train loss: 0.9320871233940125 train acc: 0.9725000262260437 test loss: 1.0261348485946655 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1259 train loss: 0.9321656823158264 train acc: 0.9725000262260437 test loss: 1.0232653617858887 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1260 train loss: 0.9321646094322205 train acc: 0.9725000262260437 test loss: 1.0234979391098022 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1261 train loss: 0.9321613907814026 train acc: 0.9725000262260437 test loss: 1.0224621295928955 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1262 train loss: 0.9321652054786682 train acc: 0.9725000262260437 test loss: 1.024221658706665 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1263 train loss: 0.9321504831314087 train acc: 0.9725000262260437 test loss: 1.0267466306686401 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1264 train loss: 0.932259738445282 train acc: 0.9725000262260437 test loss: 1.0393537282943726 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1265 train loss: 0.9321379065513611 train acc: 0.9725000262260437 test loss: 1.0313491821289062 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1266 train loss: 0.9323277473449707 train acc: 0.9725000262260437 test loss: 1.0264490842819214 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1267 train loss: 0.9322119355201721 train acc: 0.9725000262260437 test loss: 1.0226306915283203 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1268 train loss: 0.9322079420089722 train acc: 0.9725000262260437 test loss: 1.0246297121047974 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1269 train loss: 0.9320972561836243 train acc: 0.9725000262260437 test loss: 1.0247673988342285 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1270 train loss: 0.932193398475647 train acc: 0.9725000262260437 test loss: 1.0182814598083496 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1271 train loss: 0.9322157502174377 train acc: 0.9725000262260437 test loss: 1.0223872661590576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1272 train loss: 0.9320926666259766 train acc: 0.9725000262260437 test loss: 1.0249884128570557 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1273 train loss: 0.9321991801261902 train acc: 0.9725000262260437 test loss: 1.0267748832702637 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1274 train loss: 0.9321801066398621 train acc: 0.9725000262260437 test loss: 1.0244414806365967 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1275 train loss: 0.9321324229240417 train acc: 0.9725000262260437 test loss: 1.0243768692016602 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1276 train loss: 0.9322847723960876 train acc: 0.9725000262260437 test loss: 1.027646541595459 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1277 train loss: 0.9322542548179626 train acc: 0.9725000262260437 test loss: 1.028774380683899 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1278 train loss: 0.9321568012237549 train acc: 0.9725000262260437 test loss: 1.028435468673706 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1279 train loss: 0.9323626756668091 train acc: 0.9725000262260437 test loss: 1.030449390411377 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1280 train loss: 0.9323370456695557 train acc: 0.9725000262260437 test loss: 1.0250732898712158 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1281 train loss: 0.9322221279144287 train acc: 0.9725000262260437 test loss: 1.0320168733596802 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1282 train loss: 0.93215012550354 train acc: 0.9725000262260437 test loss: 1.0234954357147217 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1283 train loss: 0.9323030114173889 train acc: 0.9725000262260437 test loss: 1.022306203842163 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1284 train loss: 0.932258129119873 train acc: 0.9725000262260437 test loss: 1.0306470394134521 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1285 train loss: 0.9322319626808167 train acc: 0.9725000262260437 test loss: 1.0257139205932617 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1286 train loss: 0.9321316480636597 train acc: 0.9725000262260437 test loss: 1.0203155279159546 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1287 train loss: 0.9323161244392395 train acc: 0.9725000262260437 test loss: 1.0237740278244019 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1288 train loss: 0.9321581125259399 train acc: 0.9725000262260437 test loss: 1.0261192321777344 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1289 train loss: 0.9321126341819763 train acc: 0.9725000262260437 test loss: 1.0203921794891357 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1290 train loss: 0.9322608709335327 train acc: 0.9725000262260437 test loss: 1.025597095489502 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1291 train loss: 0.9321675300598145 train acc: 0.9725000262260437 test loss: 1.0364357233047485 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1292 train loss: 0.9322535991668701 train acc: 0.9725000262260437 test loss: 1.0249271392822266 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1293 train loss: 0.9320889115333557 train acc: 0.9725000262260437 test loss: 1.0240719318389893 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1294 train loss: 0.9321885704994202 train acc: 0.9725000262260437 test loss: 1.0303606986999512 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1295 train loss: 0.9323232769966125 train acc: 0.9725000262260437 test loss: 1.0241937637329102 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1296 train loss: 0.9321591258049011 train acc: 0.9725000262260437 test loss: 1.0279372930526733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1297 train loss: 0.9322612881660461 train acc: 0.9725000262260437 test loss: 1.0239741802215576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1298 train loss: 0.9321292042732239 train acc: 0.9725000262260437 test loss: 1.0223805904388428 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1299 train loss: 0.9322877526283264 train acc: 0.9725000262260437 test loss: 1.0247161388397217 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1300 train loss: 0.9321765899658203 train acc: 0.9725000262260437 test loss: 1.0213866233825684 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1301 train loss: 0.9321914911270142 train acc: 0.9725000262260437 test loss: 1.0220965147018433 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1302 train loss: 0.9321244955062866 train acc: 0.9725000262260437 test loss: 1.0263516902923584 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1303 train loss: 0.9323541522026062 train acc: 0.9725000262260437 test loss: 1.0293656587600708 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1304 train loss: 0.9321615099906921 train acc: 0.9725000262260437 test loss: 1.0309185981750488 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1305 train loss: 0.9322108626365662 train acc: 0.9725000262260437 test loss: 1.0251659154891968 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1306 train loss: 0.9323148131370544 train acc: 0.9725000262260437 test loss: 1.024266242980957 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1307 train loss: 0.9321005940437317 train acc: 0.9725000262260437 test loss: 1.0326544046401978 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1308 train loss: 0.9322730898857117 train acc: 0.9725000262260437 test loss: 1.022921085357666 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1309 train loss: 0.9321951866149902 train acc: 0.9725000262260437 test loss: 1.0255738496780396 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1310 train loss: 0.9322941303253174 train acc: 0.9725000262260437 test loss: 1.027574896812439 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1311 train loss: 0.9322236776351929 train acc: 0.9725000262260437 test loss: 1.0208121538162231 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1312 train loss: 0.9321640729904175 train acc: 0.9725000262260437 test loss: 1.0233250856399536 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1313 train loss: 0.9323352575302124 train acc: 0.9725000262260437 test loss: 1.0217700004577637 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1314 train loss: 0.9322792887687683 train acc: 0.9725000262260437 test loss: 1.027184247970581 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1315 train loss: 0.9322358965873718 train acc: 0.9725000262260437 test loss: 1.0247902870178223 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1316 train loss: 0.9321915507316589 train acc: 0.9725000262260437 test loss: 1.0184450149536133 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1317 train loss: 0.9322748780250549 train acc: 0.9725000262260437 test loss: 1.0231283903121948 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1318 train loss: 0.9321973919868469 train acc: 0.9725000262260437 test loss: 1.0233922004699707 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1319 train loss: 0.9323267936706543 train acc: 0.9725000262260437 test loss: 1.0244275331497192 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1320 train loss: 0.9322044253349304 train acc: 0.9725000262260437 test loss: 1.0272835493087769 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1321 train loss: 0.932107150554657 train acc: 0.9725000262260437 test loss: 1.0285265445709229 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1322 train loss: 0.932384192943573 train acc: 0.9725000262260437 test loss: 1.0351935625076294 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1323 train loss: 0.9322445392608643 train acc: 0.9725000262260437 test loss: 1.034753680229187 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1324 train loss: 0.9323243498802185 train acc: 0.9725000262260437 test loss: 1.0238733291625977 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1325 train loss: 0.9322190880775452 train acc: 0.9725000262260437 test loss: 1.0275723934173584 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1326 train loss: 0.9325452446937561 train acc: 0.9725000262260437 test loss: 1.023020625114441 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1327 train loss: 0.9323539137840271 train acc: 0.9725000262260437 test loss: 1.0286054611206055 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1328 train loss: 0.9322187900543213 train acc: 0.9725000262260437 test loss: 1.021475911140442 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1329 train loss: 0.9322697520256042 train acc: 0.9725000262260437 test loss: 1.0226800441741943 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1330 train loss: 0.9323864579200745 train acc: 0.9725000262260437 test loss: 1.028133511543274 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1331 train loss: 0.9322322010993958 train acc: 0.9725000262260437 test loss: 1.0374385118484497 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1332 train loss: 0.9323423504829407 train acc: 0.9725000262260437 test loss: 1.0297335386276245 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1333 train loss: 0.9323132038116455 train acc: 0.9725000262260437 test loss: 1.0282005071640015 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1334 train loss: 0.9322261214256287 train acc: 0.9725000262260437 test loss: 1.0197197198867798 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1335 train loss: 0.9320654273033142 train acc: 0.9725000262260437 test loss: 1.0222817659378052 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1336 train loss: 0.9321300387382507 train acc: 0.9725000262260437 test loss: 1.0265262126922607 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1337 train loss: 0.9320899248123169 train acc: 0.9725000262260437 test loss: 1.0239626169204712 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1338 train loss: 0.9320719838142395 train acc: 0.9725000262260437 test loss: 1.0254642963409424 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1339 train loss: 0.9322327971458435 train acc: 0.9725000262260437 test loss: 1.0242778062820435 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1340 train loss: 0.9320919513702393 train acc: 0.9725000262260437 test loss: 1.0279605388641357 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1341 train loss: 0.9320765733718872 train acc: 0.9725000262260437 test loss: 1.025621771812439 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1342 train loss: 0.9321391582489014 train acc: 0.9725000262260437 test loss: 1.0363280773162842 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1343 train loss: 0.9324365854263306 train acc: 0.9725000262260437 test loss: 1.0225979089736938 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1344 train loss: 0.932242751121521 train acc: 0.9725000262260437 test loss: 1.0261324644088745 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1345 train loss: 0.9323092699050903 train acc: 0.9725000262260437 test loss: 1.0268574953079224 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1346 train loss: 0.9322472214698792 train acc: 0.9725000262260437 test loss: 1.0286405086517334 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1347 train loss: 0.9322242140769958 train acc: 0.9725000262260437 test loss: 1.0337402820587158 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1348 train loss: 0.932297945022583 train acc: 0.9725000262260437 test loss: 1.0212090015411377 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1349 train loss: 0.932233989238739 train acc: 0.9725000262260437 test loss: 1.0375808477401733 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1350 train loss: 0.9322397112846375 train acc: 0.9725000262260437 test loss: 1.0315436124801636 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1351 train loss: 0.9321935176849365 train acc: 0.9725000262260437 test loss: 1.0261918306350708 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1352 train loss: 0.9321927428245544 train acc: 0.9725000262260437 test loss: 1.0190967321395874 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1353 train loss: 0.93211829662323 train acc: 0.9725000262260437 test loss: 1.0297659635543823 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1354 train loss: 0.932098388671875 train acc: 0.9725000262260437 test loss: 1.0189237594604492 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1355 train loss: 0.9321354031562805 train acc: 0.9725000262260437 test loss: 1.0282970666885376 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1356 train loss: 0.9321146607398987 train acc: 0.9725000262260437 test loss: 1.024641990661621 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1357 train loss: 0.9323083758354187 train acc: 0.9725000262260437 test loss: 1.0359259843826294 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1358 train loss: 0.9322003126144409 train acc: 0.9725000262260437 test loss: 1.0286240577697754 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1359 train loss: 0.9322648644447327 train acc: 0.9725000262260437 test loss: 1.024131178855896 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1360 train loss: 0.9323878288269043 train acc: 0.9725000262260437 test loss: 1.028357744216919 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1361 train loss: 0.9322558641433716 train acc: 0.9725000262260437 test loss: 1.0264177322387695 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1362 train loss: 0.9322071671485901 train acc: 0.9725000262260437 test loss: 1.0266671180725098 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1363 train loss: 0.9322763681411743 train acc: 0.9725000262260437 test loss: 1.0217273235321045 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1364 train loss: 0.932095468044281 train acc: 0.9725000262260437 test loss: 1.0238022804260254 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1365 train loss: 0.9322084188461304 train acc: 0.9725000262260437 test loss: 1.0233112573623657 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1366 train loss: 0.9323111176490784 train acc: 0.9725000262260437 test loss: 1.030261516571045 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1367 train loss: 0.9322304725646973 train acc: 0.9725000262260437 test loss: 1.0209524631500244 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1368 train loss: 0.9320692420005798 train acc: 0.9725000262260437 test loss: 1.0241340398788452 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1369 train loss: 0.9321061968803406 train acc: 0.9725000262260437 test loss: 1.026962161064148 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1370 train loss: 0.932202160358429 train acc: 0.9725000262260437 test loss: 1.0311412811279297 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1371 train loss: 0.9322934150695801 train acc: 0.9725000262260437 test loss: 1.0244024991989136 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1372 train loss: 0.9320887923240662 train acc: 0.9725000262260437 test loss: 1.028700590133667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1373 train loss: 0.9322469234466553 train acc: 0.9725000262260437 test loss: 1.0240533351898193 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1374 train loss: 0.9322518706321716 train acc: 0.9725000262260437 test loss: 1.039943814277649 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1375 train loss: 0.9322935342788696 train acc: 0.9725000262260437 test loss: 1.0304030179977417 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1376 train loss: 0.9321643710136414 train acc: 0.9725000262260437 test loss: 1.0311837196350098 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1377 train loss: 0.9322292804718018 train acc: 0.9725000262260437 test loss: 1.023493766784668 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1378 train loss: 0.9323412179946899 train acc: 0.9725000262260437 test loss: 1.0224859714508057 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1379 train loss: 0.9322596192359924 train acc: 0.9725000262260437 test loss: 1.021317481994629 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1380 train loss: 0.9320846796035767 train acc: 0.9725000262260437 test loss: 1.0272310972213745 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1381 train loss: 0.932066023349762 train acc: 0.9725000262260437 test loss: 1.0418081283569336 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 1382 train loss: 0.9321852326393127 train acc: 0.9725000262260437 test loss: 1.018502950668335 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1383 train loss: 0.9322094917297363 train acc: 0.9725000262260437 test loss: 1.0216635465621948 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1384 train loss: 0.9322471022605896 train acc: 0.9725000262260437 test loss: 1.0217918157577515 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1385 train loss: 0.9324095845222473 train acc: 0.9725000262260437 test loss: 1.025142788887024 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1386 train loss: 0.9320553541183472 train acc: 0.9725000262260437 test loss: 1.0291038751602173 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1387 train loss: 0.9322507977485657 train acc: 0.9725000262260437 test loss: 1.025545597076416 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1388 train loss: 0.9321817755699158 train acc: 0.9725000262260437 test loss: 1.0230133533477783 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1389 train loss: 0.9321845173835754 train acc: 0.9725000262260437 test loss: 1.0308566093444824 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1390 train loss: 0.9321584105491638 train acc: 0.9725000262260437 test loss: 1.0309865474700928 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1391 train loss: 0.9322254061698914 train acc: 0.9725000262260437 test loss: 1.0301969051361084 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1392 train loss: 0.9321812391281128 train acc: 0.9725000262260437 test loss: 1.0303771495819092 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1393 train loss: 0.9322490692138672 train acc: 0.9725000262260437 test loss: 1.025938630104065 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1394 train loss: 0.9320942759513855 train acc: 0.9725000262260437 test loss: 1.0226314067840576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1395 train loss: 0.9323363304138184 train acc: 0.9725000262260437 test loss: 1.0292288064956665 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1396 train loss: 0.9321548938751221 train acc: 0.9725000262260437 test loss: 1.0235650539398193 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1397 train loss: 0.9321732521057129 train acc: 0.9725000262260437 test loss: 1.021241307258606 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1398 train loss: 0.9320840239524841 train acc: 0.9725000262260437 test loss: 1.0311799049377441 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1399 train loss: 0.9323288202285767 train acc: 0.9725000262260437 test loss: 1.0275920629501343 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1400 train loss: 0.932274341583252 train acc: 0.9725000262260437 test loss: 1.025115728378296 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1401 train loss: 0.9320932030677795 train acc: 0.9725000262260437 test loss: 1.0274219512939453 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1402 train loss: 0.9321889281272888 train acc: 0.9725000262260437 test loss: 1.025476098060608 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1403 train loss: 0.9321637749671936 train acc: 0.9725000262260437 test loss: 1.037443995475769 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1404 train loss: 0.9323503971099854 train acc: 0.9725000262260437 test loss: 1.0250855684280396 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1405 train loss: 0.9320801496505737 train acc: 0.9725000262260437 test loss: 1.0269831418991089 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1406 train loss: 0.9321942925453186 train acc: 0.9725000262260437 test loss: 1.020609974861145 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1407 train loss: 0.932238757610321 train acc: 0.9725000262260437 test loss: 1.0286054611206055 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1408 train loss: 0.9322054982185364 train acc: 0.9725000262260437 test loss: 1.0221490859985352 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1409 train loss: 0.9321197271347046 train acc: 0.9725000262260437 test loss: 1.0237250328063965 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1410 train loss: 0.932080864906311 train acc: 0.9725000262260437 test loss: 1.0347391366958618 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1411 train loss: 0.9322389960289001 train acc: 0.9725000262260437 test loss: 1.022665023803711 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1412 train loss: 0.9321907162666321 train acc: 0.9725000262260437 test loss: 1.025108814239502 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1413 train loss: 0.9322025775909424 train acc: 0.9725000262260437 test loss: 1.0222749710083008 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1414 train loss: 0.932238757610321 train acc: 0.9725000262260437 test loss: 1.0274745225906372 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1415 train loss: 0.9322937726974487 train acc: 0.9725000262260437 test loss: 1.0274983644485474 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1416 train loss: 0.9320900440216064 train acc: 0.9725000262260437 test loss: 1.0291765928268433 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1417 train loss: 0.932280957698822 train acc: 0.9725000262260437 test loss: 1.0316317081451416 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1418 train loss: 0.9322057962417603 train acc: 0.9725000262260437 test loss: 1.0250388383865356 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1419 train loss: 0.9321363568305969 train acc: 0.9725000262260437 test loss: 1.0290889739990234 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1420 train loss: 0.9321616888046265 train acc: 0.9725000262260437 test loss: 1.0288184881210327 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1421 train loss: 0.9326741099357605 train acc: 0.9725000262260437 test loss: 1.0271812677383423 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1422 train loss: 0.9320598840713501 train acc: 0.9725000262260437 test loss: 1.0256983041763306 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1423 train loss: 0.9321494102478027 train acc: 0.9725000262260437 test loss: 1.0245789289474487 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1424 train loss: 0.9321140050888062 train acc: 0.9725000262260437 test loss: 1.0245869159698486 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1425 train loss: 0.9324060678482056 train acc: 0.9725000262260437 test loss: 1.0254191160202026 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1426 train loss: 0.9321834444999695 train acc: 0.9725000262260437 test loss: 1.0430595874786377 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1427 train loss: 0.9320523142814636 train acc: 0.9725000262260437 test loss: 1.0300893783569336 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1428 train loss: 0.9321984052658081 train acc: 0.9725000262260437 test loss: 1.0224601030349731 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1429 train loss: 0.9322564601898193 train acc: 0.9725000262260437 test loss: 1.024227261543274 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1430 train loss: 0.932138979434967 train acc: 0.9725000262260437 test loss: 1.0216631889343262 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1431 train loss: 0.9321684241294861 train acc: 0.9725000262260437 test loss: 1.0231602191925049 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1432 train loss: 0.9322062730789185 train acc: 0.9725000262260437 test loss: 1.0207637548446655 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1433 train loss: 0.9322400093078613 train acc: 0.9725000262260437 test loss: 1.0313369035720825 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1434 train loss: 0.9323034882545471 train acc: 0.9725000262260437 test loss: 1.0238876342773438 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1435 train loss: 0.9321634769439697 train acc: 0.9725000262260437 test loss: 1.0370632410049438 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1436 train loss: 0.9322534203529358 train acc: 0.9725000262260437 test loss: 1.021315336227417 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1437 train loss: 0.9322454929351807 train acc: 0.9725000262260437 test loss: 1.0227230787277222 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1438 train loss: 0.9322636127471924 train acc: 0.9725000262260437 test loss: 1.024001955986023 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1439 train loss: 0.9323021173477173 train acc: 0.9725000262260437 test loss: 1.0259742736816406 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1440 train loss: 0.9321504831314087 train acc: 0.9725000262260437 test loss: 1.0307929515838623 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1441 train loss: 0.9322376847267151 train acc: 0.9725000262260437 test loss: 1.0253032445907593 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1442 train loss: 0.9322095513343811 train acc: 0.9725000262260437 test loss: 1.0264354944229126 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1443 train loss: 0.9322424530982971 train acc: 0.9725000262260437 test loss: 1.0246695280075073 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1444 train loss: 0.9323423504829407 train acc: 0.9725000262260437 test loss: 1.026401400566101 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1445 train loss: 0.9322673082351685 train acc: 0.9725000262260437 test loss: 1.0244804620742798 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1446 train loss: 0.9322366118431091 train acc: 0.9725000262260437 test loss: 1.0245476961135864 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1447 train loss: 0.9322613477706909 train acc: 0.9725000262260437 test loss: 1.0261797904968262 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1448 train loss: 0.9322801232337952 train acc: 0.9725000262260437 test loss: 1.025262713432312 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1449 train loss: 0.9322488903999329 train acc: 0.9725000262260437 test loss: 1.0265896320343018 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1450 train loss: 0.9322414398193359 train acc: 0.9725000262260437 test loss: 1.0297605991363525 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1451 train loss: 0.9320834279060364 train acc: 0.9725000262260437 test loss: 1.0190194845199585 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1452 train loss: 0.9322695136070251 train acc: 0.9725000262260437 test loss: 1.0178613662719727 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1453 train loss: 0.9321848154067993 train acc: 0.9725000262260437 test loss: 1.0282474756240845 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1454 train loss: 0.9321931600570679 train acc: 0.9725000262260437 test loss: 1.031816840171814 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1455 train loss: 0.9321064949035645 train acc: 0.9725000262260437 test loss: 1.024549126625061 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1456 train loss: 0.932159423828125 train acc: 0.9725000262260437 test loss: 1.0248574018478394 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1457 train loss: 0.9321954250335693 train acc: 0.9725000262260437 test loss: 1.0283836126327515 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1458 train loss: 0.932279109954834 train acc: 0.9725000262260437 test loss: 1.0217158794403076 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1459 train loss: 0.9322177171707153 train acc: 0.9725000262260437 test loss: 1.0241832733154297 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1460 train loss: 0.9321994185447693 train acc: 0.9725000262260437 test loss: 1.0228348970413208 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1461 train loss: 0.9321709275245667 train acc: 0.9725000262260437 test loss: 1.028071403503418 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1462 train loss: 0.9322444200515747 train acc: 0.9725000262260437 test loss: 1.0208871364593506 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1463 train loss: 0.9322329163551331 train acc: 0.9725000262260437 test loss: 1.0309627056121826 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1464 train loss: 0.9321790337562561 train acc: 0.9725000262260437 test loss: 1.0208642482757568 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1465 train loss: 0.9322251081466675 train acc: 0.9725000262260437 test loss: 1.0298562049865723 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1466 train loss: 0.9320819973945618 train acc: 0.9725000262260437 test loss: 1.0234849452972412 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1467 train loss: 0.9322934150695801 train acc: 0.9725000262260437 test loss: 1.028538703918457 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1468 train loss: 0.9321081042289734 train acc: 0.9725000262260437 test loss: 1.0260028839111328 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1469 train loss: 0.9320642948150635 train acc: 0.9725000262260437 test loss: 1.0321826934814453 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1470 train loss: 0.9321815371513367 train acc: 0.9725000262260437 test loss: 1.0249313116073608 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1471 train loss: 0.932268500328064 train acc: 0.9725000262260437 test loss: 1.0264062881469727 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1472 train loss: 0.9321807026863098 train acc: 0.9725000262260437 test loss: 1.031135082244873 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1473 train loss: 0.9321182370185852 train acc: 0.9725000262260437 test loss: 1.0263984203338623 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1474 train loss: 0.9322332143783569 train acc: 0.9725000262260437 test loss: 1.0234010219573975 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1475 train loss: 0.9320815205574036 train acc: 0.9725000262260437 test loss: 1.0230942964553833 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1476 train loss: 0.9321984648704529 train acc: 0.9725000262260437 test loss: 1.0248613357543945 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1477 train loss: 0.9322068691253662 train acc: 0.9725000262260437 test loss: 1.0310100317001343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1478 train loss: 0.932156503200531 train acc: 0.9725000262260437 test loss: 1.0257376432418823 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1479 train loss: 0.932197093963623 train acc: 0.9725000262260437 test loss: 1.0222285985946655 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1480 train loss: 0.9322178363800049 train acc: 0.9725000262260437 test loss: 1.0244158506393433 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1481 train loss: 0.9322393536567688 train acc: 0.9725000262260437 test loss: 1.024597406387329 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1482 train loss: 0.9320871829986572 train acc: 0.9725000262260437 test loss: 1.0225361585617065 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1483 train loss: 0.932151198387146 train acc: 0.9725000262260437 test loss: 1.0210049152374268 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1484 train loss: 0.9321087598800659 train acc: 0.9725000262260437 test loss: 1.0267609357833862 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1485 train loss: 0.9322156310081482 train acc: 0.9725000262260437 test loss: 1.0284477472305298 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1486 train loss: 0.9322001934051514 train acc: 0.9725000262260437 test loss: 1.0257612466812134 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1487 train loss: 0.9323064684867859 train acc: 0.9725000262260437 test loss: 1.0277395248413086 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1488 train loss: 0.9323463439941406 train acc: 0.9725000262260437 test loss: 1.0260756015777588 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1489 train loss: 0.9322375655174255 train acc: 0.9725000262260437 test loss: 1.026755690574646 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1490 train loss: 0.9321541786193848 train acc: 0.9725000262260437 test loss: 1.0372686386108398 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1491 train loss: 0.9323062896728516 train acc: 0.9725000262260437 test loss: 1.0314844846725464 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1492 train loss: 0.932204008102417 train acc: 0.9725000262260437 test loss: 1.02778959274292 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1493 train loss: 0.9321871399879456 train acc: 0.9725000262260437 test loss: 1.0328036546707153 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1494 train loss: 0.9321030378341675 train acc: 0.9725000262260437 test loss: 1.0308191776275635 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1495 train loss: 0.9321349263191223 train acc: 0.9725000262260437 test loss: 1.027327537536621 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1496 train loss: 0.9322917461395264 train acc: 0.9725000262260437 test loss: 1.027795672416687 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1497 train loss: 0.9321975708007812 train acc: 0.9725000262260437 test loss: 1.0231572389602661 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1498 train loss: 0.9322608709335327 train acc: 0.9725000262260437 test loss: 1.031882882118225 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1499 train loss: 0.9322828650474548 train acc: 0.9725000262260437 test loss: 1.0298575162887573 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1500 train loss: 0.9321359395980835 train acc: 0.9725000262260437 test loss: 1.0301989316940308 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1501 train loss: 0.9322816729545593 train acc: 0.9725000262260437 test loss: 1.0290718078613281 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1502 train loss: 0.9321984648704529 train acc: 0.9725000262260437 test loss: 1.0343128442764282 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1503 train loss: 0.9320915341377258 train acc: 0.9725000262260437 test loss: 1.0219587087631226 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1504 train loss: 0.9322144985198975 train acc: 0.9725000262260437 test loss: 1.0307912826538086 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1505 train loss: 0.9321362972259521 train acc: 0.9725000262260437 test loss: 1.024401307106018 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1506 train loss: 0.9321880340576172 train acc: 0.9725000262260437 test loss: 1.0244994163513184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1507 train loss: 0.9322335124015808 train acc: 0.9725000262260437 test loss: 1.0244749784469604 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1508 train loss: 0.9321330785751343 train acc: 0.9725000262260437 test loss: 1.0239554643630981 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1509 train loss: 0.9321011900901794 train acc: 0.9725000262260437 test loss: 1.029327154159546 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1510 train loss: 0.9322501420974731 train acc: 0.9725000262260437 test loss: 1.0224254131317139 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1511 train loss: 0.9321165680885315 train acc: 0.9725000262260437 test loss: 1.0318059921264648 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1512 train loss: 0.9322420358657837 train acc: 0.9725000262260437 test loss: 1.0237362384796143 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1513 train loss: 0.9320574402809143 train acc: 0.9725000262260437 test loss: 1.0263776779174805 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1514 train loss: 0.9320520162582397 train acc: 0.9725000262260437 test loss: 1.029473900794983 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1515 train loss: 0.9320461750030518 train acc: 0.9725000262260437 test loss: 1.030347466468811 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1516 train loss: 0.9322499632835388 train acc: 0.9725000262260437 test loss: 1.0245295763015747 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1517 train loss: 0.9322978258132935 train acc: 0.9725000262260437 test loss: 1.0245620012283325 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1518 train loss: 0.9320523738861084 train acc: 0.9725000262260437 test loss: 1.019792914390564 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1519 train loss: 0.9322372674942017 train acc: 0.9725000262260437 test loss: 1.0233596563339233 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1520 train loss: 0.9323264360427856 train acc: 0.9725000262260437 test loss: 1.0404285192489624 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1521 train loss: 0.9322092533111572 train acc: 0.9725000262260437 test loss: 1.0302784442901611 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1522 train loss: 0.9322591423988342 train acc: 0.9725000262260437 test loss: 1.0247416496276855 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1523 train loss: 0.9323193430900574 train acc: 0.9725000262260437 test loss: 1.0320611000061035 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1524 train loss: 0.9322088360786438 train acc: 0.9725000262260437 test loss: 1.0288013219833374 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1525 train loss: 0.9322575926780701 train acc: 0.9725000262260437 test loss: 1.020160436630249 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1526 train loss: 0.9322028160095215 train acc: 0.9725000262260437 test loss: 1.0204299688339233 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1527 train loss: 0.9321811199188232 train acc: 0.9725000262260437 test loss: 1.0276174545288086 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1528 train loss: 0.9323033094406128 train acc: 0.9725000262260437 test loss: 1.0251295566558838 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1529 train loss: 0.9320982098579407 train acc: 0.9725000262260437 test loss: 1.0267624855041504 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1530 train loss: 0.9321098327636719 train acc: 0.9725000262260437 test loss: 1.0252734422683716 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1531 train loss: 0.9320465326309204 train acc: 0.9725000262260437 test loss: 1.033799409866333 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1532 train loss: 0.9320817589759827 train acc: 0.9725000262260437 test loss: 1.0266566276550293 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1533 train loss: 0.932107150554657 train acc: 0.9725000262260437 test loss: 1.031114935874939 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1534 train loss: 0.9322155714035034 train acc: 0.9725000262260437 test loss: 1.0239524841308594 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1535 train loss: 0.9320708513259888 train acc: 0.9725000262260437 test loss: 1.0250561237335205 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1536 train loss: 0.9320992231369019 train acc: 0.9725000262260437 test loss: 1.026748776435852 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1537 train loss: 0.9322136640548706 train acc: 0.9725000262260437 test loss: 1.0310031175613403 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1538 train loss: 0.9322253465652466 train acc: 0.9725000262260437 test loss: 1.0242501497268677 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1539 train loss: 0.9322810173034668 train acc: 0.9725000262260437 test loss: 1.0277345180511475 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1540 train loss: 0.9322141408920288 train acc: 0.9725000262260437 test loss: 1.0251150131225586 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1541 train loss: 0.9320708513259888 train acc: 0.9725000262260437 test loss: 1.0244455337524414 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1542 train loss: 0.932150661945343 train acc: 0.9725000262260437 test loss: 1.039394497871399 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1543 train loss: 0.9321661591529846 train acc: 0.9725000262260437 test loss: 1.028867244720459 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1544 train loss: 0.9323921799659729 train acc: 0.9725000262260437 test loss: 1.0267744064331055 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1545 train loss: 0.9323527812957764 train acc: 0.9725000262260437 test loss: 1.0276641845703125 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1546 train loss: 0.9322071075439453 train acc: 0.9725000262260437 test loss: 1.0228697061538696 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1547 train loss: 0.9321035742759705 train acc: 0.9725000262260437 test loss: 1.0206159353256226 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1548 train loss: 0.932343065738678 train acc: 0.9725000262260437 test loss: 1.022047519683838 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1549 train loss: 0.9321244955062866 train acc: 0.9725000262260437 test loss: 1.0258524417877197 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1550 train loss: 0.9320602416992188 train acc: 0.9725000262260437 test loss: 1.0352705717086792 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1551 train loss: 0.9321355223655701 train acc: 0.9725000262260437 test loss: 1.0230910778045654 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1552 train loss: 0.9321652054786682 train acc: 0.9725000262260437 test loss: 1.0251085758209229 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1553 train loss: 0.9321826696395874 train acc: 0.9725000262260437 test loss: 1.0216407775878906 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1554 train loss: 0.9321056604385376 train acc: 0.9725000262260437 test loss: 1.0236644744873047 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1555 train loss: 0.9322308301925659 train acc: 0.9725000262260437 test loss: 1.032347321510315 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1556 train loss: 0.9322991967201233 train acc: 0.9725000262260437 test loss: 1.0235474109649658 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1557 train loss: 0.9322149157524109 train acc: 0.9725000262260437 test loss: 1.028702974319458 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1558 train loss: 0.932181179523468 train acc: 0.9725000262260437 test loss: 1.0297651290893555 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1559 train loss: 0.9320545792579651 train acc: 0.9725000262260437 test loss: 1.0218929052352905 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1560 train loss: 0.9322224259376526 train acc: 0.9725000262260437 test loss: 1.0253682136535645 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1561 train loss: 0.9320777654647827 train acc: 0.9725000262260437 test loss: 1.028131365776062 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1562 train loss: 0.9322078227996826 train acc: 0.9725000262260437 test loss: 1.0251708030700684 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1563 train loss: 0.9321874976158142 train acc: 0.9725000262260437 test loss: 1.032570242881775 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1564 train loss: 0.9321856498718262 train acc: 0.9725000262260437 test loss: 1.0255191326141357 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1565 train loss: 0.9321310520172119 train acc: 0.9725000262260437 test loss: 1.026547908782959 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1566 train loss: 0.9321749210357666 train acc: 0.9725000262260437 test loss: 1.023604154586792 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1567 train loss: 0.9322343468666077 train acc: 0.9725000262260437 test loss: 1.023097276687622 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1568 train loss: 0.9321618676185608 train acc: 0.9725000262260437 test loss: 1.028647780418396 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1569 train loss: 0.9321267008781433 train acc: 0.9725000262260437 test loss: 1.0243209600448608 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1570 train loss: 0.9322142004966736 train acc: 0.9725000262260437 test loss: 1.030357837677002 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1571 train loss: 0.9321833848953247 train acc: 0.9725000262260437 test loss: 1.0329240560531616 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1572 train loss: 0.9321027994155884 train acc: 0.9725000262260437 test loss: 1.022786021232605 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1573 train loss: 0.9322088360786438 train acc: 0.9725000262260437 test loss: 1.0306729078292847 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1574 train loss: 0.9321722984313965 train acc: 0.9725000262260437 test loss: 1.0289655923843384 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1575 train loss: 0.932238757610321 train acc: 0.9725000262260437 test loss: 1.0217901468276978 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1576 train loss: 0.9321898818016052 train acc: 0.9725000262260437 test loss: 1.028728723526001 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1577 train loss: 0.9321607947349548 train acc: 0.9725000262260437 test loss: 1.0232417583465576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1578 train loss: 0.9321838617324829 train acc: 0.9725000262260437 test loss: 1.0311877727508545 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1579 train loss: 0.9322030544281006 train acc: 0.9725000262260437 test loss: 1.026153564453125 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1580 train loss: 0.9320811629295349 train acc: 0.9725000262260437 test loss: 1.0315382480621338 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1581 train loss: 0.9322585463523865 train acc: 0.9725000262260437 test loss: 1.0241241455078125 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1582 train loss: 0.9321720004081726 train acc: 0.9725000262260437 test loss: 1.0311800241470337 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1583 train loss: 0.9322561025619507 train acc: 0.9725000262260437 test loss: 1.0198742151260376 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1584 train loss: 0.9321343898773193 train acc: 0.9725000262260437 test loss: 1.024712085723877 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1585 train loss: 0.9321809411048889 train acc: 0.9725000262260437 test loss: 1.0242931842803955 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1586 train loss: 0.9321891069412231 train acc: 0.9725000262260437 test loss: 1.0286915302276611 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1587 train loss: 0.932218611240387 train acc: 0.9725000262260437 test loss: 1.033961534500122 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1588 train loss: 0.932086169719696 train acc: 0.9725000262260437 test loss: 1.025269865989685 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1589 train loss: 0.9321310520172119 train acc: 0.9725000262260437 test loss: 1.0258897542953491 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1590 train loss: 0.9322030544281006 train acc: 0.9725000262260437 test loss: 1.0192084312438965 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1591 train loss: 0.9321882128715515 train acc: 0.9725000262260437 test loss: 1.0213028192520142 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1592 train loss: 0.9321697950363159 train acc: 0.9725000262260437 test loss: 1.0278857946395874 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1593 train loss: 0.9320650696754456 train acc: 0.9725000262260437 test loss: 1.0269324779510498 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1594 train loss: 0.9323157668113708 train acc: 0.9725000262260437 test loss: 1.0234910249710083 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1595 train loss: 0.9322764873504639 train acc: 0.9725000262260437 test loss: 1.039731502532959 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1596 train loss: 0.932267427444458 train acc: 0.9725000262260437 test loss: 1.0400639772415161 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1597 train loss: 0.9322447180747986 train acc: 0.9725000262260437 test loss: 1.0297642946243286 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1598 train loss: 0.9320659637451172 train acc: 0.9725000262260437 test loss: 1.02352774143219 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1599 train loss: 0.9322612285614014 train acc: 0.9725000262260437 test loss: 1.027706503868103 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1600 train loss: 0.9322155117988586 train acc: 0.9725000262260437 test loss: 1.0279631614685059 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1601 train loss: 0.9321771264076233 train acc: 0.9725000262260437 test loss: 1.0271345376968384 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1602 train loss: 0.932206392288208 train acc: 0.9725000262260437 test loss: 1.021033525466919 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1603 train loss: 0.9323062896728516 train acc: 0.9725000262260437 test loss: 1.0273438692092896 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1604 train loss: 0.9322733879089355 train acc: 0.9725000262260437 test loss: 1.0235826969146729 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1605 train loss: 0.9322738647460938 train acc: 0.9725000262260437 test loss: 1.0179650783538818 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1606 train loss: 0.9321523904800415 train acc: 0.9725000262260437 test loss: 1.0224281549453735 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1607 train loss: 0.9320866465568542 train acc: 0.9725000262260437 test loss: 1.0288006067276 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1608 train loss: 0.9321722388267517 train acc: 0.9725000262260437 test loss: 1.0200762748718262 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1609 train loss: 0.9322551488876343 train acc: 0.9725000262260437 test loss: 1.0239485502243042 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1610 train loss: 0.932040810585022 train acc: 0.9725000262260437 test loss: 1.034682273864746 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1611 train loss: 0.9320755004882812 train acc: 0.9725000262260437 test loss: 1.032432198524475 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1612 train loss: 0.9321523904800415 train acc: 0.9725000262260437 test loss: 1.0275934934616089 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1613 train loss: 0.9322709441184998 train acc: 0.9725000262260437 test loss: 1.0234501361846924 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1614 train loss: 0.932125449180603 train acc: 0.9725000262260437 test loss: 1.024398684501648 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1615 train loss: 0.932125985622406 train acc: 0.9725000262260437 test loss: 1.0259840488433838 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1616 train loss: 0.932149350643158 train acc: 0.9725000262260437 test loss: 1.0396015644073486 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1617 train loss: 0.9321090579032898 train acc: 0.9725000262260437 test loss: 1.0215142965316772 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1618 train loss: 0.9322571754455566 train acc: 0.9725000262260437 test loss: 1.0281970500946045 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1619 train loss: 0.9321343898773193 train acc: 0.9725000262260437 test loss: 1.0227428674697876 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1620 train loss: 0.9321700930595398 train acc: 0.9725000262260437 test loss: 1.0292983055114746 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1621 train loss: 0.9320443868637085 train acc: 0.9725000262260437 test loss: 1.0276610851287842 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1622 train loss: 0.9320509433746338 train acc: 0.9725000262260437 test loss: 1.0322471857070923 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1623 train loss: 0.9321908354759216 train acc: 0.9725000262260437 test loss: 1.0313048362731934 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1624 train loss: 0.9322100281715393 train acc: 0.9725000262260437 test loss: 1.0234516859054565 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1625 train loss: 0.9322518706321716 train acc: 0.9725000262260437 test loss: 1.0234581232070923 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1626 train loss: 0.9321973919868469 train acc: 0.9725000262260437 test loss: 1.0257800817489624 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1627 train loss: 0.9322146773338318 train acc: 0.9725000262260437 test loss: 1.0244114398956299 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1628 train loss: 0.9321379661560059 train acc: 0.9725000262260437 test loss: 1.026634693145752 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1629 train loss: 0.9321149587631226 train acc: 0.9725000262260437 test loss: 1.0255690813064575 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1630 train loss: 0.9321814775466919 train acc: 0.9725000262260437 test loss: 1.0210851430892944 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1631 train loss: 0.9322057366371155 train acc: 0.9725000262260437 test loss: 1.0252366065979004 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1632 train loss: 0.9321280121803284 train acc: 0.9725000262260437 test loss: 1.0211281776428223 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1633 train loss: 0.9320895671844482 train acc: 0.9725000262260437 test loss: 1.0245968103408813 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1634 train loss: 0.9320749640464783 train acc: 0.9725000262260437 test loss: 1.0236327648162842 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1635 train loss: 0.932035505771637 train acc: 0.9725000262260437 test loss: 1.0306768417358398 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1636 train loss: 0.9320487976074219 train acc: 0.9725000262260437 test loss: 1.0236940383911133 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1637 train loss: 0.932206392288208 train acc: 0.9725000262260437 test loss: 1.0250096321105957 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1638 train loss: 0.9321823716163635 train acc: 0.9725000262260437 test loss: 1.0282540321350098 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1639 train loss: 0.9322434067726135 train acc: 0.9725000262260437 test loss: 1.0287790298461914 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1640 train loss: 0.9320782423019409 train acc: 0.9725000262260437 test loss: 1.022679328918457 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1641 train loss: 0.9321714639663696 train acc: 0.9725000262260437 test loss: 1.020809292793274 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1642 train loss: 0.9321196675300598 train acc: 0.9725000262260437 test loss: 1.0232205390930176 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1643 train loss: 0.9321751594543457 train acc: 0.9725000262260437 test loss: 1.0309348106384277 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1644 train loss: 0.9322441816329956 train acc: 0.9725000262260437 test loss: 1.0245016813278198 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1645 train loss: 0.9321285486221313 train acc: 0.9725000262260437 test loss: 1.0231853723526 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1646 train loss: 0.9322502017021179 train acc: 0.9725000262260437 test loss: 1.0292844772338867 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1647 train loss: 0.9321248531341553 train acc: 0.9725000262260437 test loss: 1.0274277925491333 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1648 train loss: 0.932121753692627 train acc: 0.9725000262260437 test loss: 1.0269849300384521 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1649 train loss: 0.9320947527885437 train acc: 0.9725000262260437 test loss: 1.0259281396865845 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1650 train loss: 0.9321346879005432 train acc: 0.9725000262260437 test loss: 1.0228711366653442 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1651 train loss: 0.9321801066398621 train acc: 0.9725000262260437 test loss: 1.0234957933425903 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1652 train loss: 0.9321277141571045 train acc: 0.9725000262260437 test loss: 1.0448073148727417 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 1653 train loss: 0.9321157932281494 train acc: 0.9725000262260437 test loss: 1.0249691009521484 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1654 train loss: 0.9321585297584534 train acc: 0.9725000262260437 test loss: 1.028721809387207 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1655 train loss: 0.9322425723075867 train acc: 0.9725000262260437 test loss: 1.0244735479354858 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1656 train loss: 0.9321292638778687 train acc: 0.9725000262260437 test loss: 1.0345573425292969 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1657 train loss: 0.9321550130844116 train acc: 0.9725000262260437 test loss: 1.020095705986023 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1658 train loss: 0.932097315788269 train acc: 0.9725000262260437 test loss: 1.026816964149475 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1659 train loss: 0.9322386384010315 train acc: 0.9725000262260437 test loss: 1.0217430591583252 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1660 train loss: 0.9321932792663574 train acc: 0.9725000262260437 test loss: 1.029636025428772 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1661 train loss: 0.932178795337677 train acc: 0.9725000262260437 test loss: 1.033464789390564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1662 train loss: 0.9321407079696655 train acc: 0.9725000262260437 test loss: 1.027580738067627 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1663 train loss: 0.9322583675384521 train acc: 0.9725000262260437 test loss: 1.0185997486114502 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1664 train loss: 0.9320756793022156 train acc: 0.9725000262260437 test loss: 1.0246697664260864 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1665 train loss: 0.9320451617240906 train acc: 0.9725000262260437 test loss: 1.026916742324829 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1666 train loss: 0.932192325592041 train acc: 0.9725000262260437 test loss: 1.0275745391845703 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1667 train loss: 0.9321339130401611 train acc: 0.9725000262260437 test loss: 1.02700674533844 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1668 train loss: 0.9321911334991455 train acc: 0.9725000262260437 test loss: 1.029974102973938 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1669 train loss: 0.9321362376213074 train acc: 0.9725000262260437 test loss: 1.0299383401870728 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1670 train loss: 0.9320457577705383 train acc: 0.9725000262260437 test loss: 1.0225836038589478 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1671 train loss: 0.9320859313011169 train acc: 0.9725000262260437 test loss: 1.023512363433838 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1672 train loss: 0.9320594668388367 train acc: 0.9725000262260437 test loss: 1.0257774591445923 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1673 train loss: 0.9321497082710266 train acc: 0.9725000262260437 test loss: 1.0213772058486938 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1674 train loss: 0.9323071241378784 train acc: 0.9725000262260437 test loss: 1.0217373371124268 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1675 train loss: 0.9321533441543579 train acc: 0.9725000262260437 test loss: 1.0352122783660889 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1676 train loss: 0.9320521354675293 train acc: 0.9725000262260437 test loss: 1.0196257829666138 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1677 train loss: 0.9322262406349182 train acc: 0.9725000262260437 test loss: 1.0260151624679565 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1678 train loss: 0.932263195514679 train acc: 0.9725000262260437 test loss: 1.024084448814392 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1679 train loss: 0.9321503639221191 train acc: 0.9725000262260437 test loss: 1.0264599323272705 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1680 train loss: 0.9320631623268127 train acc: 0.9725000262260437 test loss: 1.0201945304870605 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1681 train loss: 0.9322275519371033 train acc: 0.9725000262260437 test loss: 1.0292528867721558 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1682 train loss: 0.9321051836013794 train acc: 0.9725000262260437 test loss: 1.0242844820022583 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1683 train loss: 0.9321560859680176 train acc: 0.9725000262260437 test loss: 1.0321964025497437 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1684 train loss: 0.9322441816329956 train acc: 0.9725000262260437 test loss: 1.033961534500122 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1685 train loss: 0.9322196841239929 train acc: 0.9725000262260437 test loss: 1.0297746658325195 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1686 train loss: 0.9322477579116821 train acc: 0.9725000262260437 test loss: 1.0269132852554321 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1687 train loss: 0.9322405457496643 train acc: 0.9725000262260437 test loss: 1.021991491317749 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1688 train loss: 0.9321518540382385 train acc: 0.9725000262260437 test loss: 1.0293216705322266 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1689 train loss: 0.932269275188446 train acc: 0.9725000262260437 test loss: 1.0277525186538696 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1690 train loss: 0.9322190284729004 train acc: 0.9725000262260437 test loss: 1.023013710975647 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1691 train loss: 0.9322029948234558 train acc: 0.9725000262260437 test loss: 1.0269705057144165 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1692 train loss: 0.932280957698822 train acc: 0.9725000262260437 test loss: 1.02976393699646 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1693 train loss: 0.9321994781494141 train acc: 0.9725000262260437 test loss: 1.0242730379104614 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1694 train loss: 0.9322991371154785 train acc: 0.9725000262260437 test loss: 1.0294770002365112 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1695 train loss: 0.9321826100349426 train acc: 0.9725000262260437 test loss: 1.0237048864364624 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1696 train loss: 0.9321290850639343 train acc: 0.9725000262260437 test loss: 1.0227724313735962 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1697 train loss: 0.932081937789917 train acc: 0.9725000262260437 test loss: 1.0273486375808716 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1698 train loss: 0.9321819543838501 train acc: 0.9725000262260437 test loss: 1.0250478982925415 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1699 train loss: 0.9322292804718018 train acc: 0.9725000262260437 test loss: 1.0274840593338013 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1700 train loss: 0.9320627450942993 train acc: 0.9725000262260437 test loss: 1.0290409326553345 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1701 train loss: 0.9322564601898193 train acc: 0.9725000262260437 test loss: 1.0245444774627686 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1702 train loss: 0.9321016669273376 train acc: 0.9725000262260437 test loss: 1.0301978588104248 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1703 train loss: 0.9320704936981201 train acc: 0.9725000262260437 test loss: 1.0250675678253174 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1704 train loss: 0.9320542812347412 train acc: 0.9725000262260437 test loss: 1.0201420783996582 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1705 train loss: 0.9320976734161377 train acc: 0.9725000262260437 test loss: 1.0248675346374512 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1706 train loss: 0.9323469400405884 train acc: 0.9725000262260437 test loss: 1.028172254562378 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1707 train loss: 0.9320409893989563 train acc: 0.9725000262260437 test loss: 1.0263675451278687 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1708 train loss: 0.9322673082351685 train acc: 0.9725000262260437 test loss: 1.0230625867843628 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1709 train loss: 0.9324573278427124 train acc: 0.9725000262260437 test loss: 1.0327503681182861 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1710 train loss: 0.9322768449783325 train acc: 0.9725000262260437 test loss: 1.024776577949524 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1711 train loss: 0.9321733713150024 train acc: 0.9725000262260437 test loss: 1.0263174772262573 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1712 train loss: 0.9322212934494019 train acc: 0.9725000262260437 test loss: 1.0279079675674438 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1713 train loss: 0.9320549964904785 train acc: 0.9725000262260437 test loss: 1.0303339958190918 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1714 train loss: 0.9321838617324829 train acc: 0.9725000262260437 test loss: 1.035357117652893 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1715 train loss: 0.9321597218513489 train acc: 0.9725000262260437 test loss: 1.022175908088684 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1716 train loss: 0.9322319626808167 train acc: 0.9725000262260437 test loss: 1.026770830154419 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1717 train loss: 0.9322043061256409 train acc: 0.9725000262260437 test loss: 1.028928279876709 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1718 train loss: 0.9321098327636719 train acc: 0.9725000262260437 test loss: 1.0227949619293213 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1719 train loss: 0.9321872591972351 train acc: 0.9725000262260437 test loss: 1.0231295824050903 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1720 train loss: 0.9320899844169617 train acc: 0.9725000262260437 test loss: 1.0254191160202026 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1721 train loss: 0.9322956204414368 train acc: 0.9725000262260437 test loss: 1.0160423517227173 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1722 train loss: 0.9320520758628845 train acc: 0.9725000262260437 test loss: 1.0303033590316772 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1723 train loss: 0.9322023987770081 train acc: 0.9725000262260437 test loss: 1.0270555019378662 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1724 train loss: 0.9321902394294739 train acc: 0.9725000262260437 test loss: 1.0287760496139526 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1725 train loss: 0.9320753216743469 train acc: 0.9725000262260437 test loss: 1.0244495868682861 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1726 train loss: 0.9322242736816406 train acc: 0.9725000262260437 test loss: 1.0309994220733643 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1727 train loss: 0.9321802258491516 train acc: 0.9725000262260437 test loss: 1.0311956405639648 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1728 train loss: 0.9322783946990967 train acc: 0.9725000262260437 test loss: 1.0270041227340698 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1729 train loss: 0.9322004914283752 train acc: 0.9725000262260437 test loss: 1.0289416313171387 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1730 train loss: 0.9322709441184998 train acc: 0.9725000262260437 test loss: 1.0279484987258911 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1731 train loss: 0.9321547150611877 train acc: 0.9725000262260437 test loss: 1.0290995836257935 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1732 train loss: 0.932209312915802 train acc: 0.9725000262260437 test loss: 1.029341220855713 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1733 train loss: 0.9321391582489014 train acc: 0.9725000262260437 test loss: 1.0228087902069092 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1734 train loss: 0.9322259426116943 train acc: 0.9725000262260437 test loss: 1.0222381353378296 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1735 train loss: 0.9321753978729248 train acc: 0.9725000262260437 test loss: 1.0191198587417603 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1736 train loss: 0.9321771264076233 train acc: 0.9725000262260437 test loss: 1.0307754278182983 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1737 train loss: 0.9320757985115051 train acc: 0.9725000262260437 test loss: 1.0262138843536377 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1738 train loss: 0.9320719242095947 train acc: 0.9725000262260437 test loss: 1.0256158113479614 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1739 train loss: 0.9322107434272766 train acc: 0.9725000262260437 test loss: 1.022449016571045 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1740 train loss: 0.9320954084396362 train acc: 0.9725000262260437 test loss: 1.027195930480957 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1741 train loss: 0.9322332739830017 train acc: 0.9725000262260437 test loss: 1.0317280292510986 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1742 train loss: 0.9322311878204346 train acc: 0.9725000262260437 test loss: 1.0194438695907593 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1743 train loss: 0.9322084784507751 train acc: 0.9725000262260437 test loss: 1.0250390768051147 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1744 train loss: 0.9321808815002441 train acc: 0.9725000262260437 test loss: 1.0286531448364258 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1745 train loss: 0.932176947593689 train acc: 0.9725000262260437 test loss: 1.0299596786499023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1746 train loss: 0.9320951700210571 train acc: 0.9725000262260437 test loss: 1.028395652770996 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1747 train loss: 0.9322128891944885 train acc: 0.9725000262260437 test loss: 1.0305333137512207 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1748 train loss: 0.9325487613677979 train acc: 0.9725000262260437 test loss: 1.0378624200820923 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1749 train loss: 0.9321801662445068 train acc: 0.9725000262260437 test loss: 1.0235494375228882 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1750 train loss: 0.9321867227554321 train acc: 0.9725000262260437 test loss: 1.0340704917907715 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1751 train loss: 0.9321690201759338 train acc: 0.9725000262260437 test loss: 1.0233032703399658 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1752 train loss: 0.9321744441986084 train acc: 0.9725000262260437 test loss: 1.02729070186615 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1753 train loss: 0.9320399761199951 train acc: 0.9725000262260437 test loss: 1.0227612257003784 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1754 train loss: 0.9321252703666687 train acc: 0.9725000262260437 test loss: 1.0266737937927246 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1755 train loss: 0.9321989417076111 train acc: 0.9725000262260437 test loss: 1.0181900262832642 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1756 train loss: 0.9321064949035645 train acc: 0.9725000262260437 test loss: 1.0267510414123535 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1757 train loss: 0.9321972131729126 train acc: 0.9725000262260437 test loss: 1.0295422077178955 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1758 train loss: 0.9321200847625732 train acc: 0.9725000262260437 test loss: 1.0213254690170288 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1759 train loss: 0.9322006106376648 train acc: 0.9725000262260437 test loss: 1.029640555381775 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1760 train loss: 0.9322023987770081 train acc: 0.9725000262260437 test loss: 1.030409574508667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1761 train loss: 0.9320681095123291 train acc: 0.9725000262260437 test loss: 1.0280344486236572 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1762 train loss: 0.9321841597557068 train acc: 0.9725000262260437 test loss: 1.0267977714538574 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1763 train loss: 0.9322016835212708 train acc: 0.9725000262260437 test loss: 1.028027892112732 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1764 train loss: 0.9321107268333435 train acc: 0.9725000262260437 test loss: 1.0266345739364624 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1765 train loss: 0.9320588111877441 train acc: 0.9725000262260437 test loss: 1.0337367057800293 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1766 train loss: 0.9321582317352295 train acc: 0.9725000262260437 test loss: 1.0271164178848267 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1767 train loss: 0.9321670532226562 train acc: 0.9725000262260437 test loss: 1.0314444303512573 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1768 train loss: 0.932110071182251 train acc: 0.9725000262260437 test loss: 1.0225211381912231 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1769 train loss: 0.9320811629295349 train acc: 0.9725000262260437 test loss: 1.0244790315628052 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1770 train loss: 0.93216872215271 train acc: 0.9725000262260437 test loss: 1.025549054145813 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1771 train loss: 0.9322001934051514 train acc: 0.9725000262260437 test loss: 1.029468059539795 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1772 train loss: 0.9322653412818909 train acc: 0.9725000262260437 test loss: 1.028152585029602 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1773 train loss: 0.9321062564849854 train acc: 0.9725000262260437 test loss: 1.0303587913513184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1774 train loss: 0.9321997165679932 train acc: 0.9725000262260437 test loss: 1.0265846252441406 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1775 train loss: 0.9321259260177612 train acc: 0.9725000262260437 test loss: 1.029931902885437 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1776 train loss: 0.9321209788322449 train acc: 0.9725000262260437 test loss: 1.0248684883117676 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1777 train loss: 0.9321747422218323 train acc: 0.9725000262260437 test loss: 1.0325099229812622 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1778 train loss: 0.932094931602478 train acc: 0.9725000262260437 test loss: 1.0283058881759644 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1779 train loss: 0.9321362376213074 train acc: 0.9725000262260437 test loss: 1.0229393243789673 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1780 train loss: 0.9322497844696045 train acc: 0.9725000262260437 test loss: 1.0288231372833252 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1781 train loss: 0.9323146939277649 train acc: 0.9725000262260437 test loss: 1.031294584274292 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1782 train loss: 0.932234525680542 train acc: 0.9725000262260437 test loss: 1.0240777730941772 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1783 train loss: 0.9321038126945496 train acc: 0.9725000262260437 test loss: 1.0328258275985718 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1784 train loss: 0.932300329208374 train acc: 0.9725000262260437 test loss: 1.0290956497192383 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1785 train loss: 0.9320828318595886 train acc: 0.9725000262260437 test loss: 1.0233498811721802 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1786 train loss: 0.932148277759552 train acc: 0.9725000262260437 test loss: 1.024274230003357 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1787 train loss: 0.9320929050445557 train acc: 0.9725000262260437 test loss: 1.0270475149154663 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1788 train loss: 0.9321234822273254 train acc: 0.9725000262260437 test loss: 1.0277621746063232 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1789 train loss: 0.9321951866149902 train acc: 0.9725000262260437 test loss: 1.0291074514389038 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1790 train loss: 0.9321770668029785 train acc: 0.9725000262260437 test loss: 1.0212178230285645 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1791 train loss: 0.9321661591529846 train acc: 0.9725000262260437 test loss: 1.0313247442245483 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1792 train loss: 0.9321715831756592 train acc: 0.9725000262260437 test loss: 1.0231919288635254 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1793 train loss: 0.9322171211242676 train acc: 0.9725000262260437 test loss: 1.0280301570892334 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1794 train loss: 0.9321498870849609 train acc: 0.9725000262260437 test loss: 1.024716854095459 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1795 train loss: 0.9321711659431458 train acc: 0.9725000262260437 test loss: 1.0313154458999634 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1796 train loss: 0.9322212338447571 train acc: 0.9725000262260437 test loss: 1.0233769416809082 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1797 train loss: 0.9322197437286377 train acc: 0.9725000262260437 test loss: 1.0244452953338623 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1798 train loss: 0.9322716593742371 train acc: 0.9725000262260437 test loss: 1.02400803565979 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1799 train loss: 0.9320982098579407 train acc: 0.9725000262260437 test loss: 1.0302915573120117 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1800 train loss: 0.9320652484893799 train acc: 0.9725000262260437 test loss: 1.032230019569397 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1801 train loss: 0.9321882724761963 train acc: 0.9725000262260437 test loss: 1.0210119485855103 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1802 train loss: 0.9321247935295105 train acc: 0.9725000262260437 test loss: 1.02518630027771 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1803 train loss: 0.9320594668388367 train acc: 0.9725000262260437 test loss: 1.02328360080719 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1804 train loss: 0.9321953654289246 train acc: 0.9725000262260437 test loss: 1.0267874002456665 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1805 train loss: 0.9321611523628235 train acc: 0.9725000262260437 test loss: 1.0318368673324585 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1806 train loss: 0.9320782423019409 train acc: 0.9725000262260437 test loss: 1.027977705001831 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1807 train loss: 0.9321375489234924 train acc: 0.9725000262260437 test loss: 1.0262621641159058 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1808 train loss: 0.9321324229240417 train acc: 0.9725000262260437 test loss: 1.0271999835968018 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1809 train loss: 0.9322016835212708 train acc: 0.9725000262260437 test loss: 1.0341969728469849 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1810 train loss: 0.9321866035461426 train acc: 0.9725000262260437 test loss: 1.0240005254745483 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1811 train loss: 0.9322207570075989 train acc: 0.9725000262260437 test loss: 1.0214275121688843 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1812 train loss: 0.9321911334991455 train acc: 0.9725000262260437 test loss: 1.0266129970550537 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1813 train loss: 0.9322956800460815 train acc: 0.9725000262260437 test loss: 1.0292770862579346 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1814 train loss: 0.9322036504745483 train acc: 0.9725000262260437 test loss: 1.0343953371047974 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1815 train loss: 0.9321730732917786 train acc: 0.9725000262260437 test loss: 1.025712251663208 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1816 train loss: 0.9321057796478271 train acc: 0.9725000262260437 test loss: 1.0243438482284546 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1817 train loss: 0.932147204875946 train acc: 0.9725000262260437 test loss: 1.0309392213821411 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1818 train loss: 0.9322294592857361 train acc: 0.9725000262260437 test loss: 1.023041009902954 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1819 train loss: 0.9321823716163635 train acc: 0.9725000262260437 test loss: 1.0259872674942017 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1820 train loss: 0.9323019981384277 train acc: 0.9725000262260437 test loss: 1.027449369430542 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1821 train loss: 0.9321466684341431 train acc: 0.9725000262260437 test loss: 1.0253912210464478 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1822 train loss: 0.9320924282073975 train acc: 0.9725000262260437 test loss: 1.028364896774292 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1823 train loss: 0.9321567416191101 train acc: 0.9725000262260437 test loss: 1.029579758644104 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1824 train loss: 0.9322449564933777 train acc: 0.9725000262260437 test loss: 1.0240411758422852 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1825 train loss: 0.9322138428688049 train acc: 0.9725000262260437 test loss: 1.035976767539978 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1826 train loss: 0.932170033454895 train acc: 0.9725000262260437 test loss: 1.0274001359939575 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1827 train loss: 0.9322245717048645 train acc: 0.9725000262260437 test loss: 1.027450442314148 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1828 train loss: 0.932176947593689 train acc: 0.9725000262260437 test loss: 1.0285427570343018 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1829 train loss: 0.9321866631507874 train acc: 0.9725000262260437 test loss: 1.0351125001907349 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1830 train loss: 0.9322062134742737 train acc: 0.9725000262260437 test loss: 1.0269240140914917 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1831 train loss: 0.9320625066757202 train acc: 0.9725000262260437 test loss: 1.023275375366211 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1832 train loss: 0.9321613907814026 train acc: 0.9725000262260437 test loss: 1.0270298719406128 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1833 train loss: 0.9321499466896057 train acc: 0.9725000262260437 test loss: 1.0232149362564087 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1834 train loss: 0.9321407079696655 train acc: 0.9725000262260437 test loss: 1.0277378559112549 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1835 train loss: 0.932187020778656 train acc: 0.9725000262260437 test loss: 1.035911202430725 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1836 train loss: 0.9321052432060242 train acc: 0.9725000262260437 test loss: 1.0266965627670288 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1837 train loss: 0.9322398900985718 train acc: 0.9725000262260437 test loss: 1.0269798040390015 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1838 train loss: 0.9321913123130798 train acc: 0.9725000262260437 test loss: 1.0216662883758545 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1839 train loss: 0.9320482611656189 train acc: 0.9725000262260437 test loss: 1.0217621326446533 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1840 train loss: 0.9321896433830261 train acc: 0.9725000262260437 test loss: 1.0277793407440186 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1841 train loss: 0.9321618676185608 train acc: 0.9725000262260437 test loss: 1.0232131481170654 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1842 train loss: 0.9320964217185974 train acc: 0.9725000262260437 test loss: 1.03160560131073 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1843 train loss: 0.9321700930595398 train acc: 0.9725000262260437 test loss: 1.031734824180603 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1844 train loss: 0.9320961236953735 train acc: 0.9725000262260437 test loss: 1.027974009513855 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1845 train loss: 0.9320823550224304 train acc: 0.9725000262260437 test loss: 1.0277957916259766 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1846 train loss: 0.9320353865623474 train acc: 0.9725000262260437 test loss: 1.0251492261886597 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1847 train loss: 0.9320932030677795 train acc: 0.9725000262260437 test loss: 1.029026985168457 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1848 train loss: 0.9320616126060486 train acc: 0.9725000262260437 test loss: 1.0252764225006104 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1849 train loss: 0.9320871233940125 train acc: 0.9725000262260437 test loss: 1.0269237756729126 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1850 train loss: 0.9321487545967102 train acc: 0.9725000262260437 test loss: 1.0294570922851562 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1851 train loss: 0.9321463108062744 train acc: 0.9725000262260437 test loss: 1.0399326086044312 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1852 train loss: 0.9322307109832764 train acc: 0.9725000262260437 test loss: 1.0301066637039185 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1853 train loss: 0.9321841597557068 train acc: 0.9725000262260437 test loss: 1.0312471389770508 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1854 train loss: 0.9321956634521484 train acc: 0.9725000262260437 test loss: 1.0347846746444702 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1855 train loss: 0.9322939515113831 train acc: 0.9725000262260437 test loss: 1.024084210395813 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1856 train loss: 0.9321585297584534 train acc: 0.9725000262260437 test loss: 1.0271027088165283 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1857 train loss: 0.932297945022583 train acc: 0.9725000262260437 test loss: 1.0295661687850952 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1858 train loss: 0.9320828914642334 train acc: 0.9725000262260437 test loss: 1.0276962518692017 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1859 train loss: 0.9321850538253784 train acc: 0.9725000262260437 test loss: 1.0250329971313477 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1860 train loss: 0.9321562051773071 train acc: 0.9725000262260437 test loss: 1.0254416465759277 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1861 train loss: 0.9322525262832642 train acc: 0.9725000262260437 test loss: 1.0358561277389526 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1862 train loss: 0.9321834444999695 train acc: 0.9725000262260437 test loss: 1.0280885696411133 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1863 train loss: 0.9320967197418213 train acc: 0.9725000262260437 test loss: 1.0249425172805786 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1864 train loss: 0.9321714639663696 train acc: 0.9725000262260437 test loss: 1.0265837907791138 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1865 train loss: 0.9322125315666199 train acc: 0.9725000262260437 test loss: 1.0258896350860596 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1866 train loss: 0.9320834875106812 train acc: 0.9725000262260437 test loss: 1.0260051488876343 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1867 train loss: 0.9322044253349304 train acc: 0.9725000262260437 test loss: 1.0303692817687988 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1868 train loss: 0.9321967959403992 train acc: 0.9725000262260437 test loss: 1.0291221141815186 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1869 train loss: 0.9322289228439331 train acc: 0.9725000262260437 test loss: 1.0258073806762695 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1870 train loss: 0.9321618676185608 train acc: 0.9725000262260437 test loss: 1.030258059501648 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1871 train loss: 0.9322268962860107 train acc: 0.9725000262260437 test loss: 1.0294378995895386 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1872 train loss: 0.9320583939552307 train acc: 0.9725000262260437 test loss: 1.032111406326294 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1873 train loss: 0.9322501420974731 train acc: 0.9725000262260437 test loss: 1.0289278030395508 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1874 train loss: 0.9321618676185608 train acc: 0.9725000262260437 test loss: 1.0351641178131104 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1875 train loss: 0.9322627186775208 train acc: 0.9725000262260437 test loss: 1.031286597251892 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1876 train loss: 0.9320455193519592 train acc: 0.9725000262260437 test loss: 1.0302568674087524 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1877 train loss: 0.9322633147239685 train acc: 0.9725000262260437 test loss: 1.027448296546936 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1878 train loss: 0.9321939945220947 train acc: 0.9725000262260437 test loss: 1.0256798267364502 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1879 train loss: 0.9321515560150146 train acc: 0.9725000262260437 test loss: 1.0267717838287354 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1880 train loss: 0.9321244955062866 train acc: 0.9725000262260437 test loss: 1.0316393375396729 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1881 train loss: 0.9321615695953369 train acc: 0.9725000262260437 test loss: 1.0260772705078125 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1882 train loss: 0.9321004748344421 train acc: 0.9725000262260437 test loss: 1.0357471704483032 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1883 train loss: 0.932131826877594 train acc: 0.9725000262260437 test loss: 1.0313864946365356 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1884 train loss: 0.9321139454841614 train acc: 0.9725000262260437 test loss: 1.0301510095596313 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1885 train loss: 0.9320907592773438 train acc: 0.9725000262260437 test loss: 1.028277039527893 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1886 train loss: 0.932264506816864 train acc: 0.9725000262260437 test loss: 1.0329654216766357 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1887 train loss: 0.9322818517684937 train acc: 0.9725000262260437 test loss: 1.0257923603057861 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1888 train loss: 0.9321885108947754 train acc: 0.9725000262260437 test loss: 1.0265620946884155 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1889 train loss: 0.9320909380912781 train acc: 0.9725000262260437 test loss: 1.0316945314407349 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1890 train loss: 0.9321733713150024 train acc: 0.9725000262260437 test loss: 1.0292675495147705 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1891 train loss: 0.9321644306182861 train acc: 0.9725000262260437 test loss: 1.0313812494277954 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1892 train loss: 0.9321977496147156 train acc: 0.9725000262260437 test loss: 1.025559425354004 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1893 train loss: 0.932105302810669 train acc: 0.9725000262260437 test loss: 1.0259442329406738 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1894 train loss: 0.932138979434967 train acc: 0.9725000262260437 test loss: 1.0330791473388672 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1895 train loss: 0.9322257041931152 train acc: 0.9725000262260437 test loss: 1.0405383110046387 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1896 train loss: 0.9322474002838135 train acc: 0.9725000262260437 test loss: 1.0339553356170654 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1897 train loss: 0.932145893573761 train acc: 0.9725000262260437 test loss: 1.0298415422439575 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1898 train loss: 0.9320424795150757 train acc: 0.9725000262260437 test loss: 1.0257874727249146 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1899 train loss: 0.9321999549865723 train acc: 0.9725000262260437 test loss: 1.0303871631622314 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1900 train loss: 0.9320404529571533 train acc: 0.9725000262260437 test loss: 1.0295987129211426 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1901 train loss: 0.9321906566619873 train acc: 0.9725000262260437 test loss: 1.035193681716919 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1902 train loss: 0.9321330785751343 train acc: 0.9725000262260437 test loss: 1.0302447080612183 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1903 train loss: 0.9320836067199707 train acc: 0.9725000262260437 test loss: 1.0351656675338745 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1904 train loss: 0.932187020778656 train acc: 0.9725000262260437 test loss: 1.028812050819397 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1905 train loss: 0.9321571588516235 train acc: 0.9725000262260437 test loss: 1.031896710395813 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1906 train loss: 0.9320757985115051 train acc: 0.9725000262260437 test loss: 1.0253958702087402 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1907 train loss: 0.9322776198387146 train acc: 0.9725000262260437 test loss: 1.0308775901794434 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1908 train loss: 0.9321615695953369 train acc: 0.9725000262260437 test loss: 1.030827283859253 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1909 train loss: 0.9322414398193359 train acc: 0.9725000262260437 test loss: 1.025109052658081 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1910 train loss: 0.9322355389595032 train acc: 0.9725000262260437 test loss: 1.028990387916565 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1911 train loss: 0.9320439696311951 train acc: 0.9725000262260437 test loss: 1.0163395404815674 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1912 train loss: 0.9322553873062134 train acc: 0.9725000262260437 test loss: 1.0327486991882324 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1913 train loss: 0.9321611523628235 train acc: 0.9725000262260437 test loss: 1.020910382270813 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1914 train loss: 0.9322106242179871 train acc: 0.9725000262260437 test loss: 1.034290075302124 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1915 train loss: 0.9321944713592529 train acc: 0.9725000262260437 test loss: 1.0399399995803833 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1916 train loss: 0.9322401285171509 train acc: 0.9725000262260437 test loss: 1.0238597393035889 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1917 train loss: 0.9320981502532959 train acc: 0.9725000262260437 test loss: 1.0290576219558716 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1918 train loss: 0.9321492910385132 train acc: 0.9725000262260437 test loss: 1.039098858833313 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1919 train loss: 0.9322144985198975 train acc: 0.9725000262260437 test loss: 1.0252741575241089 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1920 train loss: 0.9321932792663574 train acc: 0.9725000262260437 test loss: 1.0337177515029907 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1921 train loss: 0.9321749806404114 train acc: 0.9725000262260437 test loss: 1.029565691947937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1922 train loss: 0.9320485591888428 train acc: 0.9725000262260437 test loss: 1.0289791822433472 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1923 train loss: 0.9322653412818909 train acc: 0.9725000262260437 test loss: 1.0352708101272583 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1924 train loss: 0.9323141574859619 train acc: 0.9725000262260437 test loss: 1.024706482887268 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1925 train loss: 0.9321977496147156 train acc: 0.9725000262260437 test loss: 1.0181959867477417 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1926 train loss: 0.9322568774223328 train acc: 0.9725000262260437 test loss: 1.0265558958053589 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1927 train loss: 0.9322270750999451 train acc: 0.9725000262260437 test loss: 1.0298930406570435 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1928 train loss: 0.9321697950363159 train acc: 0.9725000262260437 test loss: 1.0271893739700317 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1929 train loss: 0.9321237206459045 train acc: 0.9725000262260437 test loss: 1.0277010202407837 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1930 train loss: 0.9322076439857483 train acc: 0.9725000262260437 test loss: 1.026516079902649 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1931 train loss: 0.9321977496147156 train acc: 0.9725000262260437 test loss: 1.0209441184997559 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1932 train loss: 0.9320871233940125 train acc: 0.9725000262260437 test loss: 1.022201657295227 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1933 train loss: 0.9321273565292358 train acc: 0.9725000262260437 test loss: 1.0263348817825317 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1934 train loss: 0.9321652054786682 train acc: 0.9725000262260437 test loss: 1.0219969749450684 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1935 train loss: 0.9322054386138916 train acc: 0.9725000262260437 test loss: 1.0256770849227905 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1936 train loss: 0.9320322275161743 train acc: 0.9725000262260437 test loss: 1.0348600149154663 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1937 train loss: 0.9322507977485657 train acc: 0.9725000262260437 test loss: 1.0224928855895996 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1938 train loss: 0.9321887493133545 train acc: 0.9725000262260437 test loss: 1.0289442539215088 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1939 train loss: 0.9321959614753723 train acc: 0.9725000262260437 test loss: 1.0328776836395264 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1940 train loss: 0.9320338368415833 train acc: 0.9725000262260437 test loss: 1.0339728593826294 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1941 train loss: 0.9321328997612 train acc: 0.9725000262260437 test loss: 1.0229188203811646 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1942 train loss: 0.9321208000183105 train acc: 0.9725000262260437 test loss: 1.0283467769622803 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1943 train loss: 0.9321198463439941 train acc: 0.9725000262260437 test loss: 1.0246533155441284 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1944 train loss: 0.9321469068527222 train acc: 0.9725000262260437 test loss: 1.0402833223342896 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1945 train loss: 0.9321697950363159 train acc: 0.9725000262260437 test loss: 1.032393217086792 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1946 train loss: 0.9321842193603516 train acc: 0.9725000262260437 test loss: 1.0219063758850098 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1947 train loss: 0.932163417339325 train acc: 0.9725000262260437 test loss: 1.0271433591842651 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1948 train loss: 0.932157576084137 train acc: 0.9725000262260437 test loss: 1.0343847274780273 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1949 train loss: 0.9322090148925781 train acc: 0.9725000262260437 test loss: 1.029098391532898 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1950 train loss: 0.9320337772369385 train acc: 0.9725000262260437 test loss: 1.031928539276123 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1951 train loss: 0.9321830868721008 train acc: 0.9725000262260437 test loss: 1.0343143939971924 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1952 train loss: 0.9321618676185608 train acc: 0.9725000262260437 test loss: 1.023727297782898 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1953 train loss: 0.932292640209198 train acc: 0.9725000262260437 test loss: 1.0343108177185059 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1954 train loss: 0.9321864247322083 train acc: 0.9725000262260437 test loss: 1.0251659154891968 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1955 train loss: 0.9321838617324829 train acc: 0.9725000262260437 test loss: 1.0255736112594604 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1956 train loss: 0.9321715831756592 train acc: 0.9725000262260437 test loss: 1.0248836278915405 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1957 train loss: 0.9322435855865479 train acc: 0.9725000262260437 test loss: 1.0265840291976929 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1958 train loss: 0.9322065114974976 train acc: 0.9725000262260437 test loss: 1.029917597770691 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1959 train loss: 0.932168185710907 train acc: 0.9725000262260437 test loss: 1.0265194177627563 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1960 train loss: 0.932155966758728 train acc: 0.9725000262260437 test loss: 1.0269784927368164 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1961 train loss: 0.932242751121521 train acc: 0.9725000262260437 test loss: 1.0231248140335083 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1962 train loss: 0.9320710897445679 train acc: 0.9725000262260437 test loss: 1.0259641408920288 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1963 train loss: 0.9321856498718262 train acc: 0.9725000262260437 test loss: 1.0182217359542847 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 1964 train loss: 0.932071328163147 train acc: 0.9725000262260437 test loss: 1.035874843597412 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1965 train loss: 0.9321504831314087 train acc: 0.9725000262260437 test loss: 1.0258735418319702 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1966 train loss: 0.9321896433830261 train acc: 0.9725000262260437 test loss: 1.030562162399292 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1967 train loss: 0.932039201259613 train acc: 0.9725000262260437 test loss: 1.029372215270996 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1968 train loss: 0.9322105646133423 train acc: 0.9725000262260437 test loss: 1.029157280921936 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1969 train loss: 0.9321854114532471 train acc: 0.9725000262260437 test loss: 1.0293052196502686 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1970 train loss: 0.9321630597114563 train acc: 0.9725000262260437 test loss: 1.0287442207336426 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1971 train loss: 0.9320918917655945 train acc: 0.9725000262260437 test loss: 1.032902479171753 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1972 train loss: 0.9320746064186096 train acc: 0.9725000262260437 test loss: 1.0228205919265747 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1973 train loss: 0.9321394562721252 train acc: 0.9725000262260437 test loss: 1.0212386846542358 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1974 train loss: 0.9320821166038513 train acc: 0.9725000262260437 test loss: 1.0267332792282104 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1975 train loss: 0.9320482015609741 train acc: 0.9725000262260437 test loss: 1.0264506340026855 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1976 train loss: 0.9320573210716248 train acc: 0.9725000262260437 test loss: 1.0285652875900269 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1977 train loss: 0.9321672320365906 train acc: 0.9725000262260437 test loss: 1.0234180688858032 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1978 train loss: 0.9322143793106079 train acc: 0.9725000262260437 test loss: 1.0297528505325317 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1979 train loss: 0.9322457909584045 train acc: 0.9725000262260437 test loss: 1.021439552307129 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1980 train loss: 0.9320957064628601 train acc: 0.9725000262260437 test loss: 1.0328600406646729 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1981 train loss: 0.9321689009666443 train acc: 0.9725000262260437 test loss: 1.0273319482803345 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1982 train loss: 0.9321717023849487 train acc: 0.9725000262260437 test loss: 1.0254920721054077 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1983 train loss: 0.932183027267456 train acc: 0.9725000262260437 test loss: 1.0374979972839355 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 1984 train loss: 0.9321780204772949 train acc: 0.9725000262260437 test loss: 1.0293058156967163 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1985 train loss: 0.9321776032447815 train acc: 0.9725000262260437 test loss: 1.022990107536316 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1986 train loss: 0.9321176409721375 train acc: 0.9725000262260437 test loss: 1.0330878496170044 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1987 train loss: 0.9321745038032532 train acc: 0.9725000262260437 test loss: 1.0279673337936401 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1988 train loss: 0.9321834444999695 train acc: 0.9725000262260437 test loss: 1.0257620811462402 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1989 train loss: 0.9320968389511108 train acc: 0.9725000262260437 test loss: 1.021697998046875 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1990 train loss: 0.9320220947265625 train acc: 0.9725000262260437 test loss: 1.0300164222717285 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1991 train loss: 0.9321509003639221 train acc: 0.9725000262260437 test loss: 1.0303492546081543 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1992 train loss: 0.9322507977485657 train acc: 0.9725000262260437 test loss: 1.0304105281829834 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1993 train loss: 0.9321820139884949 train acc: 0.9725000262260437 test loss: 1.0295706987380981 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1994 train loss: 0.9320929050445557 train acc: 0.9725000262260437 test loss: 1.024488091468811 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1995 train loss: 0.932332456111908 train acc: 0.9725000262260437 test loss: 1.0248900651931763 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1996 train loss: 0.9321644306182861 train acc: 0.9725000262260437 test loss: 1.0313866138458252 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 1997 train loss: 0.9321913123130798 train acc: 0.9725000262260437 test loss: 1.028527855873108 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1998 train loss: 0.9320604205131531 train acc: 0.9725000262260437 test loss: 1.0245531797409058 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 1999 train loss: 0.9322025775909424 train acc: 0.9725000262260437 test loss: 1.0174235105514526 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2000 train loss: 0.932198166847229 train acc: 0.9725000262260437 test loss: 1.027703881263733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2001 train loss: 0.9321240186691284 train acc: 0.9725000262260437 test loss: 1.0315135717391968 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2002 train loss: 0.9321447014808655 train acc: 0.9725000262260437 test loss: 1.0241304636001587 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2003 train loss: 0.9320741295814514 train acc: 0.9725000262260437 test loss: 1.0330857038497925 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2004 train loss: 0.9321997165679932 train acc: 0.9725000262260437 test loss: 1.027197241783142 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2005 train loss: 0.9321063756942749 train acc: 0.9725000262260437 test loss: 1.0243486166000366 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2006 train loss: 0.9322224259376526 train acc: 0.9725000262260437 test loss: 1.0224016904830933 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2007 train loss: 0.9321833252906799 train acc: 0.9725000262260437 test loss: 1.0344780683517456 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2008 train loss: 0.9322314262390137 train acc: 0.9725000262260437 test loss: 1.0288364887237549 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2009 train loss: 0.9322031140327454 train acc: 0.9725000262260437 test loss: 1.0251320600509644 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2010 train loss: 0.9321776628494263 train acc: 0.9725000262260437 test loss: 1.022373080253601 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2011 train loss: 0.9320648312568665 train acc: 0.9725000262260437 test loss: 1.0242043733596802 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2012 train loss: 0.9320920705795288 train acc: 0.9725000262260437 test loss: 1.0183089971542358 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2013 train loss: 0.9322932958602905 train acc: 0.9725000262260437 test loss: 1.0217487812042236 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2014 train loss: 0.9321231245994568 train acc: 0.9725000262260437 test loss: 1.0289535522460938 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2015 train loss: 0.9321871995925903 train acc: 0.9725000262260437 test loss: 1.029703974723816 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2016 train loss: 0.9321891069412231 train acc: 0.9725000262260437 test loss: 1.0251487493515015 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2017 train loss: 0.9321824908256531 train acc: 0.9725000262260437 test loss: 1.0325428247451782 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2018 train loss: 0.932138204574585 train acc: 0.9725000262260437 test loss: 1.02873957157135 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2019 train loss: 0.9320905208587646 train acc: 0.9725000262260437 test loss: 1.0245633125305176 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2020 train loss: 0.9321525692939758 train acc: 0.9725000262260437 test loss: 1.0235422849655151 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2021 train loss: 0.932151734828949 train acc: 0.9725000262260437 test loss: 1.0329198837280273 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2022 train loss: 0.9321401715278625 train acc: 0.9725000262260437 test loss: 1.0341747999191284 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2023 train loss: 0.9322060942649841 train acc: 0.9725000262260437 test loss: 1.0303131341934204 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2024 train loss: 0.9321067929267883 train acc: 0.9725000262260437 test loss: 1.0253803730010986 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2025 train loss: 0.9321765899658203 train acc: 0.9725000262260437 test loss: 1.0221683979034424 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2026 train loss: 0.9321351051330566 train acc: 0.9725000262260437 test loss: 1.0305858850479126 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2027 train loss: 0.9321920871734619 train acc: 0.9725000262260437 test loss: 1.033797264099121 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2028 train loss: 0.9320690035820007 train acc: 0.9725000262260437 test loss: 1.0282610654830933 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2029 train loss: 0.9321341514587402 train acc: 0.9725000262260437 test loss: 1.0285438299179077 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2030 train loss: 0.932188093662262 train acc: 0.9725000262260437 test loss: 1.0222067832946777 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2031 train loss: 0.9322192668914795 train acc: 0.9725000262260437 test loss: 1.0269216299057007 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2032 train loss: 0.9321615099906921 train acc: 0.9725000262260437 test loss: 1.0345709323883057 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2033 train loss: 0.9320290088653564 train acc: 0.9725000262260437 test loss: 1.0281164646148682 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2034 train loss: 0.9321629405021667 train acc: 0.9725000262260437 test loss: 1.0260612964630127 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2035 train loss: 0.932116687297821 train acc: 0.9725000262260437 test loss: 1.0349174737930298 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2036 train loss: 0.9321714639663696 train acc: 0.9725000262260437 test loss: 1.0229544639587402 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2037 train loss: 0.9321552515029907 train acc: 0.9725000262260437 test loss: 1.0212727785110474 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2038 train loss: 0.9320441484451294 train acc: 0.9725000262260437 test loss: 1.0275498628616333 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2039 train loss: 0.9321728348731995 train acc: 0.9725000262260437 test loss: 1.0242239236831665 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2040 train loss: 0.9320411682128906 train acc: 0.9725000262260437 test loss: 1.0273246765136719 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2041 train loss: 0.9321098327636719 train acc: 0.9725000262260437 test loss: 1.0243251323699951 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2042 train loss: 0.932050347328186 train acc: 0.9725000262260437 test loss: 1.0258288383483887 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2043 train loss: 0.9321846961975098 train acc: 0.9725000262260437 test loss: 1.0292274951934814 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2044 train loss: 0.9321563243865967 train acc: 0.9725000262260437 test loss: 1.0322492122650146 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2045 train loss: 0.9320312738418579 train acc: 0.9725000262260437 test loss: 1.0250521898269653 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2046 train loss: 0.9321010112762451 train acc: 0.9725000262260437 test loss: 1.0301915407180786 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2047 train loss: 0.9322760105133057 train acc: 0.9725000262260437 test loss: 1.030784249305725 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2048 train loss: 0.9322145581245422 train acc: 0.9725000262260437 test loss: 1.0274642705917358 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2049 train loss: 0.9322155117988586 train acc: 0.9725000262260437 test loss: 1.0287790298461914 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2050 train loss: 0.9321981072425842 train acc: 0.9725000262260437 test loss: 1.036522388458252 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2051 train loss: 0.9321084022521973 train acc: 0.9725000262260437 test loss: 1.0258601903915405 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2052 train loss: 0.932040810585022 train acc: 0.9725000262260437 test loss: 1.0286544561386108 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2053 train loss: 0.9320350885391235 train acc: 0.9725000262260437 test loss: 1.0217468738555908 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2054 train loss: 0.9321997165679932 train acc: 0.9725000262260437 test loss: 1.0282665491104126 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2055 train loss: 0.9321849346160889 train acc: 0.9725000262260437 test loss: 1.0285398960113525 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2056 train loss: 0.932121992111206 train acc: 0.9725000262260437 test loss: 1.0266362428665161 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2057 train loss: 0.9322550892829895 train acc: 0.9725000262260437 test loss: 1.0279461145401 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2058 train loss: 0.9321441650390625 train acc: 0.9725000262260437 test loss: 1.0257487297058105 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2059 train loss: 0.9320904612541199 train acc: 0.9725000262260437 test loss: 1.0323528051376343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2060 train loss: 0.9321352243423462 train acc: 0.9725000262260437 test loss: 1.023431658744812 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2061 train loss: 0.9321081042289734 train acc: 0.9725000262260437 test loss: 1.0241142511367798 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2062 train loss: 0.9320680499076843 train acc: 0.9725000262260437 test loss: 1.0270373821258545 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2063 train loss: 0.9322055578231812 train acc: 0.9725000262260437 test loss: 1.0219115018844604 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2064 train loss: 0.9320477843284607 train acc: 0.9725000262260437 test loss: 1.0234627723693848 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2065 train loss: 0.9321200847625732 train acc: 0.9725000262260437 test loss: 1.0253947973251343 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2066 train loss: 0.9321430325508118 train acc: 0.9725000262260437 test loss: 1.0239365100860596 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2067 train loss: 0.9322040677070618 train acc: 0.9725000262260437 test loss: 1.0289971828460693 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2068 train loss: 0.9320624470710754 train acc: 0.9725000262260437 test loss: 1.0219975709915161 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2069 train loss: 0.9320946335792542 train acc: 0.9725000262260437 test loss: 1.0366493463516235 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2070 train loss: 0.9322018623352051 train acc: 0.9725000262260437 test loss: 1.0290120840072632 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2071 train loss: 0.9321666955947876 train acc: 0.9725000262260437 test loss: 1.0235482454299927 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2072 train loss: 0.9320984482765198 train acc: 0.9725000262260437 test loss: 1.0244253873825073 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2073 train loss: 0.9322154521942139 train acc: 0.9725000262260437 test loss: 1.0306504964828491 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2074 train loss: 0.9320271015167236 train acc: 0.9725000262260437 test loss: 1.0220916271209717 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2075 train loss: 0.9320574402809143 train acc: 0.9725000262260437 test loss: 1.026201605796814 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2076 train loss: 0.9321140050888062 train acc: 0.9725000262260437 test loss: 1.0243067741394043 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2077 train loss: 0.9321774840354919 train acc: 0.9725000262260437 test loss: 1.0248082876205444 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2078 train loss: 0.9320698380470276 train acc: 0.9725000262260437 test loss: 1.0300571918487549 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2079 train loss: 0.9322146773338318 train acc: 0.9725000262260437 test loss: 1.0255117416381836 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2080 train loss: 0.9320957064628601 train acc: 0.9725000262260437 test loss: 1.0246531963348389 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2081 train loss: 0.932211697101593 train acc: 0.9725000262260437 test loss: 1.0259599685668945 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2082 train loss: 0.9321814179420471 train acc: 0.9725000262260437 test loss: 1.0299556255340576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2083 train loss: 0.9321223497390747 train acc: 0.9725000262260437 test loss: 1.0283888578414917 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2084 train loss: 0.9320845007896423 train acc: 0.9725000262260437 test loss: 1.0190308094024658 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2085 train loss: 0.9321199655532837 train acc: 0.9725000262260437 test loss: 1.0294567346572876 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2086 train loss: 0.9321725368499756 train acc: 0.9725000262260437 test loss: 1.0238587856292725 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2087 train loss: 0.9320594668388367 train acc: 0.9725000262260437 test loss: 1.0304534435272217 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2088 train loss: 0.9320624470710754 train acc: 0.9725000262260437 test loss: 1.0270004272460938 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2089 train loss: 0.9321889281272888 train acc: 0.9725000262260437 test loss: 1.0239207744598389 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2090 train loss: 0.9320360422134399 train acc: 0.9725000262260437 test loss: 1.0208741426467896 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2091 train loss: 0.9322621822357178 train acc: 0.9725000262260437 test loss: 1.02642023563385 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2092 train loss: 0.9321702718734741 train acc: 0.9725000262260437 test loss: 1.0313481092453003 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2093 train loss: 0.9321407079696655 train acc: 0.9725000262260437 test loss: 1.0289419889450073 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2094 train loss: 0.9321632981300354 train acc: 0.9725000262260437 test loss: 1.028070330619812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2095 train loss: 0.9320775866508484 train acc: 0.9725000262260437 test loss: 1.0215222835540771 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2096 train loss: 0.932161808013916 train acc: 0.9725000262260437 test loss: 1.0342607498168945 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2097 train loss: 0.9320874214172363 train acc: 0.9725000262260437 test loss: 1.018742561340332 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2098 train loss: 0.9321679472923279 train acc: 0.9725000262260437 test loss: 1.0216431617736816 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2099 train loss: 0.9321694374084473 train acc: 0.9725000262260437 test loss: 1.0314253568649292 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2100 train loss: 0.9321849942207336 train acc: 0.9725000262260437 test loss: 1.0279313325881958 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2101 train loss: 0.9321599006652832 train acc: 0.9725000262260437 test loss: 1.025868535041809 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2102 train loss: 0.9320687055587769 train acc: 0.9725000262260437 test loss: 1.022436261177063 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2103 train loss: 0.9321208000183105 train acc: 0.9725000262260437 test loss: 1.0210703611373901 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2104 train loss: 0.9322542548179626 train acc: 0.9725000262260437 test loss: 1.0285102128982544 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2105 train loss: 0.9322229027748108 train acc: 0.9725000262260437 test loss: 1.026686429977417 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2106 train loss: 0.9322492480278015 train acc: 0.9725000262260437 test loss: 1.0225781202316284 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2107 train loss: 0.9320996999740601 train acc: 0.9725000262260437 test loss: 1.0334715843200684 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2108 train loss: 0.9322654604911804 train acc: 0.9725000262260437 test loss: 1.0265510082244873 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2109 train loss: 0.9322452545166016 train acc: 0.9725000262260437 test loss: 1.0299452543258667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2110 train loss: 0.9321032762527466 train acc: 0.9725000262260437 test loss: 1.021531105041504 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2111 train loss: 0.9322011470794678 train acc: 0.9725000262260437 test loss: 1.0283780097961426 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2112 train loss: 0.9321172833442688 train acc: 0.9725000262260437 test loss: 1.0359673500061035 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2113 train loss: 0.9322656989097595 train acc: 0.9725000262260437 test loss: 1.0271332263946533 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2114 train loss: 0.9322218298912048 train acc: 0.9725000262260437 test loss: 1.024369239807129 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2115 train loss: 0.9320959448814392 train acc: 0.9725000262260437 test loss: 1.0327539443969727 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2116 train loss: 0.9321367144584656 train acc: 0.9725000262260437 test loss: 1.015902042388916 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2117 train loss: 0.932207465171814 train acc: 0.9725000262260437 test loss: 1.0237864255905151 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2118 train loss: 0.9322348833084106 train acc: 0.9725000262260437 test loss: 1.0319414138793945 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2119 train loss: 0.9321819543838501 train acc: 0.9725000262260437 test loss: 1.027492880821228 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2120 train loss: 0.9322435259819031 train acc: 0.9725000262260437 test loss: 1.0262190103530884 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2121 train loss: 0.9322690367698669 train acc: 0.9725000262260437 test loss: 1.0258759260177612 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2122 train loss: 0.9321557879447937 train acc: 0.9725000262260437 test loss: 1.0260058641433716 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2123 train loss: 0.9322211742401123 train acc: 0.9725000262260437 test loss: 1.0328607559204102 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2124 train loss: 0.932091474533081 train acc: 0.9725000262260437 test loss: 1.0315489768981934 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2125 train loss: 0.932117223739624 train acc: 0.9725000262260437 test loss: 1.0360721349716187 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2126 train loss: 0.932114839553833 train acc: 0.9725000262260437 test loss: 1.036961317062378 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2127 train loss: 0.932146430015564 train acc: 0.9725000262260437 test loss: 1.037534236907959 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2128 train loss: 0.9321444630622864 train acc: 0.9725000262260437 test loss: 1.0283300876617432 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2129 train loss: 0.9320839047431946 train acc: 0.9725000262260437 test loss: 1.0224858522415161 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2130 train loss: 0.932045578956604 train acc: 0.9725000262260437 test loss: 1.0362756252288818 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2131 train loss: 0.932231068611145 train acc: 0.9725000262260437 test loss: 1.0263047218322754 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2132 train loss: 0.9320947527885437 train acc: 0.9725000262260437 test loss: 1.0211386680603027 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2133 train loss: 0.9321673512458801 train acc: 0.9725000262260437 test loss: 1.0238624811172485 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2134 train loss: 0.9320672750473022 train acc: 0.9725000262260437 test loss: 1.0244190692901611 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2135 train loss: 0.9321635961532593 train acc: 0.9725000262260437 test loss: 1.027260422706604 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2136 train loss: 0.9321768283843994 train acc: 0.9725000262260437 test loss: 1.0313067436218262 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2137 train loss: 0.9321810007095337 train acc: 0.9725000262260437 test loss: 1.032884120941162 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2138 train loss: 0.932198166847229 train acc: 0.9725000262260437 test loss: 1.0322973728179932 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2139 train loss: 0.9322044253349304 train acc: 0.9725000262260437 test loss: 1.0267257690429688 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2140 train loss: 0.9321620464324951 train acc: 0.9725000262260437 test loss: 1.031801700592041 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2141 train loss: 0.932218074798584 train acc: 0.9725000262260437 test loss: 1.034126877784729 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2142 train loss: 0.9321812987327576 train acc: 0.9725000262260437 test loss: 1.024142861366272 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2143 train loss: 0.932218074798584 train acc: 0.9725000262260437 test loss: 1.0329071283340454 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2144 train loss: 0.9320980310440063 train acc: 0.9725000262260437 test loss: 1.0243711471557617 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2145 train loss: 0.932583749294281 train acc: 0.9725000262260437 test loss: 1.0262281894683838 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2146 train loss: 0.9320537447929382 train acc: 0.9725000262260437 test loss: 1.0362123250961304 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2147 train loss: 0.9321720600128174 train acc: 0.9725000262260437 test loss: 1.0267852544784546 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2148 train loss: 0.9321823120117188 train acc: 0.9725000262260437 test loss: 1.0214810371398926 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2149 train loss: 0.9321073293685913 train acc: 0.9725000262260437 test loss: 1.0255028009414673 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2150 train loss: 0.9322212338447571 train acc: 0.9725000262260437 test loss: 1.0282870531082153 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2151 train loss: 0.9321907162666321 train acc: 0.9725000262260437 test loss: 1.0299437046051025 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2152 train loss: 0.932180643081665 train acc: 0.9725000262260437 test loss: 1.0313314199447632 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2153 train loss: 0.9321998357772827 train acc: 0.9725000262260437 test loss: 1.0313266515731812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2154 train loss: 0.9320728778839111 train acc: 0.9725000262260437 test loss: 1.0242687463760376 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2155 train loss: 0.9321882128715515 train acc: 0.9725000262260437 test loss: 1.0202322006225586 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2156 train loss: 0.9321447014808655 train acc: 0.9725000262260437 test loss: 1.0271751880645752 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2157 train loss: 0.9320496916770935 train acc: 0.9725000262260437 test loss: 1.0290471315383911 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2158 train loss: 0.9321943521499634 train acc: 0.9725000262260437 test loss: 1.0238099098205566 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2159 train loss: 0.9320684671401978 train acc: 0.9725000262260437 test loss: 1.0228782892227173 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2160 train loss: 0.9320456981658936 train acc: 0.9725000262260437 test loss: 1.0333924293518066 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2161 train loss: 0.9320411682128906 train acc: 0.9725000262260437 test loss: 1.0311893224716187 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2162 train loss: 0.9322189092636108 train acc: 0.9725000262260437 test loss: 1.0215973854064941 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2163 train loss: 0.9321526288986206 train acc: 0.9725000262260437 test loss: 1.0309102535247803 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2164 train loss: 0.9322254657745361 train acc: 0.9725000262260437 test loss: 1.0370622873306274 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2165 train loss: 0.932176411151886 train acc: 0.9725000262260437 test loss: 1.0269488096237183 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2166 train loss: 0.9320806264877319 train acc: 0.9725000262260437 test loss: 1.0246210098266602 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2167 train loss: 0.9320548176765442 train acc: 0.9725000262260437 test loss: 1.0245341062545776 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2168 train loss: 0.9321793913841248 train acc: 0.9725000262260437 test loss: 1.0291770696640015 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2169 train loss: 0.9321897029876709 train acc: 0.9725000262260437 test loss: 1.0262839794158936 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2170 train loss: 0.9321264624595642 train acc: 0.9725000262260437 test loss: 1.0254945755004883 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2171 train loss: 0.9320770502090454 train acc: 0.9725000262260437 test loss: 1.0263310670852661 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2172 train loss: 0.9320157766342163 train acc: 0.9725000262260437 test loss: 1.0246448516845703 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2173 train loss: 0.9321057796478271 train acc: 0.9725000262260437 test loss: 1.0290824174880981 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2174 train loss: 0.9320126175880432 train acc: 0.9725000262260437 test loss: 1.0246678590774536 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2175 train loss: 0.9320305585861206 train acc: 0.9725000262260437 test loss: 1.0293453931808472 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2176 train loss: 0.9320585131645203 train acc: 0.9725000262260437 test loss: 1.0312421321868896 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2177 train loss: 0.932076096534729 train acc: 0.9725000262260437 test loss: 1.0244389772415161 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2178 train loss: 0.9322294592857361 train acc: 0.9725000262260437 test loss: 1.0236730575561523 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2179 train loss: 0.9321566820144653 train acc: 0.9725000262260437 test loss: 1.0388803482055664 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2180 train loss: 0.9322073459625244 train acc: 0.9725000262260437 test loss: 1.0261608362197876 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2181 train loss: 0.9322346448898315 train acc: 0.9725000262260437 test loss: 1.0284860134124756 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2182 train loss: 0.93217933177948 train acc: 0.9725000262260437 test loss: 1.0245962142944336 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2183 train loss: 0.932040810585022 train acc: 0.9725000262260437 test loss: 1.04068124294281 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2184 train loss: 0.9322259426116943 train acc: 0.9725000262260437 test loss: 1.0261995792388916 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2185 train loss: 0.9321504235267639 train acc: 0.9725000262260437 test loss: 1.033384084701538 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2186 train loss: 0.9320487976074219 train acc: 0.9725000262260437 test loss: 1.033956527709961 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2187 train loss: 0.9320921897888184 train acc: 0.9725000262260437 test loss: 1.0289348363876343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2188 train loss: 0.932180643081665 train acc: 0.9725000262260437 test loss: 1.022660255432129 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2189 train loss: 0.9321388006210327 train acc: 0.9725000262260437 test loss: 1.0333240032196045 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2190 train loss: 0.9320553541183472 train acc: 0.9725000262260437 test loss: 1.0252742767333984 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2191 train loss: 0.932178795337677 train acc: 0.9725000262260437 test loss: 1.0311516523361206 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2192 train loss: 0.9321788549423218 train acc: 0.9725000262260437 test loss: 1.0343708992004395 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2193 train loss: 0.9321528673171997 train acc: 0.9725000262260437 test loss: 1.0260038375854492 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2194 train loss: 0.9321888089179993 train acc: 0.9725000262260437 test loss: 1.023126244544983 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2195 train loss: 0.9321860671043396 train acc: 0.9725000262260437 test loss: 1.0240122079849243 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2196 train loss: 0.9320793151855469 train acc: 0.9725000262260437 test loss: 1.0443103313446045 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2197 train loss: 0.9320529699325562 train acc: 0.9725000262260437 test loss: 1.0203158855438232 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2198 train loss: 0.932147204875946 train acc: 0.9725000262260437 test loss: 1.0201247930526733 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2199 train loss: 0.9320400357246399 train acc: 0.9725000262260437 test loss: 1.0241142511367798 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2200 train loss: 0.9321755170822144 train acc: 0.9725000262260437 test loss: 1.0346022844314575 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2201 train loss: 0.9321403503417969 train acc: 0.9725000262260437 test loss: 1.0289348363876343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2202 train loss: 0.9321059584617615 train acc: 0.9725000262260437 test loss: 1.032504916191101 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2203 train loss: 0.9320690631866455 train acc: 0.9725000262260437 test loss: 1.0314408540725708 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2204 train loss: 0.9321327805519104 train acc: 0.9725000262260437 test loss: 1.0343663692474365 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2205 train loss: 0.9320421814918518 train acc: 0.9725000262260437 test loss: 1.0219972133636475 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2206 train loss: 0.9320826530456543 train acc: 0.9725000262260437 test loss: 1.0283117294311523 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2207 train loss: 0.9321966767311096 train acc: 0.9725000262260437 test loss: 1.0270997285842896 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2208 train loss: 0.9321052432060242 train acc: 0.9725000262260437 test loss: 1.0272282361984253 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2209 train loss: 0.9321752190589905 train acc: 0.9725000262260437 test loss: 1.025590419769287 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2210 train loss: 0.9322018623352051 train acc: 0.9725000262260437 test loss: 1.0222254991531372 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2211 train loss: 0.9322097301483154 train acc: 0.9725000262260437 test loss: 1.0344637632369995 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2212 train loss: 0.9323132038116455 train acc: 0.9725000262260437 test loss: 1.0277094841003418 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2213 train loss: 0.932220458984375 train acc: 0.9725000262260437 test loss: 1.0256808996200562 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2214 train loss: 0.9322190880775452 train acc: 0.9725000262260437 test loss: 1.023563265800476 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2215 train loss: 0.9322787523269653 train acc: 0.9725000262260437 test loss: 1.0323700904846191 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2216 train loss: 0.932263195514679 train acc: 0.9725000262260437 test loss: 1.033623218536377 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2217 train loss: 0.9321600198745728 train acc: 0.9725000262260437 test loss: 1.029808521270752 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2218 train loss: 0.932231068611145 train acc: 0.9725000262260437 test loss: 1.0189224481582642 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2219 train loss: 0.9322570562362671 train acc: 0.9725000262260437 test loss: 1.030990481376648 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2220 train loss: 0.9321195483207703 train acc: 0.9725000262260437 test loss: 1.0358473062515259 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2221 train loss: 0.9321705102920532 train acc: 0.9725000262260437 test loss: 1.0207908153533936 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2222 train loss: 0.932096004486084 train acc: 0.9725000262260437 test loss: 1.0362201929092407 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2223 train loss: 0.9320746660232544 train acc: 0.9725000262260437 test loss: 1.0191020965576172 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2224 train loss: 0.9320319890975952 train acc: 0.9725000262260437 test loss: 1.0333999395370483 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2225 train loss: 0.9321452379226685 train acc: 0.9725000262260437 test loss: 1.018908977508545 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2226 train loss: 0.9321691393852234 train acc: 0.9725000262260437 test loss: 1.0267852544784546 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2227 train loss: 0.9321311712265015 train acc: 0.9725000262260437 test loss: 1.0237807035446167 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2228 train loss: 0.9320254325866699 train acc: 0.9725000262260437 test loss: 1.0263441801071167 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2229 train loss: 0.9321147799491882 train acc: 0.9725000262260437 test loss: 1.0276668071746826 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2230 train loss: 0.9322472214698792 train acc: 0.9725000262260437 test loss: 1.0179150104522705 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2231 train loss: 0.9321861863136292 train acc: 0.9725000262260437 test loss: 1.0288112163543701 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2232 train loss: 0.9321112632751465 train acc: 0.9725000262260437 test loss: 1.0333685874938965 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2233 train loss: 0.9321830868721008 train acc: 0.9725000262260437 test loss: 1.0203757286071777 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2234 train loss: 0.9321475028991699 train acc: 0.9725000262260437 test loss: 1.032464623451233 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2235 train loss: 0.9320898652076721 train acc: 0.9725000262260437 test loss: 1.0231705904006958 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2236 train loss: 0.9320619106292725 train acc: 0.9725000262260437 test loss: 1.0292147397994995 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2237 train loss: 0.9320874214172363 train acc: 0.9725000262260437 test loss: 1.0383843183517456 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2238 train loss: 0.932202935218811 train acc: 0.9725000262260437 test loss: 1.0280389785766602 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2239 train loss: 0.932077944278717 train acc: 0.9725000262260437 test loss: 1.02605402469635 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2240 train loss: 0.9320642352104187 train acc: 0.9725000262260437 test loss: 1.0304924249649048 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2241 train loss: 0.9320912957191467 train acc: 0.9725000262260437 test loss: 1.0282089710235596 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2242 train loss: 0.9322383999824524 train acc: 0.9725000262260437 test loss: 1.0282464027404785 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2243 train loss: 0.9321008324623108 train acc: 0.9725000262260437 test loss: 1.021196722984314 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2244 train loss: 0.9320860505104065 train acc: 0.9725000262260437 test loss: 1.0327346324920654 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2245 train loss: 0.9321076273918152 train acc: 0.9725000262260437 test loss: 1.0243006944656372 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2246 train loss: 0.9321277737617493 train acc: 0.9725000262260437 test loss: 1.037531852722168 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2247 train loss: 0.9321457147598267 train acc: 0.9725000262260437 test loss: 1.0320355892181396 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2248 train loss: 0.9321457743644714 train acc: 0.9725000262260437 test loss: 1.0202738046646118 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2249 train loss: 0.9321807026863098 train acc: 0.9725000262260437 test loss: 1.0330555438995361 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2250 train loss: 0.9321043491363525 train acc: 0.9725000262260437 test loss: 1.031449794769287 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2251 train loss: 0.9320524334907532 train acc: 0.9725000262260437 test loss: 1.0235025882720947 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2252 train loss: 0.9321181774139404 train acc: 0.9725000262260437 test loss: 1.0249685049057007 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2253 train loss: 0.9320720434188843 train acc: 0.9725000262260437 test loss: 1.0332261323928833 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2254 train loss: 0.9321826696395874 train acc: 0.9725000262260437 test loss: 1.0274879932403564 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2255 train loss: 0.9321879744529724 train acc: 0.9725000262260437 test loss: 1.0253061056137085 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2256 train loss: 0.9322201609611511 train acc: 0.9725000262260437 test loss: 1.032394528388977 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2257 train loss: 0.9320419430732727 train acc: 0.9725000262260437 test loss: 1.0317318439483643 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2258 train loss: 0.9320237636566162 train acc: 0.9725000262260437 test loss: 1.034479022026062 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2259 train loss: 0.9320144057273865 train acc: 0.9725000262260437 test loss: 1.0331976413726807 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2260 train loss: 0.932034432888031 train acc: 0.9725000262260437 test loss: 1.0346165895462036 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2261 train loss: 0.9320310950279236 train acc: 0.9725000262260437 test loss: 1.0245078802108765 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2262 train loss: 0.9320971965789795 train acc: 0.9725000262260437 test loss: 1.0358046293258667 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2263 train loss: 0.932163417339325 train acc: 0.9725000262260437 test loss: 1.0284814834594727 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2264 train loss: 0.9321060180664062 train acc: 0.9725000262260437 test loss: 1.0340776443481445 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2265 train loss: 0.9320383667945862 train acc: 0.9725000262260437 test loss: 1.030088186264038 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2266 train loss: 0.9321226477622986 train acc: 0.9725000262260437 test loss: 1.0309629440307617 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2267 train loss: 0.9321605563163757 train acc: 0.9725000262260437 test loss: 1.0269185304641724 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2268 train loss: 0.9321119785308838 train acc: 0.9725000262260437 test loss: 1.0323271751403809 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2269 train loss: 0.9321710467338562 train acc: 0.9725000262260437 test loss: 1.0309104919433594 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2270 train loss: 0.9321275353431702 train acc: 0.9725000262260437 test loss: 1.0294922590255737 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2271 train loss: 0.9321223497390747 train acc: 0.9725000262260437 test loss: 1.0267682075500488 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2272 train loss: 0.9322013854980469 train acc: 0.9725000262260437 test loss: 1.0297648906707764 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2273 train loss: 0.9321490526199341 train acc: 0.9725000262260437 test loss: 1.0226441621780396 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2274 train loss: 0.9321147203445435 train acc: 0.9725000262260437 test loss: 1.0395258665084839 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2275 train loss: 0.9320141077041626 train acc: 0.9725000262260437 test loss: 1.0364145040512085 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2276 train loss: 0.9321795701980591 train acc: 0.9725000262260437 test loss: 1.0364664793014526 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2277 train loss: 0.932065486907959 train acc: 0.9725000262260437 test loss: 1.025208830833435 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2278 train loss: 0.932037353515625 train acc: 0.9725000262260437 test loss: 1.0361032485961914 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2279 train loss: 0.9320458173751831 train acc: 0.9725000262260437 test loss: 1.0325111150741577 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2280 train loss: 0.9320755004882812 train acc: 0.9725000262260437 test loss: 1.0351935625076294 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2281 train loss: 0.9320555925369263 train acc: 0.9725000262260437 test loss: 1.0363973379135132 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2282 train loss: 0.9321328997612 train acc: 0.9725000262260437 test loss: 1.0198129415512085 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2283 train loss: 0.9321743249893188 train acc: 0.9725000262260437 test loss: 1.0321576595306396 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2284 train loss: 0.9321174025535583 train acc: 0.9725000262260437 test loss: 1.024085521697998 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2285 train loss: 0.9322215914726257 train acc: 0.9725000262260437 test loss: 1.0280176401138306 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2286 train loss: 0.9321467876434326 train acc: 0.9725000262260437 test loss: 1.0237789154052734 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2287 train loss: 0.9320682287216187 train acc: 0.9725000262260437 test loss: 1.0261834859848022 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2288 train loss: 0.9320815205574036 train acc: 0.9725000262260437 test loss: 1.031112551689148 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2289 train loss: 0.9321733117103577 train acc: 0.9725000262260437 test loss: 1.027297019958496 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2290 train loss: 0.932015597820282 train acc: 0.9725000262260437 test loss: 1.0207465887069702 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2291 train loss: 0.932090163230896 train acc: 0.9725000262260437 test loss: 1.029309630393982 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2292 train loss: 0.9320439696311951 train acc: 0.9725000262260437 test loss: 1.0292274951934814 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2293 train loss: 0.9321090579032898 train acc: 0.9725000262260437 test loss: 1.03061842918396 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2294 train loss: 0.9320415258407593 train acc: 0.9725000262260437 test loss: 1.0304392576217651 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2295 train loss: 0.9320449829101562 train acc: 0.9725000262260437 test loss: 1.0327844619750977 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2296 train loss: 0.9320404529571533 train acc: 0.9725000262260437 test loss: 1.0272904634475708 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2297 train loss: 0.9321228265762329 train acc: 0.9725000262260437 test loss: 1.0222831964492798 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2298 train loss: 0.9321629405021667 train acc: 0.9725000262260437 test loss: 1.0262361764907837 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2299 train loss: 0.9321048855781555 train acc: 0.9725000262260437 test loss: 1.0258582830429077 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2300 train loss: 0.9321575164794922 train acc: 0.9725000262260437 test loss: 1.018545150756836 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2301 train loss: 0.9320747256278992 train acc: 0.9725000262260437 test loss: 1.0291310548782349 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2302 train loss: 0.932176411151886 train acc: 0.9725000262260437 test loss: 1.0298842191696167 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2303 train loss: 0.9321491718292236 train acc: 0.9725000262260437 test loss: 1.0267623662948608 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2304 train loss: 0.9320535063743591 train acc: 0.9725000262260437 test loss: 1.0335092544555664 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2305 train loss: 0.9321330785751343 train acc: 0.9725000262260437 test loss: 1.0411015748977661 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2306 train loss: 0.9321539402008057 train acc: 0.9725000262260437 test loss: 1.0298126935958862 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2307 train loss: 0.9319956302642822 train acc: 0.9725000262260437 test loss: 1.0317168235778809 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2308 train loss: 0.932066798210144 train acc: 0.9725000262260437 test loss: 1.0365514755249023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2309 train loss: 0.9319319128990173 train acc: 0.9725000262260437 test loss: 1.0344245433807373 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2310 train loss: 0.9320999979972839 train acc: 0.9725000262260437 test loss: 1.0302350521087646 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2311 train loss: 0.9320968389511108 train acc: 0.9725000262260437 test loss: 1.0286391973495483 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2312 train loss: 0.9320884943008423 train acc: 0.9725000262260437 test loss: 1.0422173738479614 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2313 train loss: 0.9321278929710388 train acc: 0.9725000262260437 test loss: 1.0327675342559814 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2314 train loss: 0.931952953338623 train acc: 0.9725000262260437 test loss: 1.0450607538223267 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2315 train loss: 0.9319416284561157 train acc: 0.9725000262260437 test loss: 1.0271927118301392 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2316 train loss: 0.9320536255836487 train acc: 0.9725000262260437 test loss: 1.0300896167755127 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2317 train loss: 0.9320882558822632 train acc: 0.9725000262260437 test loss: 1.033172845840454 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2318 train loss: 0.9320030212402344 train acc: 0.9725000262260437 test loss: 1.0330564975738525 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2319 train loss: 0.9319531917572021 train acc: 0.9725000262260437 test loss: 1.0313056707382202 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2320 train loss: 0.9320126175880432 train acc: 0.9725000262260437 test loss: 1.0338436365127563 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2321 train loss: 0.9319881200790405 train acc: 0.9725000262260437 test loss: 1.0363574028015137 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2322 train loss: 0.9318426251411438 train acc: 0.9725000262260437 test loss: 1.0281484127044678 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2323 train loss: 0.9318782091140747 train acc: 0.9725000262260437 test loss: 1.032446026802063 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2324 train loss: 0.9319485425949097 train acc: 0.9725000262260437 test loss: 1.032282829284668 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2325 train loss: 0.931908130645752 train acc: 0.9725000262260437 test loss: 1.0304452180862427 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2326 train loss: 0.9318243265151978 train acc: 0.9725000262260437 test loss: 1.0332380533218384 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2327 train loss: 0.9318244457244873 train acc: 0.9725000262260437 test loss: 1.0307048559188843 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2328 train loss: 0.9315746426582336 train acc: 0.9725000262260437 test loss: 1.0343084335327148 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2329 train loss: 0.9314901828765869 train acc: 0.9725000262260437 test loss: 1.0253161191940308 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2330 train loss: 0.9315251708030701 train acc: 0.9725000262260437 test loss: 1.037685513496399 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2331 train loss: 0.9310484528541565 train acc: 0.9750000238418579 test loss: 1.0246742963790894 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2332 train loss: 0.9313051104545593 train acc: 0.9750000238418579 test loss: 1.0441228151321411 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2333 train loss: 0.9309945702552795 train acc: 0.9750000238418579 test loss: 1.0344362258911133 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2334 train loss: 0.9307397603988647 train acc: 0.9750000238418579 test loss: 1.0367153882980347 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2335 train loss: 0.9302942752838135 train acc: 0.9750000238418579 test loss: 1.0386167764663696 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2336 train loss: 0.9301670789718628 train acc: 0.9750000238418579 test loss: 1.0374280214309692 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2337 train loss: 0.9299286007881165 train acc: 0.9750000238418579 test loss: 1.0376251935958862 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2338 train loss: 0.9299734234809875 train acc: 0.9750000238418579 test loss: 1.0266722440719604 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2339 train loss: 0.9300689697265625 train acc: 0.9750000238418579 test loss: 1.0294615030288696 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2340 train loss: 0.9298421740531921 train acc: 0.9750000238418579 test loss: 1.0377578735351562 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2341 train loss: 0.9298707842826843 train acc: 0.9750000238418579 test loss: 1.0256569385528564 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2342 train loss: 0.9300557971000671 train acc: 0.9750000238418579 test loss: 1.0407770872116089 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2343 train loss: 0.9299089312553406 train acc: 0.9750000238418579 test loss: 1.0287551879882812 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2344 train loss: 0.9298633337020874 train acc: 0.9750000238418579 test loss: 1.036989450454712 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2345 train loss: 0.9298399090766907 train acc: 0.9750000238418579 test loss: 1.0236014127731323 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2346 train loss: 0.9298015832901001 train acc: 0.9750000238418579 test loss: 1.0287832021713257 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2347 train loss: 0.9297806620597839 train acc: 0.9750000238418579 test loss: 1.0250048637390137 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2348 train loss: 0.9297545552253723 train acc: 0.9750000238418579 test loss: 1.0345207452774048 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2349 train loss: 0.9297926425933838 train acc: 0.9750000238418579 test loss: 1.03574538230896 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2350 train loss: 0.929874062538147 train acc: 0.9750000238418579 test loss: 1.0287296772003174 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2351 train loss: 0.9298216104507446 train acc: 0.9750000238418579 test loss: 1.0415964126586914 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2352 train loss: 0.9298554062843323 train acc: 0.9750000238418579 test loss: 1.0233474969863892 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2353 train loss: 0.9297538995742798 train acc: 0.9750000238418579 test loss: 1.0309659242630005 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2354 train loss: 0.9299154877662659 train acc: 0.9750000238418579 test loss: 1.0360254049301147 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2355 train loss: 0.9300926327705383 train acc: 0.9750000238418579 test loss: 1.0275100469589233 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2356 train loss: 0.9298315644264221 train acc: 0.9750000238418579 test loss: 1.0279535055160522 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2357 train loss: 0.9302939772605896 train acc: 0.9750000238418579 test loss: 1.0245684385299683 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2358 train loss: 0.9297606945037842 train acc: 0.9750000238418579 test loss: 1.0243728160858154 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2359 train loss: 0.929770827293396 train acc: 0.9750000238418579 test loss: 1.0243743658065796 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2360 train loss: 0.9298660159111023 train acc: 0.9750000238418579 test loss: 1.024914264678955 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2361 train loss: 0.9297986626625061 train acc: 0.9750000238418579 test loss: 1.0266737937927246 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2362 train loss: 0.9298367500305176 train acc: 0.9750000238418579 test loss: 1.0313339233398438 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2363 train loss: 0.929779052734375 train acc: 0.9750000238418579 test loss: 1.0251096487045288 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2364 train loss: 0.929802417755127 train acc: 0.9750000238418579 test loss: 1.0268816947937012 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2365 train loss: 0.9302411079406738 train acc: 0.9750000238418579 test loss: 1.0354043245315552 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2366 train loss: 0.9298619031906128 train acc: 0.9750000238418579 test loss: 1.0297198295593262 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2367 train loss: 0.9298514723777771 train acc: 0.9750000238418579 test loss: 1.0388020277023315 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2368 train loss: 0.9298712015151978 train acc: 0.9750000238418579 test loss: 1.0279535055160522 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2369 train loss: 0.9297036528587341 train acc: 0.9750000238418579 test loss: 1.028145432472229 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2370 train loss: 0.9298403263092041 train acc: 0.9750000238418579 test loss: 1.037075400352478 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2371 train loss: 0.9298272728919983 train acc: 0.9750000238418579 test loss: 1.030327320098877 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2372 train loss: 0.9298193454742432 train acc: 0.9750000238418579 test loss: 1.0324628353118896 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2373 train loss: 0.9297839403152466 train acc: 0.9750000238418579 test loss: 1.0311617851257324 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2374 train loss: 0.929757833480835 train acc: 0.9750000238418579 test loss: 1.0280122756958008 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2375 train loss: 0.9298110008239746 train acc: 0.9750000238418579 test loss: 1.0352745056152344 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2376 train loss: 0.9298900365829468 train acc: 0.9750000238418579 test loss: 1.0299293994903564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2377 train loss: 0.9297021627426147 train acc: 0.9750000238418579 test loss: 1.0268714427947998 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2378 train loss: 0.9298210740089417 train acc: 0.9750000238418579 test loss: 1.0277113914489746 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2379 train loss: 0.9298122525215149 train acc: 0.9750000238418579 test loss: 1.0342762470245361 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2380 train loss: 0.9297463893890381 train acc: 0.9750000238418579 test loss: 1.025194525718689 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2381 train loss: 0.9297323822975159 train acc: 0.9750000238418579 test loss: 1.0379383563995361 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2382 train loss: 0.9298415184020996 train acc: 0.9750000238418579 test loss: 1.0325638055801392 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2383 train loss: 0.9298691749572754 train acc: 0.9750000238418579 test loss: 1.0361236333847046 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2384 train loss: 0.9298510551452637 train acc: 0.9750000238418579 test loss: 1.0320038795471191 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2385 train loss: 0.9297777414321899 train acc: 0.9750000238418579 test loss: 1.0325565338134766 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2386 train loss: 0.9298365116119385 train acc: 0.9750000238418579 test loss: 1.0296210050582886 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2387 train loss: 0.9297681450843811 train acc: 0.9750000238418579 test loss: 1.0301179885864258 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2388 train loss: 0.9297321438789368 train acc: 0.9750000238418579 test loss: 1.027309775352478 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2389 train loss: 0.9298319816589355 train acc: 0.9750000238418579 test loss: 1.0296807289123535 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2390 train loss: 0.9297033548355103 train acc: 0.9750000238418579 test loss: 1.0370051860809326 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2391 train loss: 0.9298179745674133 train acc: 0.9750000238418579 test loss: 1.0293292999267578 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2392 train loss: 0.9299997687339783 train acc: 0.9750000238418579 test loss: 1.035857081413269 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2393 train loss: 0.929695725440979 train acc: 0.9750000238418579 test loss: 1.0311825275421143 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2394 train loss: 0.9296987652778625 train acc: 0.9750000238418579 test loss: 1.0239973068237305 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2395 train loss: 0.9297145009040833 train acc: 0.9750000238418579 test loss: 1.0277858972549438 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2396 train loss: 0.929734468460083 train acc: 0.9750000238418579 test loss: 1.0302377939224243 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2397 train loss: 0.9297908544540405 train acc: 0.9750000238418579 test loss: 1.030876874923706 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2398 train loss: 0.9298447966575623 train acc: 0.9750000238418579 test loss: 1.028993010520935 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2399 train loss: 0.9297026991844177 train acc: 0.9750000238418579 test loss: 1.0384068489074707 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2400 train loss: 0.9296973943710327 train acc: 0.9750000238418579 test loss: 1.0260545015335083 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2401 train loss: 0.9298667311668396 train acc: 0.9750000238418579 test loss: 1.0267117023468018 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2402 train loss: 0.9297422170639038 train acc: 0.9750000238418579 test loss: 1.0251888036727905 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2403 train loss: 0.9298378825187683 train acc: 0.9750000238418579 test loss: 1.0298749208450317 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2404 train loss: 0.929765522480011 train acc: 0.9750000238418579 test loss: 1.031387448310852 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2405 train loss: 0.9297619462013245 train acc: 0.9750000238418579 test loss: 1.0277900695800781 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2406 train loss: 0.9297007918357849 train acc: 0.9750000238418579 test loss: 1.0265945196151733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2407 train loss: 0.9298362731933594 train acc: 0.9750000238418579 test loss: 1.0234477519989014 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2408 train loss: 0.929699182510376 train acc: 0.9750000238418579 test loss: 1.0249576568603516 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2409 train loss: 0.929848313331604 train acc: 0.9750000238418579 test loss: 1.0297588109970093 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2410 train loss: 0.9298032522201538 train acc: 0.9750000238418579 test loss: 1.0272458791732788 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2411 train loss: 0.9298412799835205 train acc: 0.9750000238418579 test loss: 1.0391111373901367 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2412 train loss: 0.9298270344734192 train acc: 0.9750000238418579 test loss: 1.0230790376663208 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2413 train loss: 0.9298388957977295 train acc: 0.9750000238418579 test loss: 1.0274912118911743 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2414 train loss: 0.9297817349433899 train acc: 0.9750000238418579 test loss: 1.0265693664550781 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2415 train loss: 0.9298242926597595 train acc: 0.9750000238418579 test loss: 1.0271191596984863 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2416 train loss: 0.9298194646835327 train acc: 0.9750000238418579 test loss: 1.0254485607147217 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2417 train loss: 0.9298851490020752 train acc: 0.9750000238418579 test loss: 1.0236802101135254 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2418 train loss: 0.9298412203788757 train acc: 0.9750000238418579 test loss: 1.029554843902588 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2419 train loss: 0.9298536777496338 train acc: 0.9750000238418579 test loss: 1.031455636024475 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2420 train loss: 0.9298787713050842 train acc: 0.9750000238418579 test loss: 1.0301114320755005 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2421 train loss: 0.9297609925270081 train acc: 0.9750000238418579 test loss: 1.0363506078720093 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2422 train loss: 0.9297648072242737 train acc: 0.9750000238418579 test loss: 1.0231527090072632 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2423 train loss: 0.9298408627510071 train acc: 0.9750000238418579 test loss: 1.0248762369155884 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2424 train loss: 0.9298510551452637 train acc: 0.9750000238418579 test loss: 1.0292632579803467 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2425 train loss: 0.9298708438873291 train acc: 0.9750000238418579 test loss: 1.0324145555496216 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2426 train loss: 0.9298283457756042 train acc: 0.9750000238418579 test loss: 1.0297749042510986 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2427 train loss: 0.9296948909759521 train acc: 0.9750000238418579 test loss: 1.0266269445419312 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2428 train loss: 0.9298558831214905 train acc: 0.9750000238418579 test loss: 1.0254398584365845 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2429 train loss: 0.9298559427261353 train acc: 0.9750000238418579 test loss: 1.0335193872451782 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2430 train loss: 0.929859459400177 train acc: 0.9750000238418579 test loss: 1.0244730710983276 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2431 train loss: 0.9298859238624573 train acc: 0.9750000238418579 test loss: 1.035264015197754 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2432 train loss: 0.9298520088195801 train acc: 0.9750000238418579 test loss: 1.0249673128128052 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2433 train loss: 0.9298529028892517 train acc: 0.9750000238418579 test loss: 1.0225439071655273 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2434 train loss: 0.9296961426734924 train acc: 0.9750000238418579 test loss: 1.025168776512146 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2435 train loss: 0.9297343492507935 train acc: 0.9750000238418579 test loss: 1.0355169773101807 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2436 train loss: 0.9296923875808716 train acc: 0.9750000238418579 test loss: 1.0257704257965088 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2437 train loss: 0.9298390746116638 train acc: 0.9750000238418579 test loss: 1.0245438814163208 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2438 train loss: 0.9298471808433533 train acc: 0.9750000238418579 test loss: 1.023360252380371 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2439 train loss: 0.929889440536499 train acc: 0.9750000238418579 test loss: 1.0270400047302246 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2440 train loss: 0.9296943545341492 train acc: 0.9750000238418579 test loss: 1.0271633863449097 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2441 train loss: 0.9298559427261353 train acc: 0.9750000238418579 test loss: 1.0206514596939087 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2442 train loss: 0.9296965003013611 train acc: 0.9750000238418579 test loss: 1.024195671081543 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2443 train loss: 0.9298591017723083 train acc: 0.9750000238418579 test loss: 1.0272212028503418 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2444 train loss: 0.9298409223556519 train acc: 0.9750000238418579 test loss: 1.0307763814926147 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2445 train loss: 0.92976975440979 train acc: 0.9750000238418579 test loss: 1.0278491973876953 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2446 train loss: 0.9298591613769531 train acc: 0.9750000238418579 test loss: 1.0338280200958252 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2447 train loss: 0.9297351837158203 train acc: 0.9750000238418579 test loss: 1.0245137214660645 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2448 train loss: 0.9297013878822327 train acc: 0.9750000238418579 test loss: 1.0350558757781982 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2449 train loss: 0.9298458695411682 train acc: 0.9750000238418579 test loss: 1.0267904996871948 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2450 train loss: 0.9298405647277832 train acc: 0.9750000238418579 test loss: 1.0318142175674438 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2451 train loss: 0.9298460483551025 train acc: 0.9750000238418579 test loss: 1.0446857213974 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2452 train loss: 0.9297789931297302 train acc: 0.9750000238418579 test loss: 1.0256174802780151 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2453 train loss: 0.9298459887504578 train acc: 0.9750000238418579 test loss: 1.025112271308899 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2454 train loss: 0.9298575520515442 train acc: 0.9750000238418579 test loss: 1.0309470891952515 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2455 train loss: 0.9297060370445251 train acc: 0.9750000238418579 test loss: 1.0250458717346191 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2456 train loss: 0.9297192096710205 train acc: 0.9750000238418579 test loss: 1.0262706279754639 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2457 train loss: 0.9297184944152832 train acc: 0.9750000238418579 test loss: 1.0291600227355957 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2458 train loss: 0.9298017024993896 train acc: 0.9750000238418579 test loss: 1.0277074575424194 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2459 train loss: 0.9297884106636047 train acc: 0.9750000238418579 test loss: 1.0255658626556396 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2460 train loss: 0.9297427535057068 train acc: 0.9750000238418579 test loss: 1.0274163484573364 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2461 train loss: 0.9298388957977295 train acc: 0.9750000238418579 test loss: 1.0288642644882202 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2462 train loss: 0.9297927618026733 train acc: 0.9750000238418579 test loss: 1.0257083177566528 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2463 train loss: 0.9298401474952698 train acc: 0.9750000238418579 test loss: 1.0279796123504639 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2464 train loss: 0.9297155737876892 train acc: 0.9750000238418579 test loss: 1.0307351350784302 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2465 train loss: 0.9298499226570129 train acc: 0.9750000238418579 test loss: 1.0162559747695923 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2466 train loss: 0.9298387765884399 train acc: 0.9750000238418579 test loss: 1.0261485576629639 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2467 train loss: 0.929790198802948 train acc: 0.9750000238418579 test loss: 1.0247273445129395 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2468 train loss: 0.9298612475395203 train acc: 0.9750000238418579 test loss: 1.0414851903915405 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2469 train loss: 0.9297659993171692 train acc: 0.9750000238418579 test loss: 1.0324349403381348 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2470 train loss: 0.9298081994056702 train acc: 0.9750000238418579 test loss: 1.0241144895553589 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2471 train loss: 0.9297691583633423 train acc: 0.9750000238418579 test loss: 1.0240627527236938 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2472 train loss: 0.9297373294830322 train acc: 0.9750000238418579 test loss: 1.0257117748260498 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2473 train loss: 0.9298003911972046 train acc: 0.9750000238418579 test loss: 1.0333491563796997 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2474 train loss: 0.9297395348548889 train acc: 0.9750000238418579 test loss: 1.0254064798355103 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2475 train loss: 0.9298455715179443 train acc: 0.9750000238418579 test loss: 1.0248496532440186 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2476 train loss: 0.9297180771827698 train acc: 0.9750000238418579 test loss: 1.026787281036377 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2477 train loss: 0.9297710657119751 train acc: 0.9750000238418579 test loss: 1.03192937374115 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2478 train loss: 0.9298338294029236 train acc: 0.9750000238418579 test loss: 1.032294511795044 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2479 train loss: 0.9298022389411926 train acc: 0.9750000238418579 test loss: 1.028407096862793 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2480 train loss: 0.9298343062400818 train acc: 0.9750000238418579 test loss: 1.0256706476211548 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2481 train loss: 0.9298116564750671 train acc: 0.9750000238418579 test loss: 1.0246028900146484 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2482 train loss: 0.9297425746917725 train acc: 0.9750000238418579 test loss: 1.0240753889083862 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2483 train loss: 0.9297454357147217 train acc: 0.9750000238418579 test loss: 1.0337269306182861 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2484 train loss: 0.9297296404838562 train acc: 0.9750000238418579 test loss: 1.0258357524871826 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2485 train loss: 0.9298306107521057 train acc: 0.9750000238418579 test loss: 1.0318224430084229 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2486 train loss: 0.9298248887062073 train acc: 0.9750000238418579 test loss: 1.0261062383651733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2487 train loss: 0.9298039078712463 train acc: 0.9750000238418579 test loss: 1.024910807609558 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2488 train loss: 0.9296876788139343 train acc: 0.9750000238418579 test loss: 1.0282988548278809 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2489 train loss: 0.9298599362373352 train acc: 0.9750000238418579 test loss: 1.0370360612869263 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2490 train loss: 0.9296888113021851 train acc: 0.9750000238418579 test loss: 1.0368624925613403 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2491 train loss: 0.9298506379127502 train acc: 0.9750000238418579 test loss: 1.0255542993545532 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2492 train loss: 0.9296896457672119 train acc: 0.9750000238418579 test loss: 1.03031325340271 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2493 train loss: 0.9298458099365234 train acc: 0.9750000238418579 test loss: 1.0336681604385376 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2494 train loss: 0.9297981858253479 train acc: 0.9750000238418579 test loss: 1.0265933275222778 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2495 train loss: 0.9296861290931702 train acc: 0.9750000238418579 test loss: 1.0282478332519531 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2496 train loss: 0.9297191500663757 train acc: 0.9750000238418579 test loss: 1.0334296226501465 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2497 train loss: 0.9298186302185059 train acc: 0.9750000238418579 test loss: 1.0318368673324585 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2498 train loss: 0.9298079609870911 train acc: 0.9750000238418579 test loss: 1.0301214456558228 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2499 train loss: 0.9298191070556641 train acc: 0.9750000238418579 test loss: 1.0240840911865234 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2500 train loss: 0.929829478263855 train acc: 0.9750000238418579 test loss: 1.0246918201446533 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2501 train loss: 0.9298287034034729 train acc: 0.9750000238418579 test loss: 1.0256669521331787 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2502 train loss: 0.9297617077827454 train acc: 0.9750000238418579 test loss: 1.0345604419708252 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2503 train loss: 0.9298180937767029 train acc: 0.9750000238418579 test loss: 1.0292954444885254 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2504 train loss: 0.9297944903373718 train acc: 0.9750000238418579 test loss: 1.0238624811172485 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2505 train loss: 0.9297422170639038 train acc: 0.9750000238418579 test loss: 1.0304964780807495 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2506 train loss: 0.9296927452087402 train acc: 0.9750000238418579 test loss: 1.0248465538024902 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2507 train loss: 0.9296911358833313 train acc: 0.9750000238418579 test loss: 1.0243018865585327 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2508 train loss: 0.9297550320625305 train acc: 0.9750000238418579 test loss: 1.0265790224075317 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2509 train loss: 0.9297150373458862 train acc: 0.9750000238418579 test loss: 1.0274357795715332 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2510 train loss: 0.9298298358917236 train acc: 0.9750000238418579 test loss: 1.027166485786438 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2511 train loss: 0.9296817183494568 train acc: 0.9750000238418579 test loss: 1.0389848947525024 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2512 train loss: 0.929722011089325 train acc: 0.9750000238418579 test loss: 1.025769829750061 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2513 train loss: 0.9297037720680237 train acc: 0.9750000238418579 test loss: 1.027929663658142 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2514 train loss: 0.9297580718994141 train acc: 0.9750000238418579 test loss: 1.0265780687332153 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2515 train loss: 0.9298369884490967 train acc: 0.9750000238418579 test loss: 1.0343990325927734 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2516 train loss: 0.9297035336494446 train acc: 0.9750000238418579 test loss: 1.0351605415344238 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2517 train loss: 0.9298220872879028 train acc: 0.9750000238418579 test loss: 1.0250988006591797 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2518 train loss: 0.9298468828201294 train acc: 0.9750000238418579 test loss: 1.0260640382766724 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2519 train loss: 0.929747998714447 train acc: 0.9750000238418579 test loss: 1.0277783870697021 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2520 train loss: 0.9296886920928955 train acc: 0.9750000238418579 test loss: 1.0254757404327393 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2521 train loss: 0.9295692443847656 train acc: 0.9750000238418579 test loss: 1.0270997285842896 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2522 train loss: 0.9297741055488586 train acc: 0.9750000238418579 test loss: 1.026917815208435 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2523 train loss: 0.9298223257064819 train acc: 0.9750000238418579 test loss: 1.0240424871444702 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2524 train loss: 0.9298476576805115 train acc: 0.9750000238418579 test loss: 1.0274441242218018 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2525 train loss: 0.9298654198646545 train acc: 0.9750000238418579 test loss: 1.0281779766082764 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2526 train loss: 0.9297027587890625 train acc: 0.9750000238418579 test loss: 1.0226166248321533 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2527 train loss: 0.9297932982444763 train acc: 0.9750000238418579 test loss: 1.0367072820663452 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2528 train loss: 0.9297078847885132 train acc: 0.9750000238418579 test loss: 1.0255229473114014 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2529 train loss: 0.9298422336578369 train acc: 0.9750000238418579 test loss: 1.034804105758667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2530 train loss: 0.9298343658447266 train acc: 0.9750000238418579 test loss: 1.0251092910766602 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2531 train loss: 0.9296732544898987 train acc: 0.9750000238418579 test loss: 1.0257298946380615 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2532 train loss: 0.9298444390296936 train acc: 0.9750000238418579 test loss: 1.0296964645385742 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2533 train loss: 0.9298468828201294 train acc: 0.9750000238418579 test loss: 1.0300570726394653 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2534 train loss: 0.9298336505889893 train acc: 0.9750000238418579 test loss: 1.0254594087600708 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2535 train loss: 0.9297906756401062 train acc: 0.9750000238418579 test loss: 1.0398696660995483 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2536 train loss: 0.9297153949737549 train acc: 0.9750000238418579 test loss: 1.0246772766113281 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2537 train loss: 0.9298264384269714 train acc: 0.9750000238418579 test loss: 1.0313971042633057 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2538 train loss: 0.92974454164505 train acc: 0.9750000238418579 test loss: 1.0354689359664917 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2539 train loss: 0.9298114776611328 train acc: 0.9750000238418579 test loss: 1.0386849641799927 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2540 train loss: 0.9298468828201294 train acc: 0.9750000238418579 test loss: 1.026473045349121 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2541 train loss: 0.9298313856124878 train acc: 0.9750000238418579 test loss: 1.0311427116394043 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2542 train loss: 0.9298524260520935 train acc: 0.9750000238418579 test loss: 1.0300792455673218 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2543 train loss: 0.9297240972518921 train acc: 0.9750000238418579 test loss: 1.0388851165771484 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2544 train loss: 0.9299179315567017 train acc: 0.9750000238418579 test loss: 1.027540683746338 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2545 train loss: 0.9298155307769775 train acc: 0.9750000238418579 test loss: 1.0353742837905884 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2546 train loss: 0.9292030334472656 train acc: 0.9750000238418579 test loss: 1.0419132709503174 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2547 train loss: 0.9292411804199219 train acc: 0.9750000238418579 test loss: 1.0256145000457764 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2548 train loss: 0.9297160506248474 train acc: 0.9750000238418579 test loss: 1.0341029167175293 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2549 train loss: 0.9276634454727173 train acc: 0.9775000214576721 test loss: 1.036934494972229 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2550 train loss: 0.9286245107650757 train acc: 0.9750000238418579 test loss: 1.0347322225570679 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2551 train loss: 0.9278311133384705 train acc: 0.9775000214576721 test loss: 1.0416536331176758 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2552 train loss: 0.9315852522850037 train acc: 0.9750000238418579 test loss: 1.0385528802871704 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2553 train loss: 0.927713930606842 train acc: 0.9775000214576721 test loss: 1.0404075384140015 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2554 train loss: 0.9275752305984497 train acc: 0.9775000214576721 test loss: 1.0319664478302002 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2555 train loss: 0.9272218346595764 train acc: 0.9775000214576721 test loss: 1.0399577617645264 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2556 train loss: 0.9274289608001709 train acc: 0.9775000214576721 test loss: 1.0497034788131714 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2557 train loss: 0.927378237247467 train acc: 0.9775000214576721 test loss: 1.0431727170944214 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2558 train loss: 0.9272779822349548 train acc: 0.9775000214576721 test loss: 1.032465934753418 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2559 train loss: 0.9272809028625488 train acc: 0.9775000214576721 test loss: 1.0377482175827026 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2560 train loss: 0.9273828268051147 train acc: 0.9775000214576721 test loss: 1.0354801416397095 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2561 train loss: 0.9272709488868713 train acc: 0.9775000214576721 test loss: 1.0434919595718384 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2562 train loss: 0.9272820353507996 train acc: 0.9775000214576721 test loss: 1.0544347763061523 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2563 train loss: 0.9273568987846375 train acc: 0.9775000214576721 test loss: 1.0420832633972168 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2564 train loss: 0.9273707866668701 train acc: 0.9775000214576721 test loss: 1.0506867170333862 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2565 train loss: 0.9276482462882996 train acc: 0.9775000214576721 test loss: 1.0471067428588867 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2566 train loss: 0.9274110198020935 train acc: 0.9775000214576721 test loss: 1.0458831787109375 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2567 train loss: 0.9274242520332336 train acc: 0.9775000214576721 test loss: 1.0577919483184814 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 2568 train loss: 0.9274002313613892 train acc: 0.9775000214576721 test loss: 1.0391390323638916 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2569 train loss: 0.9273254871368408 train acc: 0.9775000214576721 test loss: 1.0420544147491455 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2570 train loss: 0.9273872375488281 train acc: 0.9775000214576721 test loss: 1.0368653535842896 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2571 train loss: 0.9273114204406738 train acc: 0.9775000214576721 test loss: 1.0493143796920776 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 2572 train loss: 0.9273082613945007 train acc: 0.9775000214576721 test loss: 1.041658878326416 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2573 train loss: 0.9274085760116577 train acc: 0.9775000214576721 test loss: 1.0470374822616577 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2574 train loss: 0.9273713827133179 train acc: 0.9775000214576721 test loss: 1.0379830598831177 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2575 train loss: 0.9274504780769348 train acc: 0.9775000214576721 test loss: 1.0398393869400024 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2576 train loss: 0.9272701144218445 train acc: 0.9775000214576721 test loss: 1.0399754047393799 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2577 train loss: 0.9272134304046631 train acc: 0.9775000214576721 test loss: 1.0539436340332031 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2578 train loss: 0.9273887872695923 train acc: 0.9775000214576721 test loss: 1.0379611253738403 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2579 train loss: 0.927259624004364 train acc: 0.9775000214576721 test loss: 1.0384618043899536 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2580 train loss: 0.9273078441619873 train acc: 0.9775000214576721 test loss: 1.0402781963348389 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2581 train loss: 0.9273295402526855 train acc: 0.9775000214576721 test loss: 1.04054594039917 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2582 train loss: 0.9273191690444946 train acc: 0.9775000214576721 test loss: 1.0332680940628052 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2583 train loss: 0.9272075891494751 train acc: 0.9775000214576721 test loss: 1.0264654159545898 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2584 train loss: 0.927310585975647 train acc: 0.9775000214576721 test loss: 1.027876853942871 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2585 train loss: 0.9272751808166504 train acc: 0.9775000214576721 test loss: 1.0436803102493286 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2586 train loss: 0.9272655248641968 train acc: 0.9775000214576721 test loss: 1.03554105758667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2587 train loss: 0.9272719025611877 train acc: 0.9775000214576721 test loss: 1.0453596115112305 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2588 train loss: 0.9274179339408875 train acc: 0.9775000214576721 test loss: 1.028900146484375 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2589 train loss: 0.9272248148918152 train acc: 0.9775000214576721 test loss: 1.0391517877578735 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2590 train loss: 0.9271910786628723 train acc: 0.9775000214576721 test loss: 1.0384628772735596 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2591 train loss: 0.9272550940513611 train acc: 0.9775000214576721 test loss: 1.0269474983215332 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2592 train loss: 0.9272103309631348 train acc: 0.9775000214576721 test loss: 1.0339410305023193 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2593 train loss: 0.9272720813751221 train acc: 0.9775000214576721 test loss: 1.0336389541625977 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2594 train loss: 0.9273483157157898 train acc: 0.9775000214576721 test loss: 1.0370410680770874 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2595 train loss: 0.9273557066917419 train acc: 0.9775000214576721 test loss: 1.0327507257461548 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2596 train loss: 0.9273040890693665 train acc: 0.9775000214576721 test loss: 1.0309051275253296 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2597 train loss: 0.9273470044136047 train acc: 0.9775000214576721 test loss: 1.0358514785766602 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2598 train loss: 0.9272943735122681 train acc: 0.9775000214576721 test loss: 1.0408227443695068 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2599 train loss: 0.9273701310157776 train acc: 0.9775000214576721 test loss: 1.0334550142288208 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2600 train loss: 0.9273860454559326 train acc: 0.9775000214576721 test loss: 1.046475887298584 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2601 train loss: 0.9272804260253906 train acc: 0.9775000214576721 test loss: 1.0375972986221313 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2602 train loss: 0.9271941184997559 train acc: 0.9775000214576721 test loss: 1.0321604013442993 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2603 train loss: 0.9273629784584045 train acc: 0.9775000214576721 test loss: 1.0242525339126587 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2604 train loss: 0.9273393750190735 train acc: 0.9775000214576721 test loss: 1.0392029285430908 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2605 train loss: 0.9273069500923157 train acc: 0.9775000214576721 test loss: 1.0372182130813599 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2606 train loss: 0.9273280501365662 train acc: 0.9775000214576721 test loss: 1.0277138948440552 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2607 train loss: 0.9272009134292603 train acc: 0.9775000214576721 test loss: 1.039635419845581 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2608 train loss: 0.9272599816322327 train acc: 0.9775000214576721 test loss: 1.027245044708252 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2609 train loss: 0.9273455142974854 train acc: 0.9775000214576721 test loss: 1.0432251691818237 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2610 train loss: 0.9272159337997437 train acc: 0.9775000214576721 test loss: 1.0450279712677002 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2611 train loss: 0.9272931814193726 train acc: 0.9775000214576721 test loss: 1.037043571472168 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2612 train loss: 0.9272782802581787 train acc: 0.9775000214576721 test loss: 1.0391640663146973 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2613 train loss: 0.9272770881652832 train acc: 0.9775000214576721 test loss: 1.0281798839569092 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2614 train loss: 0.927204430103302 train acc: 0.9775000214576721 test loss: 1.0404930114746094 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2615 train loss: 0.9272041320800781 train acc: 0.9775000214576721 test loss: 1.031160831451416 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2616 train loss: 0.927206814289093 train acc: 0.9775000214576721 test loss: 1.0394506454467773 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2617 train loss: 0.9271963238716125 train acc: 0.9775000214576721 test loss: 1.0357056856155396 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2618 train loss: 0.9272254109382629 train acc: 0.9775000214576721 test loss: 1.0347425937652588 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2619 train loss: 0.9272670149803162 train acc: 0.9775000214576721 test loss: 1.031599521636963 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2620 train loss: 0.9273372888565063 train acc: 0.9775000214576721 test loss: 1.0548747777938843 best test loss: 1.0130972862243652 test acc: 0.8299999833106995\n",
      "Epoch 2621 train loss: 0.9273619055747986 train acc: 0.9775000214576721 test loss: 1.0334123373031616 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2622 train loss: 0.9272435903549194 train acc: 0.9775000214576721 test loss: 1.0387439727783203 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2623 train loss: 0.9272001385688782 train acc: 0.9775000214576721 test loss: 1.0415825843811035 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2624 train loss: 0.9271969795227051 train acc: 0.9775000214576721 test loss: 1.0364482402801514 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2625 train loss: 0.9272451996803284 train acc: 0.9775000214576721 test loss: 1.0309370756149292 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2626 train loss: 0.9271862506866455 train acc: 0.9775000214576721 test loss: 1.0284762382507324 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2627 train loss: 0.9272654056549072 train acc: 0.9775000214576721 test loss: 1.0319494009017944 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2628 train loss: 0.9272094964981079 train acc: 0.9775000214576721 test loss: 1.0403388738632202 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2629 train loss: 0.9271988868713379 train acc: 0.9775000214576721 test loss: 1.0266913175582886 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2630 train loss: 0.9272262454032898 train acc: 0.9775000214576721 test loss: 1.0436615943908691 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2631 train loss: 0.9272300601005554 train acc: 0.9775000214576721 test loss: 1.0391124486923218 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2632 train loss: 0.9272090196609497 train acc: 0.9775000214576721 test loss: 1.0323185920715332 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2633 train loss: 0.9272196292877197 train acc: 0.9775000214576721 test loss: 1.03971266746521 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2634 train loss: 0.9272509217262268 train acc: 0.9775000214576721 test loss: 1.0292035341262817 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2635 train loss: 0.9272366166114807 train acc: 0.9775000214576721 test loss: 1.0300439596176147 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2636 train loss: 0.9272961616516113 train acc: 0.9775000214576721 test loss: 1.0420854091644287 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2637 train loss: 0.9273403882980347 train acc: 0.9775000214576721 test loss: 1.0324163436889648 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2638 train loss: 0.9272377490997314 train acc: 0.9775000214576721 test loss: 1.0302042961120605 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2639 train loss: 0.9273279309272766 train acc: 0.9775000214576721 test loss: 1.0392831563949585 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2640 train loss: 0.9273285865783691 train acc: 0.9775000214576721 test loss: 1.038212537765503 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2641 train loss: 0.9272213578224182 train acc: 0.9775000214576721 test loss: 1.0360982418060303 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2642 train loss: 0.9272823333740234 train acc: 0.9775000214576721 test loss: 1.043992280960083 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2643 train loss: 0.9272640943527222 train acc: 0.9775000214576721 test loss: 1.036394476890564 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2644 train loss: 0.927351176738739 train acc: 0.9775000214576721 test loss: 1.0426714420318604 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2645 train loss: 0.9272215366363525 train acc: 0.9775000214576721 test loss: 1.0361591577529907 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2646 train loss: 0.9273305535316467 train acc: 0.9775000214576721 test loss: 1.0334664583206177 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2647 train loss: 0.9273406863212585 train acc: 0.9775000214576721 test loss: 1.027435541152954 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2648 train loss: 0.9272738695144653 train acc: 0.9775000214576721 test loss: 1.0415785312652588 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2649 train loss: 0.927218496799469 train acc: 0.9775000214576721 test loss: 1.033658504486084 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2650 train loss: 0.927192747592926 train acc: 0.9775000214576721 test loss: 1.0368640422821045 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2651 train loss: 0.9272356629371643 train acc: 0.9775000214576721 test loss: 1.0300285816192627 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2652 train loss: 0.9273127913475037 train acc: 0.9775000214576721 test loss: 1.0304834842681885 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2653 train loss: 0.9272433519363403 train acc: 0.9775000214576721 test loss: 1.0292091369628906 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2654 train loss: 0.9272777438163757 train acc: 0.9775000214576721 test loss: 1.0420695543289185 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2655 train loss: 0.9273310303688049 train acc: 0.9775000214576721 test loss: 1.0398489236831665 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2656 train loss: 0.9273360371589661 train acc: 0.9775000214576721 test loss: 1.0303443670272827 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2657 train loss: 0.9273449182510376 train acc: 0.9775000214576721 test loss: 1.0480724573135376 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2658 train loss: 0.9271981120109558 train acc: 0.9775000214576721 test loss: 1.0446972846984863 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2659 train loss: 0.9273513555526733 train acc: 0.9775000214576721 test loss: 1.025184154510498 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2660 train loss: 0.9273315668106079 train acc: 0.9775000214576721 test loss: 1.0275071859359741 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2661 train loss: 0.9273232817649841 train acc: 0.9775000214576721 test loss: 1.0302660465240479 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2662 train loss: 0.9272469282150269 train acc: 0.9775000214576721 test loss: 1.0433130264282227 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2663 train loss: 0.9272177219390869 train acc: 0.9775000214576721 test loss: 1.032007098197937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2664 train loss: 0.9271929860115051 train acc: 0.9775000214576721 test loss: 1.0442252159118652 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2665 train loss: 0.9271968007087708 train acc: 0.9775000214576721 test loss: 1.0426963567733765 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2666 train loss: 0.9273436665534973 train acc: 0.9775000214576721 test loss: 1.0340909957885742 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2667 train loss: 0.927279531955719 train acc: 0.9775000214576721 test loss: 1.0433411598205566 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2668 train loss: 0.927309513092041 train acc: 0.9775000214576721 test loss: 1.0262526273727417 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2669 train loss: 0.9273468255996704 train acc: 0.9775000214576721 test loss: 1.031878113746643 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2670 train loss: 0.9272493124008179 train acc: 0.9775000214576721 test loss: 1.0306514501571655 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2671 train loss: 0.9272584319114685 train acc: 0.9775000214576721 test loss: 1.0485758781433105 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2672 train loss: 0.9273536801338196 train acc: 0.9775000214576721 test loss: 1.0395015478134155 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2673 train loss: 0.9271929264068604 train acc: 0.9775000214576721 test loss: 1.0401173830032349 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2674 train loss: 0.9272091388702393 train acc: 0.9775000214576721 test loss: 1.0309122800827026 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2675 train loss: 0.9272410869598389 train acc: 0.9775000214576721 test loss: 1.035833716392517 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2676 train loss: 0.9272739887237549 train acc: 0.9775000214576721 test loss: 1.0358030796051025 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2677 train loss: 0.9272468686103821 train acc: 0.9775000214576721 test loss: 1.02619206905365 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2678 train loss: 0.9273484945297241 train acc: 0.9775000214576721 test loss: 1.033358097076416 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2679 train loss: 0.9273325204849243 train acc: 0.9775000214576721 test loss: 1.0296940803527832 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2680 train loss: 0.9273611307144165 train acc: 0.9775000214576721 test loss: 1.0362868309020996 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2681 train loss: 0.9271882772445679 train acc: 0.9775000214576721 test loss: 1.0328632593154907 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2682 train loss: 0.9272223114967346 train acc: 0.9775000214576721 test loss: 1.0352643728256226 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2683 train loss: 0.927276611328125 train acc: 0.9775000214576721 test loss: 1.0298149585723877 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2684 train loss: 0.9272180795669556 train acc: 0.9775000214576721 test loss: 1.0363364219665527 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2685 train loss: 0.9272233843803406 train acc: 0.9775000214576721 test loss: 1.041215181350708 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2686 train loss: 0.9272139668464661 train acc: 0.9775000214576721 test loss: 1.0377438068389893 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2687 train loss: 0.9271975159645081 train acc: 0.9775000214576721 test loss: 1.0372838973999023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2688 train loss: 0.9272308349609375 train acc: 0.9775000214576721 test loss: 1.029974341392517 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2689 train loss: 0.9272394776344299 train acc: 0.9775000214576721 test loss: 1.0350288152694702 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2690 train loss: 0.9272739291191101 train acc: 0.9775000214576721 test loss: 1.0453417301177979 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2691 train loss: 0.9272090196609497 train acc: 0.9775000214576721 test loss: 1.0293850898742676 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2692 train loss: 0.9271993041038513 train acc: 0.9775000214576721 test loss: 1.030460000038147 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2693 train loss: 0.9273142218589783 train acc: 0.9775000214576721 test loss: 1.0307645797729492 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2694 train loss: 0.9273484945297241 train acc: 0.9775000214576721 test loss: 1.0285451412200928 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2695 train loss: 0.9273011684417725 train acc: 0.9775000214576721 test loss: 1.0316921472549438 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2696 train loss: 0.9272907972335815 train acc: 0.9775000214576721 test loss: 1.042456865310669 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2697 train loss: 0.9272750616073608 train acc: 0.9775000214576721 test loss: 1.0419917106628418 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2698 train loss: 0.9273378252983093 train acc: 0.9775000214576721 test loss: 1.0353845357894897 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2699 train loss: 0.9271843433380127 train acc: 0.9775000214576721 test loss: 1.0266021490097046 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2700 train loss: 0.9272929430007935 train acc: 0.9775000214576721 test loss: 1.0355653762817383 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2701 train loss: 0.9272330403327942 train acc: 0.9775000214576721 test loss: 1.0351659059524536 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2702 train loss: 0.9273438453674316 train acc: 0.9775000214576721 test loss: 1.042536973953247 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2703 train loss: 0.9273462891578674 train acc: 0.9775000214576721 test loss: 1.031493902206421 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2704 train loss: 0.9273558259010315 train acc: 0.9775000214576721 test loss: 1.0443522930145264 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2705 train loss: 0.9272814393043518 train acc: 0.9775000214576721 test loss: 1.0396391153335571 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2706 train loss: 0.9272207021713257 train acc: 0.9775000214576721 test loss: 1.0453518629074097 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2707 train loss: 0.9273325204849243 train acc: 0.9775000214576721 test loss: 1.0302743911743164 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2708 train loss: 0.9272723197937012 train acc: 0.9775000214576721 test loss: 1.0317878723144531 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2709 train loss: 0.9273228645324707 train acc: 0.9775000214576721 test loss: 1.0377142429351807 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2710 train loss: 0.9273149371147156 train acc: 0.9775000214576721 test loss: 1.0348600149154663 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2711 train loss: 0.9271787405014038 train acc: 0.9775000214576721 test loss: 1.0425702333450317 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2712 train loss: 0.9271860718727112 train acc: 0.9775000214576721 test loss: 1.039475440979004 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2713 train loss: 0.9273371696472168 train acc: 0.9775000214576721 test loss: 1.03154718875885 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2714 train loss: 0.9272060990333557 train acc: 0.9775000214576721 test loss: 1.0469833612442017 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2715 train loss: 0.9272949695587158 train acc: 0.9775000214576721 test loss: 1.027207374572754 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2716 train loss: 0.9272211194038391 train acc: 0.9775000214576721 test loss: 1.035010814666748 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2717 train loss: 0.9273436665534973 train acc: 0.9775000214576721 test loss: 1.0376298427581787 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2718 train loss: 0.9273468255996704 train acc: 0.9775000214576721 test loss: 1.0335912704467773 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2719 train loss: 0.9272264242172241 train acc: 0.9775000214576721 test loss: 1.034022569656372 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2720 train loss: 0.9273446798324585 train acc: 0.9775000214576721 test loss: 1.0299264192581177 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2721 train loss: 0.9273329377174377 train acc: 0.9775000214576721 test loss: 1.038865089416504 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2722 train loss: 0.9272875785827637 train acc: 0.9775000214576721 test loss: 1.0289998054504395 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2723 train loss: 0.9274140000343323 train acc: 0.9775000214576721 test loss: 1.0287282466888428 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2724 train loss: 0.9272570013999939 train acc: 0.9775000214576721 test loss: 1.0316269397735596 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2725 train loss: 0.9272060990333557 train acc: 0.9775000214576721 test loss: 1.0313962697982788 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2726 train loss: 0.927203893661499 train acc: 0.9775000214576721 test loss: 1.0378162860870361 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2727 train loss: 0.9272104501724243 train acc: 0.9775000214576721 test loss: 1.0418446063995361 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2728 train loss: 0.9273087382316589 train acc: 0.9775000214576721 test loss: 1.0351179838180542 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2729 train loss: 0.9272421002388 train acc: 0.9775000214576721 test loss: 1.030389666557312 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2730 train loss: 0.9273149371147156 train acc: 0.9775000214576721 test loss: 1.0403640270233154 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2731 train loss: 0.9272563457489014 train acc: 0.9775000214576721 test loss: 1.0301891565322876 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2732 train loss: 0.9273414611816406 train acc: 0.9775000214576721 test loss: 1.0294569730758667 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2733 train loss: 0.9272110462188721 train acc: 0.9775000214576721 test loss: 1.0322482585906982 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2734 train loss: 0.9272071719169617 train acc: 0.9775000214576721 test loss: 1.0356882810592651 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2735 train loss: 0.9272280931472778 train acc: 0.9775000214576721 test loss: 1.037178635597229 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2736 train loss: 0.9273151159286499 train acc: 0.9775000214576721 test loss: 1.0382587909698486 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2737 train loss: 0.927348792552948 train acc: 0.9775000214576721 test loss: 1.0268677473068237 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2738 train loss: 0.9273088574409485 train acc: 0.9775000214576721 test loss: 1.0341874361038208 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2739 train loss: 0.9273129105567932 train acc: 0.9775000214576721 test loss: 1.0280096530914307 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2740 train loss: 0.9273176789283752 train acc: 0.9775000214576721 test loss: 1.0372366905212402 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2741 train loss: 0.9273015856742859 train acc: 0.9775000214576721 test loss: 1.0322418212890625 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2742 train loss: 0.9272237420082092 train acc: 0.9775000214576721 test loss: 1.0370842218399048 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2743 train loss: 0.9272968173027039 train acc: 0.9775000214576721 test loss: 1.039794683456421 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2744 train loss: 0.927290141582489 train acc: 0.9775000214576721 test loss: 1.0262072086334229 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 2745 train loss: 0.9272027611732483 train acc: 0.9775000214576721 test loss: 1.0381032228469849 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2746 train loss: 0.927328884601593 train acc: 0.9775000214576721 test loss: 1.0273187160491943 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2747 train loss: 0.9271959066390991 train acc: 0.9775000214576721 test loss: 1.0312449932098389 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2748 train loss: 0.9272213578224182 train acc: 0.9775000214576721 test loss: 1.0329504013061523 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2749 train loss: 0.9271824359893799 train acc: 0.9775000214576721 test loss: 1.03681218624115 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2750 train loss: 0.9272980690002441 train acc: 0.9775000214576721 test loss: 1.0282599925994873 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2751 train loss: 0.9271976351737976 train acc: 0.9775000214576721 test loss: 1.029561161994934 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2752 train loss: 0.9271826148033142 train acc: 0.9775000214576721 test loss: 1.0287081003189087 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2753 train loss: 0.9273025989532471 train acc: 0.9775000214576721 test loss: 1.0275545120239258 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2754 train loss: 0.9273261427879333 train acc: 0.9775000214576721 test loss: 1.0367436408996582 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2755 train loss: 0.9273439049720764 train acc: 0.9775000214576721 test loss: 1.0409607887268066 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2756 train loss: 0.9272692799568176 train acc: 0.9775000214576721 test loss: 1.0397663116455078 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2757 train loss: 0.9272018671035767 train acc: 0.9775000214576721 test loss: 1.0273923873901367 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2758 train loss: 0.9273466467857361 train acc: 0.9775000214576721 test loss: 1.0398414134979248 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2759 train loss: 0.9273226857185364 train acc: 0.9775000214576721 test loss: 1.0397056341171265 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2760 train loss: 0.9272186160087585 train acc: 0.9775000214576721 test loss: 1.0340380668640137 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2761 train loss: 0.9272775053977966 train acc: 0.9775000214576721 test loss: 1.0291321277618408 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2762 train loss: 0.9272695183753967 train acc: 0.9775000214576721 test loss: 1.0361841917037964 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2763 train loss: 0.927337110042572 train acc: 0.9775000214576721 test loss: 1.0319643020629883 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2764 train loss: 0.9272340536117554 train acc: 0.9775000214576721 test loss: 1.035212516784668 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2765 train loss: 0.9273357391357422 train acc: 0.9775000214576721 test loss: 1.0414191484451294 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2766 train loss: 0.9273307919502258 train acc: 0.9775000214576721 test loss: 1.0302748680114746 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2767 train loss: 0.9272564053535461 train acc: 0.9775000214576721 test loss: 1.0327531099319458 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2768 train loss: 0.9272174835205078 train acc: 0.9775000214576721 test loss: 1.0262326002120972 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2769 train loss: 0.927243173122406 train acc: 0.9775000214576721 test loss: 1.033862829208374 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2770 train loss: 0.9271855354309082 train acc: 0.9775000214576721 test loss: 1.0285670757293701 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2771 train loss: 0.9273315668106079 train acc: 0.9775000214576721 test loss: 1.0409903526306152 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2772 train loss: 0.9273420572280884 train acc: 0.9775000214576721 test loss: 1.0333397388458252 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2773 train loss: 0.9271859526634216 train acc: 0.9775000214576721 test loss: 1.0432161092758179 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2774 train loss: 0.9273234009742737 train acc: 0.9775000214576721 test loss: 1.0306881666183472 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2775 train loss: 0.9271826148033142 train acc: 0.9775000214576721 test loss: 1.0319945812225342 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2776 train loss: 0.9272534251213074 train acc: 0.9775000214576721 test loss: 1.0275640487670898 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2777 train loss: 0.9272384643554688 train acc: 0.9775000214576721 test loss: 1.038466453552246 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2778 train loss: 0.9272840023040771 train acc: 0.9775000214576721 test loss: 1.0364165306091309 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2779 train loss: 0.9272419214248657 train acc: 0.9775000214576721 test loss: 1.0360894203186035 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2780 train loss: 0.9271855354309082 train acc: 0.9775000214576721 test loss: 1.0310487747192383 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2781 train loss: 0.9272188544273376 train acc: 0.9775000214576721 test loss: 1.032177209854126 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2782 train loss: 0.9272627830505371 train acc: 0.9775000214576721 test loss: 1.043190360069275 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2783 train loss: 0.9271919131278992 train acc: 0.9775000214576721 test loss: 1.0330790281295776 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2784 train loss: 0.9272966980934143 train acc: 0.9775000214576721 test loss: 1.0289912223815918 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2785 train loss: 0.9273386597633362 train acc: 0.9775000214576721 test loss: 1.0325074195861816 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2786 train loss: 0.9272897243499756 train acc: 0.9775000214576721 test loss: 1.0322209596633911 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2787 train loss: 0.9272182583808899 train acc: 0.9775000214576721 test loss: 1.0298497676849365 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2788 train loss: 0.9272018671035767 train acc: 0.9775000214576721 test loss: 1.0279136896133423 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2789 train loss: 0.9272521138191223 train acc: 0.9775000214576721 test loss: 1.0368516445159912 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2790 train loss: 0.9272273778915405 train acc: 0.9775000214576721 test loss: 1.0264981985092163 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2791 train loss: 0.9272968769073486 train acc: 0.9775000214576721 test loss: 1.0401349067687988 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2792 train loss: 0.9273537397384644 train acc: 0.9775000214576721 test loss: 1.0357298851013184 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2793 train loss: 0.9272511005401611 train acc: 0.9775000214576721 test loss: 1.0359394550323486 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2794 train loss: 0.9273359179496765 train acc: 0.9775000214576721 test loss: 1.0354852676391602 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2795 train loss: 0.9272907376289368 train acc: 0.9775000214576721 test loss: 1.0281267166137695 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2796 train loss: 0.9272752404212952 train acc: 0.9775000214576721 test loss: 1.0435669422149658 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2797 train loss: 0.9273144602775574 train acc: 0.9775000214576721 test loss: 1.0327837467193604 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2798 train loss: 0.9273247718811035 train acc: 0.9775000214576721 test loss: 1.0344637632369995 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2799 train loss: 0.9272657632827759 train acc: 0.9775000214576721 test loss: 1.0361672639846802 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2800 train loss: 0.9273390173912048 train acc: 0.9775000214576721 test loss: 1.0304313898086548 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2801 train loss: 0.9273287057876587 train acc: 0.9775000214576721 test loss: 1.0243195295333862 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2802 train loss: 0.9272757768630981 train acc: 0.9775000214576721 test loss: 1.0433714389801025 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2803 train loss: 0.9273194670677185 train acc: 0.9775000214576721 test loss: 1.027313232421875 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2804 train loss: 0.9272239804267883 train acc: 0.9775000214576721 test loss: 1.0304304361343384 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2805 train loss: 0.9273440837860107 train acc: 0.9775000214576721 test loss: 1.0340847969055176 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2806 train loss: 0.9273611307144165 train acc: 0.9775000214576721 test loss: 1.0313458442687988 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2807 train loss: 0.9273360967636108 train acc: 0.9775000214576721 test loss: 1.0293036699295044 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2808 train loss: 0.9273592233657837 train acc: 0.9775000214576721 test loss: 1.029403567314148 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2809 train loss: 0.927338182926178 train acc: 0.9775000214576721 test loss: 1.0329240560531616 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2810 train loss: 0.9273402690887451 train acc: 0.9775000214576721 test loss: 1.0347763299942017 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2811 train loss: 0.9272265434265137 train acc: 0.9775000214576721 test loss: 1.0398344993591309 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2812 train loss: 0.9272769093513489 train acc: 0.9775000214576721 test loss: 1.043723702430725 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2813 train loss: 0.9273008704185486 train acc: 0.9775000214576721 test loss: 1.0272700786590576 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2814 train loss: 0.9272791147232056 train acc: 0.9775000214576721 test loss: 1.0356639623641968 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2815 train loss: 0.9272310137748718 train acc: 0.9775000214576721 test loss: 1.0273476839065552 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2816 train loss: 0.9273354411125183 train acc: 0.9775000214576721 test loss: 1.0352716445922852 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2817 train loss: 0.9273341298103333 train acc: 0.9775000214576721 test loss: 1.026609182357788 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2818 train loss: 0.9271944165229797 train acc: 0.9775000214576721 test loss: 1.0391398668289185 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2819 train loss: 0.9273210167884827 train acc: 0.9775000214576721 test loss: 1.027138113975525 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2820 train loss: 0.9272373914718628 train acc: 0.9775000214576721 test loss: 1.027248501777649 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2821 train loss: 0.927340030670166 train acc: 0.9775000214576721 test loss: 1.0354230403900146 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2822 train loss: 0.9273426532745361 train acc: 0.9775000214576721 test loss: 1.0379860401153564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2823 train loss: 0.9272698163986206 train acc: 0.9775000214576721 test loss: 1.025586724281311 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2824 train loss: 0.9273461699485779 train acc: 0.9775000214576721 test loss: 1.0347673892974854 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2825 train loss: 0.9273409247398376 train acc: 0.9775000214576721 test loss: 1.02940034866333 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2826 train loss: 0.9271956086158752 train acc: 0.9775000214576721 test loss: 1.039868712425232 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2827 train loss: 0.9273481965065002 train acc: 0.9775000214576721 test loss: 1.0295970439910889 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2828 train loss: 0.9272616505622864 train acc: 0.9775000214576721 test loss: 1.0258753299713135 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2829 train loss: 0.927321195602417 train acc: 0.9775000214576721 test loss: 1.0320550203323364 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2830 train loss: 0.9273627400398254 train acc: 0.9775000214576721 test loss: 1.0289283990859985 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2831 train loss: 0.9273425340652466 train acc: 0.9775000214576721 test loss: 1.0315862894058228 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2832 train loss: 0.9273430705070496 train acc: 0.9775000214576721 test loss: 1.0442461967468262 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2833 train loss: 0.9272125363349915 train acc: 0.9775000214576721 test loss: 1.0360738039016724 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2834 train loss: 0.9273422360420227 train acc: 0.9775000214576721 test loss: 1.0388917922973633 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2835 train loss: 0.9273446798324585 train acc: 0.9775000214576721 test loss: 1.0394498109817505 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2836 train loss: 0.9273442029953003 train acc: 0.9775000214576721 test loss: 1.037042498588562 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2837 train loss: 0.927347719669342 train acc: 0.9775000214576721 test loss: 1.0254327058792114 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2838 train loss: 0.927262544631958 train acc: 0.9775000214576721 test loss: 1.0230247974395752 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2839 train loss: 0.9273393750190735 train acc: 0.9775000214576721 test loss: 1.03737473487854 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2840 train loss: 0.9273395538330078 train acc: 0.9775000214576721 test loss: 1.0425738096237183 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2841 train loss: 0.9273319840431213 train acc: 0.9775000214576721 test loss: 1.0270041227340698 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2842 train loss: 0.9273256659507751 train acc: 0.9775000214576721 test loss: 1.0347261428833008 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2843 train loss: 0.9273412823677063 train acc: 0.9775000214576721 test loss: 1.0464084148406982 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2844 train loss: 0.9271780252456665 train acc: 0.9775000214576721 test loss: 1.026205062866211 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2845 train loss: 0.9273477792739868 train acc: 0.9775000214576721 test loss: 1.037757158279419 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2846 train loss: 0.9273137450218201 train acc: 0.9775000214576721 test loss: 1.0313972234725952 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2847 train loss: 0.9273084998130798 train acc: 0.9775000214576721 test loss: 1.0367050170898438 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2848 train loss: 0.9272981286048889 train acc: 0.9775000214576721 test loss: 1.0274863243103027 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2849 train loss: 0.9273065328598022 train acc: 0.9775000214576721 test loss: 1.0275779962539673 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2850 train loss: 0.9272318482398987 train acc: 0.9775000214576721 test loss: 1.0365848541259766 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2851 train loss: 0.9272667169570923 train acc: 0.9775000214576721 test loss: 1.0328924655914307 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2852 train loss: 0.9272729754447937 train acc: 0.9775000214576721 test loss: 1.0332262516021729 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2853 train loss: 0.9272142052650452 train acc: 0.9775000214576721 test loss: 1.0361789464950562 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2854 train loss: 0.9272096157073975 train acc: 0.9775000214576721 test loss: 1.0395511388778687 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2855 train loss: 0.9271824359893799 train acc: 0.9775000214576721 test loss: 1.027662992477417 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2856 train loss: 0.927277684211731 train acc: 0.9775000214576721 test loss: 1.0397212505340576 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2857 train loss: 0.9272682070732117 train acc: 0.9775000214576721 test loss: 1.039604902267456 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2858 train loss: 0.9271804690361023 train acc: 0.9775000214576721 test loss: 1.0265915393829346 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2859 train loss: 0.9272540211677551 train acc: 0.9775000214576721 test loss: 1.0356450080871582 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2860 train loss: 0.9271842241287231 train acc: 0.9775000214576721 test loss: 1.0258272886276245 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2861 train loss: 0.9272780418395996 train acc: 0.9775000214576721 test loss: 1.0351332426071167 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2862 train loss: 0.9272004961967468 train acc: 0.9775000214576721 test loss: 1.0282979011535645 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2863 train loss: 0.9272410869598389 train acc: 0.9775000214576721 test loss: 1.0286651849746704 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2864 train loss: 0.9271845817565918 train acc: 0.9775000214576721 test loss: 1.0266934633255005 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2865 train loss: 0.9273616075515747 train acc: 0.9775000214576721 test loss: 1.0266720056533813 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2866 train loss: 0.9272860884666443 train acc: 0.9775000214576721 test loss: 1.0303988456726074 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2867 train loss: 0.9272145628929138 train acc: 0.9775000214576721 test loss: 1.0450481176376343 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2868 train loss: 0.9272112250328064 train acc: 0.9775000214576721 test loss: 1.0414090156555176 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2869 train loss: 0.9273209571838379 train acc: 0.9775000214576721 test loss: 1.029083251953125 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2870 train loss: 0.927193820476532 train acc: 0.9775000214576721 test loss: 1.0343300104141235 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2871 train loss: 0.9272327423095703 train acc: 0.9775000214576721 test loss: 1.0336060523986816 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2872 train loss: 0.9272912740707397 train acc: 0.9775000214576721 test loss: 1.04583740234375 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2873 train loss: 0.9272077679634094 train acc: 0.9775000214576721 test loss: 1.033522605895996 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2874 train loss: 0.9272142052650452 train acc: 0.9775000214576721 test loss: 1.0304479598999023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2875 train loss: 0.9273378849029541 train acc: 0.9775000214576721 test loss: 1.0464637279510498 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2876 train loss: 0.9273485541343689 train acc: 0.9775000214576721 test loss: 1.0275349617004395 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2877 train loss: 0.9272660613059998 train acc: 0.9775000214576721 test loss: 1.0288701057434082 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2878 train loss: 0.9271978735923767 train acc: 0.9775000214576721 test loss: 1.0359100103378296 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2879 train loss: 0.9272651076316833 train acc: 0.9775000214576721 test loss: 1.0268000364303589 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2880 train loss: 0.927323579788208 train acc: 0.9775000214576721 test loss: 1.0418568849563599 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2881 train loss: 0.9271999597549438 train acc: 0.9775000214576721 test loss: 1.0305073261260986 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2882 train loss: 0.9273340106010437 train acc: 0.9775000214576721 test loss: 1.0371204614639282 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2883 train loss: 0.9271984100341797 train acc: 0.9775000214576721 test loss: 1.0306472778320312 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2884 train loss: 0.9271906018257141 train acc: 0.9775000214576721 test loss: 1.0273661613464355 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2885 train loss: 0.9272482991218567 train acc: 0.9775000214576721 test loss: 1.034812092781067 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2886 train loss: 0.9273374080657959 train acc: 0.9775000214576721 test loss: 1.034812569618225 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2887 train loss: 0.9271928668022156 train acc: 0.9775000214576721 test loss: 1.0368801355361938 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2888 train loss: 0.9273055791854858 train acc: 0.9775000214576721 test loss: 1.0337059497833252 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2889 train loss: 0.9271825551986694 train acc: 0.9775000214576721 test loss: 1.0319842100143433 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2890 train loss: 0.9273161292076111 train acc: 0.9775000214576721 test loss: 1.0259910821914673 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2891 train loss: 0.9273010492324829 train acc: 0.9775000214576721 test loss: 1.0375677347183228 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2892 train loss: 0.9272332191467285 train acc: 0.9775000214576721 test loss: 1.0329779386520386 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2893 train loss: 0.9273566603660583 train acc: 0.9775000214576721 test loss: 1.0278884172439575 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2894 train loss: 0.9272058010101318 train acc: 0.9775000214576721 test loss: 1.0264395475387573 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2895 train loss: 0.9272482991218567 train acc: 0.9775000214576721 test loss: 1.025036334991455 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2896 train loss: 0.9272933006286621 train acc: 0.9775000214576721 test loss: 1.0349887609481812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2897 train loss: 0.9273384809494019 train acc: 0.9775000214576721 test loss: 1.028580665588379 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2898 train loss: 0.9272087812423706 train acc: 0.9775000214576721 test loss: 1.0317710638046265 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2899 train loss: 0.9271823167800903 train acc: 0.9775000214576721 test loss: 1.026599645614624 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2900 train loss: 0.9271872639656067 train acc: 0.9775000214576721 test loss: 1.0321804285049438 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2901 train loss: 0.9271867275238037 train acc: 0.9775000214576721 test loss: 1.0419150590896606 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2902 train loss: 0.9273037910461426 train acc: 0.9775000214576721 test loss: 1.0315525531768799 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2903 train loss: 0.9273301959037781 train acc: 0.9775000214576721 test loss: 1.033106803894043 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2904 train loss: 0.927258312702179 train acc: 0.9775000214576721 test loss: 1.0256754159927368 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2905 train loss: 0.9272324442863464 train acc: 0.9775000214576721 test loss: 1.03436279296875 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2906 train loss: 0.9273291230201721 train acc: 0.9775000214576721 test loss: 1.0317903757095337 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2907 train loss: 0.9273965358734131 train acc: 0.9775000214576721 test loss: 1.0338032245635986 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2908 train loss: 0.9273388385772705 train acc: 0.9775000214576721 test loss: 1.038335919380188 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2909 train loss: 0.9272136092185974 train acc: 0.9775000214576721 test loss: 1.0448172092437744 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2910 train loss: 0.9272087216377258 train acc: 0.9775000214576721 test loss: 1.0415111780166626 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2911 train loss: 0.9272265434265137 train acc: 0.9775000214576721 test loss: 1.0308754444122314 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2912 train loss: 0.9272189140319824 train acc: 0.9775000214576721 test loss: 1.0339468717575073 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2913 train loss: 0.9273637533187866 train acc: 0.9775000214576721 test loss: 1.027445912361145 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2914 train loss: 0.9273223876953125 train acc: 0.9775000214576721 test loss: 1.0381503105163574 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2915 train loss: 0.9273425340652466 train acc: 0.9775000214576721 test loss: 1.038454532623291 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2916 train loss: 0.9273346066474915 train acc: 0.9775000214576721 test loss: 1.0334657430648804 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2917 train loss: 0.9273351430892944 train acc: 0.9775000214576721 test loss: 1.0335426330566406 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2918 train loss: 0.9273440837860107 train acc: 0.9775000214576721 test loss: 1.0245108604431152 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2919 train loss: 0.927198052406311 train acc: 0.9775000214576721 test loss: 1.0346556901931763 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2920 train loss: 0.9271987676620483 train acc: 0.9775000214576721 test loss: 1.0311336517333984 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2921 train loss: 0.927185595035553 train acc: 0.9775000214576721 test loss: 1.036134958267212 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2922 train loss: 0.9271848201751709 train acc: 0.9775000214576721 test loss: 1.0316790342330933 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2923 train loss: 0.9273490309715271 train acc: 0.9775000214576721 test loss: 1.0363672971725464 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2924 train loss: 0.927345335483551 train acc: 0.9775000214576721 test loss: 1.0408663749694824 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2925 train loss: 0.9273112416267395 train acc: 0.9775000214576721 test loss: 1.0249971151351929 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2926 train loss: 0.9273330569267273 train acc: 0.9775000214576721 test loss: 1.030142903327942 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2927 train loss: 0.9273227453231812 train acc: 0.9775000214576721 test loss: 1.037134051322937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2928 train loss: 0.9273325204849243 train acc: 0.9775000214576721 test loss: 1.0390545129776 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2929 train loss: 0.9273440837860107 train acc: 0.9775000214576721 test loss: 1.0330983400344849 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2930 train loss: 0.9273359775543213 train acc: 0.9775000214576721 test loss: 1.0339070558547974 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2931 train loss: 0.9272622466087341 train acc: 0.9775000214576721 test loss: 1.041805386543274 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2932 train loss: 0.9272615313529968 train acc: 0.9775000214576721 test loss: 1.0350821018218994 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2933 train loss: 0.9273101091384888 train acc: 0.9775000214576721 test loss: 1.0311427116394043 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2934 train loss: 0.927264928817749 train acc: 0.9775000214576721 test loss: 1.042635202407837 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2935 train loss: 0.9273175001144409 train acc: 0.9775000214576721 test loss: 1.031426191329956 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2936 train loss: 0.9273411631584167 train acc: 0.9775000214576721 test loss: 1.0284751653671265 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2937 train loss: 0.927327036857605 train acc: 0.9775000214576721 test loss: 1.0335105657577515 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2938 train loss: 0.9271847009658813 train acc: 0.9775000214576721 test loss: 1.0284450054168701 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2939 train loss: 0.927266001701355 train acc: 0.9775000214576721 test loss: 1.0397125482559204 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2940 train loss: 0.9273387789726257 train acc: 0.9775000214576721 test loss: 1.0242717266082764 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2941 train loss: 0.9273136854171753 train acc: 0.9775000214576721 test loss: 1.0438098907470703 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2942 train loss: 0.9273116588592529 train acc: 0.9775000214576721 test loss: 1.0323214530944824 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2943 train loss: 0.9273385405540466 train acc: 0.9775000214576721 test loss: 1.0357524156570435 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2944 train loss: 0.927336573600769 train acc: 0.9775000214576721 test loss: 1.027316927909851 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2945 train loss: 0.9273366332054138 train acc: 0.9775000214576721 test loss: 1.0273535251617432 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2946 train loss: 0.9273288249969482 train acc: 0.9775000214576721 test loss: 1.0388078689575195 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2947 train loss: 0.9271988868713379 train acc: 0.9775000214576721 test loss: 1.0244133472442627 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2948 train loss: 0.9273289442062378 train acc: 0.9775000214576721 test loss: 1.0395076274871826 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2949 train loss: 0.927315890789032 train acc: 0.9775000214576721 test loss: 1.0284972190856934 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2950 train loss: 0.9273000359535217 train acc: 0.9775000214576721 test loss: 1.0485917329788208 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2951 train loss: 0.9272185564041138 train acc: 0.9775000214576721 test loss: 1.0522027015686035 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2952 train loss: 0.9272201657295227 train acc: 0.9775000214576721 test loss: 1.0329898595809937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2953 train loss: 0.9272614121437073 train acc: 0.9775000214576721 test loss: 1.0383458137512207 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2954 train loss: 0.9272242188453674 train acc: 0.9775000214576721 test loss: 1.0277924537658691 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2955 train loss: 0.9272782802581787 train acc: 0.9775000214576721 test loss: 1.0381181240081787 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2956 train loss: 0.9272725582122803 train acc: 0.9775000214576721 test loss: 1.0400511026382446 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2957 train loss: 0.9273194074630737 train acc: 0.9775000214576721 test loss: 1.0339876413345337 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2958 train loss: 0.927458643913269 train acc: 0.9775000214576721 test loss: 1.0246343612670898 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2959 train loss: 0.9273297190666199 train acc: 0.9775000214576721 test loss: 1.0305062532424927 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2960 train loss: 0.9272583723068237 train acc: 0.9775000214576721 test loss: 1.034266710281372 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2961 train loss: 0.9272029399871826 train acc: 0.9775000214576721 test loss: 1.0346500873565674 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2962 train loss: 0.9273407459259033 train acc: 0.9775000214576721 test loss: 1.0451664924621582 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2963 train loss: 0.9273439049720764 train acc: 0.9775000214576721 test loss: 1.0281803607940674 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2964 train loss: 0.9272367358207703 train acc: 0.9775000214576721 test loss: 1.0419219732284546 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2965 train loss: 0.9273236989974976 train acc: 0.9775000214576721 test loss: 1.0392497777938843 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2966 train loss: 0.9273120164871216 train acc: 0.9775000214576721 test loss: 1.026660680770874 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2967 train loss: 0.9273373484611511 train acc: 0.9775000214576721 test loss: 1.0316411256790161 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2968 train loss: 0.9273297190666199 train acc: 0.9775000214576721 test loss: 1.0435675382614136 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2969 train loss: 0.9272449016571045 train acc: 0.9775000214576721 test loss: 1.029468059539795 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2970 train loss: 0.9273229241371155 train acc: 0.9775000214576721 test loss: 1.0255000591278076 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2971 train loss: 0.9273226261138916 train acc: 0.9775000214576721 test loss: 1.0347988605499268 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2972 train loss: 0.9273005127906799 train acc: 0.9775000214576721 test loss: 1.028626799583435 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2973 train loss: 0.9272424578666687 train acc: 0.9775000214576721 test loss: 1.0346786975860596 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2974 train loss: 0.9272932410240173 train acc: 0.9775000214576721 test loss: 1.0372726917266846 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2975 train loss: 0.9273243546485901 train acc: 0.9775000214576721 test loss: 1.030267596244812 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2976 train loss: 0.9271838665008545 train acc: 0.9775000214576721 test loss: 1.0310863256454468 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2977 train loss: 0.9272139072418213 train acc: 0.9775000214576721 test loss: 1.0283520221710205 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2978 train loss: 0.9272707104682922 train acc: 0.9775000214576721 test loss: 1.0349340438842773 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2979 train loss: 0.9271823763847351 train acc: 0.9775000214576721 test loss: 1.0329606533050537 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2980 train loss: 0.9272348880767822 train acc: 0.9775000214576721 test loss: 1.0413674116134644 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2981 train loss: 0.9273383617401123 train acc: 0.9775000214576721 test loss: 1.0399107933044434 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2982 train loss: 0.9272226691246033 train acc: 0.9775000214576721 test loss: 1.0421229600906372 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2983 train loss: 0.9272348880767822 train acc: 0.9775000214576721 test loss: 1.0333778858184814 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2984 train loss: 0.9273103475570679 train acc: 0.9775000214576721 test loss: 1.039193034172058 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2985 train loss: 0.9272550344467163 train acc: 0.9775000214576721 test loss: 1.0316548347473145 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2986 train loss: 0.9273471832275391 train acc: 0.9775000214576721 test loss: 1.0431475639343262 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 2987 train loss: 0.9273461699485779 train acc: 0.9775000214576721 test loss: 1.0317294597625732 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2988 train loss: 0.9271975755691528 train acc: 0.9775000214576721 test loss: 1.0249885320663452 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2989 train loss: 0.9273449778556824 train acc: 0.9775000214576721 test loss: 1.0306354761123657 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2990 train loss: 0.9273236989974976 train acc: 0.9775000214576721 test loss: 1.0264838933944702 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 2991 train loss: 0.9272701144218445 train acc: 0.9775000214576721 test loss: 1.0368925333023071 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2992 train loss: 0.9273134469985962 train acc: 0.9775000214576721 test loss: 1.0316493511199951 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2993 train loss: 0.9273422956466675 train acc: 0.9775000214576721 test loss: 1.030375361442566 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2994 train loss: 0.9271912574768066 train acc: 0.9775000214576721 test loss: 1.0323389768600464 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2995 train loss: 0.9273368120193481 train acc: 0.9775000214576721 test loss: 1.0348166227340698 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2996 train loss: 0.9273362755775452 train acc: 0.9775000214576721 test loss: 1.029040813446045 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2997 train loss: 0.9273210167884827 train acc: 0.9775000214576721 test loss: 1.0437535047531128 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 2998 train loss: 0.9273278117179871 train acc: 0.9775000214576721 test loss: 1.03484308719635 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 2999 train loss: 0.9273106455802917 train acc: 0.9775000214576721 test loss: 1.0285130739212036 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3000 train loss: 0.92731112241745 train acc: 0.9775000214576721 test loss: 1.0271207094192505 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3001 train loss: 0.9273344278335571 train acc: 0.9775000214576721 test loss: 1.0379397869110107 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3002 train loss: 0.9272928833961487 train acc: 0.9775000214576721 test loss: 1.026110053062439 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3003 train loss: 0.9272923469543457 train acc: 0.9775000214576721 test loss: 1.0288975238800049 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3004 train loss: 0.9273049235343933 train acc: 0.9775000214576721 test loss: 1.0310509204864502 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3005 train loss: 0.9273106455802917 train acc: 0.9775000214576721 test loss: 1.0376029014587402 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3006 train loss: 0.9271950721740723 train acc: 0.9775000214576721 test loss: 1.0368320941925049 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3007 train loss: 0.9273391962051392 train acc: 0.9775000214576721 test loss: 1.0314306020736694 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3008 train loss: 0.9273384809494019 train acc: 0.9775000214576721 test loss: 1.0332131385803223 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3009 train loss: 0.9271825551986694 train acc: 0.9775000214576721 test loss: 1.032739520072937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3010 train loss: 0.9272861480712891 train acc: 0.9775000214576721 test loss: 1.0348048210144043 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3011 train loss: 0.9272847175598145 train acc: 0.9775000214576721 test loss: 1.0375361442565918 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3012 train loss: 0.9273373484611511 train acc: 0.9775000214576721 test loss: 1.038822889328003 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3013 train loss: 0.9272251725196838 train acc: 0.9775000214576721 test loss: 1.041317343711853 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3014 train loss: 0.9273071885108948 train acc: 0.9775000214576721 test loss: 1.0311310291290283 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3015 train loss: 0.9273192882537842 train acc: 0.9775000214576721 test loss: 1.0361857414245605 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3016 train loss: 0.9273220896720886 train acc: 0.9775000214576721 test loss: 1.0331367254257202 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3017 train loss: 0.9273278713226318 train acc: 0.9775000214576721 test loss: 1.0343430042266846 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3018 train loss: 0.9272128939628601 train acc: 0.9775000214576721 test loss: 1.028466820716858 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3019 train loss: 0.9272747039794922 train acc: 0.9775000214576721 test loss: 1.0271531343460083 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3020 train loss: 0.9272044897079468 train acc: 0.9775000214576721 test loss: 1.0382999181747437 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3021 train loss: 0.9273383617401123 train acc: 0.9775000214576721 test loss: 1.0407307147979736 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3022 train loss: 0.9273028373718262 train acc: 0.9775000214576721 test loss: 1.0388871431350708 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3023 train loss: 0.9272019863128662 train acc: 0.9775000214576721 test loss: 1.0302947759628296 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3024 train loss: 0.9272599220275879 train acc: 0.9775000214576721 test loss: 1.0269907712936401 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3025 train loss: 0.9271808862686157 train acc: 0.9775000214576721 test loss: 1.0289841890335083 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3026 train loss: 0.9273121356964111 train acc: 0.9775000214576721 test loss: 1.0381031036376953 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3027 train loss: 0.9272090196609497 train acc: 0.9775000214576721 test loss: 1.0290608406066895 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3028 train loss: 0.9272160530090332 train acc: 0.9775000214576721 test loss: 1.034611463546753 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3029 train loss: 0.9273136854171753 train acc: 0.9775000214576721 test loss: 1.0275521278381348 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3030 train loss: 0.9272605180740356 train acc: 0.9775000214576721 test loss: 1.0312988758087158 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3031 train loss: 0.9271798729896545 train acc: 0.9775000214576721 test loss: 1.0314903259277344 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3032 train loss: 0.9272618293762207 train acc: 0.9775000214576721 test loss: 1.0484809875488281 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3033 train loss: 0.9271914958953857 train acc: 0.9775000214576721 test loss: 1.0309313535690308 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3034 train loss: 0.9273754358291626 train acc: 0.9775000214576721 test loss: 1.0402532815933228 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3035 train loss: 0.9273316860198975 train acc: 0.9775000214576721 test loss: 1.0391117334365845 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3036 train loss: 0.9271858930587769 train acc: 0.9775000214576721 test loss: 1.0445114374160767 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3037 train loss: 0.92720627784729 train acc: 0.9775000214576721 test loss: 1.0256378650665283 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3038 train loss: 0.9273052215576172 train acc: 0.9775000214576721 test loss: 1.0264525413513184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3039 train loss: 0.9272428154945374 train acc: 0.9775000214576721 test loss: 1.035982608795166 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3040 train loss: 0.9271790981292725 train acc: 0.9775000214576721 test loss: 1.0449541807174683 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3041 train loss: 0.9271834492683411 train acc: 0.9775000214576721 test loss: 1.034255862236023 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3042 train loss: 0.9273251295089722 train acc: 0.9775000214576721 test loss: 1.0347882509231567 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3043 train loss: 0.9273384213447571 train acc: 0.9775000214576721 test loss: 1.0265865325927734 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3044 train loss: 0.9272616505622864 train acc: 0.9775000214576721 test loss: 1.0308657884597778 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3045 train loss: 0.9272583723068237 train acc: 0.9775000214576721 test loss: 1.0285369157791138 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3046 train loss: 0.927219033241272 train acc: 0.9775000214576721 test loss: 1.0449720621109009 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3047 train loss: 0.9273267984390259 train acc: 0.9775000214576721 test loss: 1.032301664352417 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3048 train loss: 0.9273412823677063 train acc: 0.9775000214576721 test loss: 1.0441420078277588 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3049 train loss: 0.9272657036781311 train acc: 0.9775000214576721 test loss: 1.0291404724121094 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3050 train loss: 0.9272879958152771 train acc: 0.9775000214576721 test loss: 1.02839195728302 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3051 train loss: 0.9271776080131531 train acc: 0.9775000214576721 test loss: 1.0277283191680908 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3052 train loss: 0.927192211151123 train acc: 0.9775000214576721 test loss: 1.0397926568984985 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3053 train loss: 0.9273424744606018 train acc: 0.9775000214576721 test loss: 1.0354138612747192 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3054 train loss: 0.9271842241287231 train acc: 0.9775000214576721 test loss: 1.0402555465698242 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3055 train loss: 0.9272317290306091 train acc: 0.9775000214576721 test loss: 1.0294289588928223 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3056 train loss: 0.927327036857605 train acc: 0.9775000214576721 test loss: 1.0292099714279175 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3057 train loss: 0.9271929264068604 train acc: 0.9775000214576721 test loss: 1.0302928686141968 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3058 train loss: 0.9271930456161499 train acc: 0.9775000214576721 test loss: 1.0264610052108765 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3059 train loss: 0.9272821545600891 train acc: 0.9775000214576721 test loss: 1.0360466241836548 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3060 train loss: 0.9271921515464783 train acc: 0.9775000214576721 test loss: 1.034427523612976 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3061 train loss: 0.9272753000259399 train acc: 0.9775000214576721 test loss: 1.02593195438385 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3062 train loss: 0.9272477626800537 train acc: 0.9775000214576721 test loss: 1.0329879522323608 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3063 train loss: 0.9273391962051392 train acc: 0.9775000214576721 test loss: 1.0322974920272827 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3064 train loss: 0.9273420572280884 train acc: 0.9775000214576721 test loss: 1.0341459512710571 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3065 train loss: 0.9272987246513367 train acc: 0.9775000214576721 test loss: 1.0290253162384033 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3066 train loss: 0.9272688031196594 train acc: 0.9775000214576721 test loss: 1.035993218421936 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3067 train loss: 0.9271831512451172 train acc: 0.9775000214576721 test loss: 1.0350866317749023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3068 train loss: 0.9273223280906677 train acc: 0.9775000214576721 test loss: 1.0328805446624756 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3069 train loss: 0.9271847605705261 train acc: 0.9775000214576721 test loss: 1.025973916053772 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3070 train loss: 0.9272652268409729 train acc: 0.9775000214576721 test loss: 1.033686876296997 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3071 train loss: 0.9273112416267395 train acc: 0.9775000214576721 test loss: 1.030243158340454 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3072 train loss: 0.9272263050079346 train acc: 0.9775000214576721 test loss: 1.038907527923584 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3073 train loss: 0.9271897673606873 train acc: 0.9775000214576721 test loss: 1.032328724861145 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3074 train loss: 0.9273231029510498 train acc: 0.9775000214576721 test loss: 1.0261160135269165 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3075 train loss: 0.927268385887146 train acc: 0.9775000214576721 test loss: 1.0377787351608276 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3076 train loss: 0.9271862506866455 train acc: 0.9775000214576721 test loss: 1.031396746635437 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3077 train loss: 0.927345335483551 train acc: 0.9775000214576721 test loss: 1.0330199003219604 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3078 train loss: 0.9272739887237549 train acc: 0.9775000214576721 test loss: 1.0341496467590332 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3079 train loss: 0.9273404479026794 train acc: 0.9775000214576721 test loss: 1.0337377786636353 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3080 train loss: 0.9272016286849976 train acc: 0.9775000214576721 test loss: 1.0323349237442017 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3081 train loss: 0.9273178577423096 train acc: 0.9775000214576721 test loss: 1.0284005403518677 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3082 train loss: 0.9273441433906555 train acc: 0.9775000214576721 test loss: 1.0342787504196167 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3083 train loss: 0.9273318648338318 train acc: 0.9775000214576721 test loss: 1.0307782888412476 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3084 train loss: 0.9271960258483887 train acc: 0.9775000214576721 test loss: 1.0285640954971313 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3085 train loss: 0.9272842407226562 train acc: 0.9775000214576721 test loss: 1.0383660793304443 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3086 train loss: 0.9273273944854736 train acc: 0.9775000214576721 test loss: 1.035764455795288 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3087 train loss: 0.9271894693374634 train acc: 0.9775000214576721 test loss: 1.0328397750854492 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3088 train loss: 0.927188515663147 train acc: 0.9775000214576721 test loss: 1.026483416557312 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3089 train loss: 0.927207350730896 train acc: 0.9775000214576721 test loss: 1.0336792469024658 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3090 train loss: 0.9272049069404602 train acc: 0.9775000214576721 test loss: 1.0300263166427612 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3091 train loss: 0.9272648692131042 train acc: 0.9775000214576721 test loss: 1.0253525972366333 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3092 train loss: 0.9272664785385132 train acc: 0.9775000214576721 test loss: 1.0255208015441895 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3093 train loss: 0.9272181987762451 train acc: 0.9775000214576721 test loss: 1.0293251276016235 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3094 train loss: 0.9272937774658203 train acc: 0.9775000214576721 test loss: 1.0270590782165527 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3095 train loss: 0.9273040294647217 train acc: 0.9775000214576721 test loss: 1.0326117277145386 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3096 train loss: 0.9273129105567932 train acc: 0.9775000214576721 test loss: 1.0270389318466187 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3097 train loss: 0.9272117614746094 train acc: 0.9775000214576721 test loss: 1.0259439945220947 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3098 train loss: 0.9271864891052246 train acc: 0.9775000214576721 test loss: 1.027284860610962 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3099 train loss: 0.9271872639656067 train acc: 0.9775000214576721 test loss: 1.0334581136703491 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3100 train loss: 0.9271909594535828 train acc: 0.9775000214576721 test loss: 1.0305839776992798 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3101 train loss: 0.9272583723068237 train acc: 0.9775000214576721 test loss: 1.0424727201461792 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3102 train loss: 0.9272336363792419 train acc: 0.9775000214576721 test loss: 1.0271728038787842 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3103 train loss: 0.9272022843360901 train acc: 0.9775000214576721 test loss: 1.0303140878677368 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3104 train loss: 0.9272591471672058 train acc: 0.9775000214576721 test loss: 1.0256695747375488 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3105 train loss: 0.9272994995117188 train acc: 0.9775000214576721 test loss: 1.0261322259902954 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3106 train loss: 0.927275538444519 train acc: 0.9775000214576721 test loss: 1.0364806652069092 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3107 train loss: 0.9272972345352173 train acc: 0.9775000214576721 test loss: 1.0301545858383179 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3108 train loss: 0.9272412657737732 train acc: 0.9775000214576721 test loss: 1.0259853601455688 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3109 train loss: 0.9272115230560303 train acc: 0.9775000214576721 test loss: 1.0358926057815552 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3110 train loss: 0.9273286461830139 train acc: 0.9775000214576721 test loss: 1.0313042402267456 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3111 train loss: 0.9273368120193481 train acc: 0.9775000214576721 test loss: 1.030279517173767 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3112 train loss: 0.9273220300674438 train acc: 0.9775000214576721 test loss: 1.0335054397583008 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3113 train loss: 0.9271830916404724 train acc: 0.9775000214576721 test loss: 1.03769052028656 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3114 train loss: 0.9272260069847107 train acc: 0.9775000214576721 test loss: 1.0273898839950562 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3115 train loss: 0.9273561835289001 train acc: 0.9775000214576721 test loss: 1.0320838689804077 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3116 train loss: 0.9272976517677307 train acc: 0.9775000214576721 test loss: 1.0260400772094727 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3117 train loss: 0.9271812438964844 train acc: 0.9775000214576721 test loss: 1.035197138786316 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3118 train loss: 0.9271854162216187 train acc: 0.9775000214576721 test loss: 1.0245503187179565 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3119 train loss: 0.9272401332855225 train acc: 0.9775000214576721 test loss: 1.022810935974121 best test loss: 1.0130972862243652 test acc: 0.8899999856948853\n",
      "Epoch 3120 train loss: 0.9272549152374268 train acc: 0.9775000214576721 test loss: 1.0295957326889038 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3121 train loss: 0.9272611141204834 train acc: 0.9775000214576721 test loss: 1.035180926322937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3122 train loss: 0.9272212386131287 train acc: 0.9775000214576721 test loss: 1.036807894706726 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3123 train loss: 0.9273185133934021 train acc: 0.9775000214576721 test loss: 1.0417438745498657 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3124 train loss: 0.9273232221603394 train acc: 0.9775000214576721 test loss: 1.0356955528259277 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3125 train loss: 0.9272949695587158 train acc: 0.9775000214576721 test loss: 1.0261269807815552 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3126 train loss: 0.9273190498352051 train acc: 0.9775000214576721 test loss: 1.0356550216674805 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3127 train loss: 0.9272465705871582 train acc: 0.9775000214576721 test loss: 1.0415629148483276 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3128 train loss: 0.9273016452789307 train acc: 0.9775000214576721 test loss: 1.0361884832382202 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3129 train loss: 0.9272667765617371 train acc: 0.9775000214576721 test loss: 1.0358378887176514 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3130 train loss: 0.9272634387016296 train acc: 0.9775000214576721 test loss: 1.039475440979004 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3131 train loss: 0.9271984696388245 train acc: 0.9775000214576721 test loss: 1.034901738166809 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3132 train loss: 0.9272280335426331 train acc: 0.9775000214576721 test loss: 1.027509331703186 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3133 train loss: 0.9272058606147766 train acc: 0.9775000214576721 test loss: 1.0294232368469238 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3134 train loss: 0.9273000955581665 train acc: 0.9775000214576721 test loss: 1.0327526330947876 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3135 train loss: 0.927278459072113 train acc: 0.9775000214576721 test loss: 1.0270615816116333 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3136 train loss: 0.927197277545929 train acc: 0.9775000214576721 test loss: 1.0354571342468262 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3137 train loss: 0.9272326827049255 train acc: 0.9775000214576721 test loss: 1.0343056917190552 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3138 train loss: 0.9271910786628723 train acc: 0.9775000214576721 test loss: 1.0268381834030151 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3139 train loss: 0.9271938800811768 train acc: 0.9775000214576721 test loss: 1.0307003259658813 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3140 train loss: 0.927175760269165 train acc: 0.9775000214576721 test loss: 1.0432838201522827 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3141 train loss: 0.9271867275238037 train acc: 0.9775000214576721 test loss: 1.0349217653274536 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3142 train loss: 0.9273126721382141 train acc: 0.9775000214576721 test loss: 1.0265446901321411 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3143 train loss: 0.9272273182868958 train acc: 0.9775000214576721 test loss: 1.0336840152740479 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3144 train loss: 0.9273192882537842 train acc: 0.9775000214576721 test loss: 1.0454310178756714 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3145 train loss: 0.9273326396942139 train acc: 0.9775000214576721 test loss: 1.034711480140686 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3146 train loss: 0.9272607564926147 train acc: 0.9775000214576721 test loss: 1.0337302684783936 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3147 train loss: 0.9273284077644348 train acc: 0.9775000214576721 test loss: 1.0346382856369019 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3148 train loss: 0.9271880984306335 train acc: 0.9775000214576721 test loss: 1.0245038270950317 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3149 train loss: 0.9272598028182983 train acc: 0.9775000214576721 test loss: 1.0452923774719238 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3150 train loss: 0.9271891117095947 train acc: 0.9775000214576721 test loss: 1.0290443897247314 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3151 train loss: 0.9272937178611755 train acc: 0.9775000214576721 test loss: 1.0386923551559448 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3152 train loss: 0.9273349642753601 train acc: 0.9775000214576721 test loss: 1.0455098152160645 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3153 train loss: 0.9273334741592407 train acc: 0.9775000214576721 test loss: 1.0348739624023438 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3154 train loss: 0.9272609949111938 train acc: 0.9775000214576721 test loss: 1.0283780097961426 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3155 train loss: 0.9272902607917786 train acc: 0.9775000214576721 test loss: 1.0354526042938232 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3156 train loss: 0.9272063374519348 train acc: 0.9775000214576721 test loss: 1.0266149044036865 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3157 train loss: 0.927294135093689 train acc: 0.9775000214576721 test loss: 1.0453091859817505 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3158 train loss: 0.9272657632827759 train acc: 0.9775000214576721 test loss: 1.0310616493225098 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3159 train loss: 0.927329421043396 train acc: 0.9775000214576721 test loss: 1.034401774406433 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3160 train loss: 0.9271817803382874 train acc: 0.9775000214576721 test loss: 1.0255926847457886 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3161 train loss: 0.927338719367981 train acc: 0.9775000214576721 test loss: 1.0394023656845093 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3162 train loss: 0.9271807670593262 train acc: 0.9775000214576721 test loss: 1.0344934463500977 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3163 train loss: 0.927320659160614 train acc: 0.9775000214576721 test loss: 1.0281486511230469 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3164 train loss: 0.9272510409355164 train acc: 0.9775000214576721 test loss: 1.031624436378479 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3165 train loss: 0.9273139238357544 train acc: 0.9775000214576721 test loss: 1.0352009534835815 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3166 train loss: 0.9273439645767212 train acc: 0.9775000214576721 test loss: 1.026454210281372 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3167 train loss: 0.9272825121879578 train acc: 0.9775000214576721 test loss: 1.0278043746948242 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3168 train loss: 0.9271758794784546 train acc: 0.9775000214576721 test loss: 1.0290226936340332 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3169 train loss: 0.9272016286849976 train acc: 0.9775000214576721 test loss: 1.0349540710449219 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3170 train loss: 0.9272027015686035 train acc: 0.9775000214576721 test loss: 1.0316569805145264 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3171 train loss: 0.9272847175598145 train acc: 0.9775000214576721 test loss: 1.0373070240020752 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3172 train loss: 0.9272551536560059 train acc: 0.9775000214576721 test loss: 1.0276384353637695 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3173 train loss: 0.9272265434265137 train acc: 0.9775000214576721 test loss: 1.032743215560913 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3174 train loss: 0.927337646484375 train acc: 0.9775000214576721 test loss: 1.0331395864486694 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3175 train loss: 0.927333652973175 train acc: 0.9775000214576721 test loss: 1.0328788757324219 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3176 train loss: 0.9271982312202454 train acc: 0.9775000214576721 test loss: 1.0403486490249634 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3177 train loss: 0.9271870255470276 train acc: 0.9775000214576721 test loss: 1.0326247215270996 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3178 train loss: 0.9273303151130676 train acc: 0.9775000214576721 test loss: 1.0258374214172363 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3179 train loss: 0.9272004961967468 train acc: 0.9775000214576721 test loss: 1.0354034900665283 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3180 train loss: 0.9271994233131409 train acc: 0.9775000214576721 test loss: 1.030836820602417 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3181 train loss: 0.9272987842559814 train acc: 0.9775000214576721 test loss: 1.0330089330673218 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3182 train loss: 0.9272265434265137 train acc: 0.9775000214576721 test loss: 1.0340571403503418 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3183 train loss: 0.9271954298019409 train acc: 0.9775000214576721 test loss: 1.0344114303588867 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3184 train loss: 0.9273322224617004 train acc: 0.9775000214576721 test loss: 1.0344643592834473 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3185 train loss: 0.9272592663764954 train acc: 0.9775000214576721 test loss: 1.0487903356552124 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3186 train loss: 0.9272575974464417 train acc: 0.9775000214576721 test loss: 1.0404906272888184 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3187 train loss: 0.9273341298103333 train acc: 0.9775000214576721 test loss: 1.0236636400222778 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3188 train loss: 0.9272904396057129 train acc: 0.9775000214576721 test loss: 1.0303568840026855 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3189 train loss: 0.9272814393043518 train acc: 0.9775000214576721 test loss: 1.0337305068969727 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3190 train loss: 0.9273239970207214 train acc: 0.9775000214576721 test loss: 1.0361905097961426 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3191 train loss: 0.927310049533844 train acc: 0.9775000214576721 test loss: 1.028154730796814 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3192 train loss: 0.9273526072502136 train acc: 0.9775000214576721 test loss: 1.0262742042541504 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3193 train loss: 0.9272288680076599 train acc: 0.9775000214576721 test loss: 1.0314792394638062 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3194 train loss: 0.9271899461746216 train acc: 0.9775000214576721 test loss: 1.0462092161178589 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3195 train loss: 0.9272918105125427 train acc: 0.9775000214576721 test loss: 1.0270769596099854 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3196 train loss: 0.9272651672363281 train acc: 0.9775000214576721 test loss: 1.029890537261963 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3197 train loss: 0.9272910952568054 train acc: 0.9775000214576721 test loss: 1.0229358673095703 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3198 train loss: 0.9273107647895813 train acc: 0.9775000214576721 test loss: 1.0246371030807495 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3199 train loss: 0.9272918701171875 train acc: 0.9775000214576721 test loss: 1.0272332429885864 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3200 train loss: 0.9272586703300476 train acc: 0.9775000214576721 test loss: 1.0297529697418213 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3201 train loss: 0.9272304773330688 train acc: 0.9775000214576721 test loss: 1.035345196723938 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3202 train loss: 0.9272695183753967 train acc: 0.9775000214576721 test loss: 1.0313469171524048 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3203 train loss: 0.9273177981376648 train acc: 0.9775000214576721 test loss: 1.0254487991333008 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3204 train loss: 0.9271934628486633 train acc: 0.9775000214576721 test loss: 1.0408002138137817 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3205 train loss: 0.9271897673606873 train acc: 0.9775000214576721 test loss: 1.029160499572754 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3206 train loss: 0.9273369312286377 train acc: 0.9775000214576721 test loss: 1.033139705657959 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3207 train loss: 0.9272412061691284 train acc: 0.9775000214576721 test loss: 1.0292702913284302 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3208 train loss: 0.9272670149803162 train acc: 0.9775000214576721 test loss: 1.028346300125122 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3209 train loss: 0.9272792339324951 train acc: 0.9775000214576721 test loss: 1.0276578664779663 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3210 train loss: 0.9272542595863342 train acc: 0.9775000214576721 test loss: 1.0358562469482422 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3211 train loss: 0.9273231029510498 train acc: 0.9775000214576721 test loss: 1.0351440906524658 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3212 train loss: 0.9272251129150391 train acc: 0.9775000214576721 test loss: 1.0280238389968872 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3213 train loss: 0.9272888898849487 train acc: 0.9775000214576721 test loss: 1.026871681213379 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3214 train loss: 0.9273341298103333 train acc: 0.9775000214576721 test loss: 1.0267695188522339 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3215 train loss: 0.9273366332054138 train acc: 0.9775000214576721 test loss: 1.0340700149536133 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3216 train loss: 0.9271886348724365 train acc: 0.9775000214576721 test loss: 1.0307016372680664 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3217 train loss: 0.9272018671035767 train acc: 0.9775000214576721 test loss: 1.0371127128601074 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3218 train loss: 0.9272958636283875 train acc: 0.9775000214576721 test loss: 1.0274569988250732 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3219 train loss: 0.9273219108581543 train acc: 0.9775000214576721 test loss: 1.033755898475647 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3220 train loss: 0.9272297024726868 train acc: 0.9775000214576721 test loss: 1.0403542518615723 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3221 train loss: 0.927375316619873 train acc: 0.9775000214576721 test loss: 1.0266002416610718 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3222 train loss: 0.9272602796554565 train acc: 0.9775000214576721 test loss: 1.031065821647644 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3223 train loss: 0.9272706508636475 train acc: 0.9775000214576721 test loss: 1.034838080406189 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3224 train loss: 0.9273264408111572 train acc: 0.9775000214576721 test loss: 1.0304385423660278 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3225 train loss: 0.9273416996002197 train acc: 0.9775000214576721 test loss: 1.0403822660446167 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3226 train loss: 0.9273141622543335 train acc: 0.9775000214576721 test loss: 1.0386089086532593 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3227 train loss: 0.9271981716156006 train acc: 0.9775000214576721 test loss: 1.0337319374084473 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3228 train loss: 0.9271814227104187 train acc: 0.9775000214576721 test loss: 1.0276700258255005 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3229 train loss: 0.9273365139961243 train acc: 0.9775000214576721 test loss: 1.0287331342697144 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3230 train loss: 0.9272300004959106 train acc: 0.9775000214576721 test loss: 1.0324639081954956 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3231 train loss: 0.9273303151130676 train acc: 0.9775000214576721 test loss: 1.030992865562439 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3232 train loss: 0.927243173122406 train acc: 0.9775000214576721 test loss: 1.0448050498962402 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3233 train loss: 0.9271863698959351 train acc: 0.9775000214576721 test loss: 1.0326933860778809 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3234 train loss: 0.9273204803466797 train acc: 0.9775000214576721 test loss: 1.0260748863220215 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3235 train loss: 0.927198588848114 train acc: 0.9775000214576721 test loss: 1.0296839475631714 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3236 train loss: 0.9271826148033142 train acc: 0.9775000214576721 test loss: 1.031225323677063 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3237 train loss: 0.9273391962051392 train acc: 0.9775000214576721 test loss: 1.0246182680130005 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3238 train loss: 0.92720627784729 train acc: 0.9775000214576721 test loss: 1.0305424928665161 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3239 train loss: 0.92730313539505 train acc: 0.9775000214576721 test loss: 1.033178448677063 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3240 train loss: 0.9272340536117554 train acc: 0.9775000214576721 test loss: 1.0414470434188843 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3241 train loss: 0.9272364974021912 train acc: 0.9775000214576721 test loss: 1.0358034372329712 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3242 train loss: 0.9272013902664185 train acc: 0.9775000214576721 test loss: 1.0293431282043457 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3243 train loss: 0.9272646903991699 train acc: 0.9775000214576721 test loss: 1.0263532400131226 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3244 train loss: 0.9271935224533081 train acc: 0.9775000214576721 test loss: 1.038754940032959 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3245 train loss: 0.9272882342338562 train acc: 0.9775000214576721 test loss: 1.025179386138916 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3246 train loss: 0.9272100925445557 train acc: 0.9775000214576721 test loss: 1.0321807861328125 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3247 train loss: 0.9273349642753601 train acc: 0.9775000214576721 test loss: 1.0405806303024292 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3248 train loss: 0.9271805286407471 train acc: 0.9775000214576721 test loss: 1.040311574935913 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3249 train loss: 0.9272845387458801 train acc: 0.9775000214576721 test loss: 1.0338850021362305 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3250 train loss: 0.9271842837333679 train acc: 0.9775000214576721 test loss: 1.0280828475952148 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3251 train loss: 0.9272018671035767 train acc: 0.9775000214576721 test loss: 1.0286352634429932 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3252 train loss: 0.9272944927215576 train acc: 0.9775000214576721 test loss: 1.0350697040557861 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3253 train loss: 0.9272374510765076 train acc: 0.9775000214576721 test loss: 1.0289520025253296 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3254 train loss: 0.927319347858429 train acc: 0.9775000214576721 test loss: 1.0355087518692017 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3255 train loss: 0.9272992014884949 train acc: 0.9775000214576721 test loss: 1.0318589210510254 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3256 train loss: 0.9273086786270142 train acc: 0.9775000214576721 test loss: 1.0355697870254517 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3257 train loss: 0.9271919131278992 train acc: 0.9775000214576721 test loss: 1.0274512767791748 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3258 train loss: 0.9272849559783936 train acc: 0.9775000214576721 test loss: 1.0299358367919922 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3259 train loss: 0.9273054003715515 train acc: 0.9775000214576721 test loss: 1.036613941192627 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3260 train loss: 0.9273368716239929 train acc: 0.9775000214576721 test loss: 1.0400097370147705 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3261 train loss: 0.9272855520248413 train acc: 0.9775000214576721 test loss: 1.0316495895385742 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3262 train loss: 0.9273435473442078 train acc: 0.9775000214576721 test loss: 1.026219129562378 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3263 train loss: 0.9272169470787048 train acc: 0.9775000214576721 test loss: 1.0368287563323975 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3264 train loss: 0.9273121356964111 train acc: 0.9775000214576721 test loss: 1.0384876728057861 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3265 train loss: 0.9272429943084717 train acc: 0.9775000214576721 test loss: 1.0273653268814087 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3266 train loss: 0.9273284077644348 train acc: 0.9775000214576721 test loss: 1.0291668176651 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3267 train loss: 0.9273123145103455 train acc: 0.9775000214576721 test loss: 1.0275225639343262 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3268 train loss: 0.9272751808166504 train acc: 0.9775000214576721 test loss: 1.0345003604888916 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3269 train loss: 0.9273177981376648 train acc: 0.9775000214576721 test loss: 1.0359843969345093 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3270 train loss: 0.9271893501281738 train acc: 0.9775000214576721 test loss: 1.032758355140686 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3271 train loss: 0.927329421043396 train acc: 0.9775000214576721 test loss: 1.0264748334884644 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3272 train loss: 0.9272838830947876 train acc: 0.9775000214576721 test loss: 1.0282875299453735 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3273 train loss: 0.9273329377174377 train acc: 0.9775000214576721 test loss: 1.0287232398986816 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3274 train loss: 0.9272567629814148 train acc: 0.9775000214576721 test loss: 1.0269238948822021 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3275 train loss: 0.9273256063461304 train acc: 0.9775000214576721 test loss: 1.0304744243621826 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3276 train loss: 0.9271886944770813 train acc: 0.9775000214576721 test loss: 1.032197117805481 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3277 train loss: 0.9272730350494385 train acc: 0.9775000214576721 test loss: 1.0277917385101318 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3278 train loss: 0.9272254109382629 train acc: 0.9775000214576721 test loss: 1.0464097261428833 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3279 train loss: 0.9273087978363037 train acc: 0.9775000214576721 test loss: 1.0269618034362793 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3280 train loss: 0.9271932244300842 train acc: 0.9775000214576721 test loss: 1.0243048667907715 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3281 train loss: 0.9272379875183105 train acc: 0.9775000214576721 test loss: 1.0336817502975464 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3282 train loss: 0.9271787405014038 train acc: 0.9775000214576721 test loss: 1.0314406156539917 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3283 train loss: 0.9272529482841492 train acc: 0.9775000214576721 test loss: 1.0266917943954468 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3284 train loss: 0.9272310137748718 train acc: 0.9775000214576721 test loss: 1.0285907983779907 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3285 train loss: 0.9271783232688904 train acc: 0.9775000214576721 test loss: 1.0289661884307861 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3286 train loss: 0.9271785616874695 train acc: 0.9775000214576721 test loss: 1.0314655303955078 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3287 train loss: 0.9271855354309082 train acc: 0.9775000214576721 test loss: 1.0322703123092651 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3288 train loss: 0.9273192882537842 train acc: 0.9775000214576721 test loss: 1.0387468338012695 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3289 train loss: 0.927298367023468 train acc: 0.9775000214576721 test loss: 1.0271382331848145 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3290 train loss: 0.9273092746734619 train acc: 0.9775000214576721 test loss: 1.0375977754592896 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3291 train loss: 0.9273406267166138 train acc: 0.9775000214576721 test loss: 1.0321234464645386 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3292 train loss: 0.9273400902748108 train acc: 0.9775000214576721 test loss: 1.0371646881103516 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3293 train loss: 0.9273346662521362 train acc: 0.9775000214576721 test loss: 1.0273081064224243 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3294 train loss: 0.9272010922431946 train acc: 0.9775000214576721 test loss: 1.02482271194458 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3295 train loss: 0.927208423614502 train acc: 0.9775000214576721 test loss: 1.0320056676864624 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3296 train loss: 0.9272926449775696 train acc: 0.9775000214576721 test loss: 1.027100920677185 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3297 train loss: 0.927348256111145 train acc: 0.9775000214576721 test loss: 1.0486445426940918 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3298 train loss: 0.9271843433380127 train acc: 0.9775000214576721 test loss: 1.0340903997421265 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3299 train loss: 0.927178144454956 train acc: 0.9775000214576721 test loss: 1.0490201711654663 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3300 train loss: 0.9271931648254395 train acc: 0.9775000214576721 test loss: 1.030032992362976 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3301 train loss: 0.927278459072113 train acc: 0.9775000214576721 test loss: 1.0311998128890991 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3302 train loss: 0.9271904230117798 train acc: 0.9775000214576721 test loss: 1.0244187116622925 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3303 train loss: 0.9273183941841125 train acc: 0.9775000214576721 test loss: 1.0279006958007812 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3304 train loss: 0.9273209571838379 train acc: 0.9775000214576721 test loss: 1.0310912132263184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3305 train loss: 0.9271898865699768 train acc: 0.9775000214576721 test loss: 1.0485525131225586 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3306 train loss: 0.9272393584251404 train acc: 0.9775000214576721 test loss: 1.0334826707839966 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3307 train loss: 0.9273374676704407 train acc: 0.9775000214576721 test loss: 1.0349469184875488 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3308 train loss: 0.9272861480712891 train acc: 0.9775000214576721 test loss: 1.0248267650604248 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3309 train loss: 0.9272440075874329 train acc: 0.9775000214576721 test loss: 1.0385874509811401 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3310 train loss: 0.9272128343582153 train acc: 0.9775000214576721 test loss: 1.0353240966796875 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3311 train loss: 0.9272125363349915 train acc: 0.9775000214576721 test loss: 1.029038429260254 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3312 train loss: 0.9272951483726501 train acc: 0.9775000214576721 test loss: 1.0249242782592773 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3313 train loss: 0.9273338913917542 train acc: 0.9775000214576721 test loss: 1.0273396968841553 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3314 train loss: 0.927315890789032 train acc: 0.9775000214576721 test loss: 1.0364584922790527 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3315 train loss: 0.9272976517677307 train acc: 0.9775000214576721 test loss: 1.0348776578903198 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3316 train loss: 0.9273374080657959 train acc: 0.9775000214576721 test loss: 1.033839464187622 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3317 train loss: 0.9273333549499512 train acc: 0.9775000214576721 test loss: 1.029807448387146 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3318 train loss: 0.9272944927215576 train acc: 0.9775000214576721 test loss: 1.0264177322387695 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3319 train loss: 0.9273204207420349 train acc: 0.9775000214576721 test loss: 1.0256692171096802 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3320 train loss: 0.927255392074585 train acc: 0.9775000214576721 test loss: 1.0364941358566284 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3321 train loss: 0.9271921515464783 train acc: 0.9775000214576721 test loss: 1.0274604558944702 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3322 train loss: 0.9271998405456543 train acc: 0.9775000214576721 test loss: 1.0302362442016602 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3323 train loss: 0.9273330569267273 train acc: 0.9775000214576721 test loss: 1.0330337285995483 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3324 train loss: 0.9272873401641846 train acc: 0.9775000214576721 test loss: 1.0422041416168213 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3325 train loss: 0.9272577166557312 train acc: 0.9775000214576721 test loss: 1.0421634912490845 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3326 train loss: 0.9273313879966736 train acc: 0.9775000214576721 test loss: 1.0515942573547363 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3327 train loss: 0.9272125363349915 train acc: 0.9775000214576721 test loss: 1.0270507335662842 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3328 train loss: 0.9272539615631104 train acc: 0.9775000214576721 test loss: 1.0342503786087036 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3329 train loss: 0.9273350238800049 train acc: 0.9775000214576721 test loss: 1.031052827835083 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3330 train loss: 0.92732834815979 train acc: 0.9775000214576721 test loss: 1.035900592803955 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3331 train loss: 0.9272041320800781 train acc: 0.9775000214576721 test loss: 1.0294278860092163 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3332 train loss: 0.9272446632385254 train acc: 0.9775000214576721 test loss: 1.0298285484313965 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3333 train loss: 0.927223265171051 train acc: 0.9775000214576721 test loss: 1.033431887626648 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3334 train loss: 0.9272355437278748 train acc: 0.9775000214576721 test loss: 1.024064302444458 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3335 train loss: 0.9271802306175232 train acc: 0.9775000214576721 test loss: 1.0280176401138306 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3336 train loss: 0.927234947681427 train acc: 0.9775000214576721 test loss: 1.0332555770874023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3337 train loss: 0.927193284034729 train acc: 0.9775000214576721 test loss: 1.0250111818313599 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3338 train loss: 0.9271776080131531 train acc: 0.9775000214576721 test loss: 1.0416712760925293 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3339 train loss: 0.9271815419197083 train acc: 0.9775000214576721 test loss: 1.031171441078186 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3340 train loss: 0.9272046089172363 train acc: 0.9775000214576721 test loss: 1.0276278257369995 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3341 train loss: 0.9271937608718872 train acc: 0.9775000214576721 test loss: 1.0306954383850098 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3342 train loss: 0.927339494228363 train acc: 0.9775000214576721 test loss: 1.02506685256958 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3343 train loss: 0.9273357391357422 train acc: 0.9775000214576721 test loss: 1.024303674697876 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3344 train loss: 0.9273344278335571 train acc: 0.9775000214576721 test loss: 1.0280816555023193 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3345 train loss: 0.9273214936256409 train acc: 0.9775000214576721 test loss: 1.030044674873352 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3346 train loss: 0.9272749423980713 train acc: 0.9775000214576721 test loss: 1.0465803146362305 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3347 train loss: 0.9272973537445068 train acc: 0.9775000214576721 test loss: 1.0250146389007568 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3348 train loss: 0.9273239374160767 train acc: 0.9775000214576721 test loss: 1.0258185863494873 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3349 train loss: 0.9271984696388245 train acc: 0.9775000214576721 test loss: 1.0422903299331665 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3350 train loss: 0.9273040890693665 train acc: 0.9775000214576721 test loss: 1.027952790260315 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3351 train loss: 0.9272902011871338 train acc: 0.9775000214576721 test loss: 1.0348724126815796 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3352 train loss: 0.927222490310669 train acc: 0.9775000214576721 test loss: 1.027997612953186 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3353 train loss: 0.9273160696029663 train acc: 0.9775000214576721 test loss: 1.031080961227417 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3354 train loss: 0.9272417426109314 train acc: 0.9775000214576721 test loss: 1.036007046699524 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3355 train loss: 0.9272017478942871 train acc: 0.9775000214576721 test loss: 1.0314579010009766 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3356 train loss: 0.9271973371505737 train acc: 0.9775000214576721 test loss: 1.0257542133331299 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3357 train loss: 0.9271931648254395 train acc: 0.9775000214576721 test loss: 1.0262980461120605 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3358 train loss: 0.9273310899734497 train acc: 0.9775000214576721 test loss: 1.028385043144226 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3359 train loss: 0.9272946119308472 train acc: 0.9775000214576721 test loss: 1.0339945554733276 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3360 train loss: 0.9273231625556946 train acc: 0.9775000214576721 test loss: 1.030410885810852 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3361 train loss: 0.9272382259368896 train acc: 0.9775000214576721 test loss: 1.0315362215042114 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3362 train loss: 0.9273399114608765 train acc: 0.9775000214576721 test loss: 1.0355359315872192 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3363 train loss: 0.92724609375 train acc: 0.9775000214576721 test loss: 1.0391110181808472 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3364 train loss: 0.9271759986877441 train acc: 0.9775000214576721 test loss: 1.0259815454483032 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3365 train loss: 0.9273322820663452 train acc: 0.9775000214576721 test loss: 1.041032314300537 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3366 train loss: 0.927325427532196 train acc: 0.9775000214576721 test loss: 1.030034065246582 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3367 train loss: 0.9273353815078735 train acc: 0.9775000214576721 test loss: 1.029144287109375 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3368 train loss: 0.9273509383201599 train acc: 0.9775000214576721 test loss: 1.0297623872756958 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3369 train loss: 0.9272729754447937 train acc: 0.9775000214576721 test loss: 1.0318763256072998 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3370 train loss: 0.9272407293319702 train acc: 0.9775000214576721 test loss: 1.0322191715240479 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3371 train loss: 0.9272291660308838 train acc: 0.9775000214576721 test loss: 1.0379953384399414 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3372 train loss: 0.9271842241287231 train acc: 0.9775000214576721 test loss: 1.0394421815872192 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3373 train loss: 0.9272885918617249 train acc: 0.9775000214576721 test loss: 1.0247740745544434 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3374 train loss: 0.927177906036377 train acc: 0.9775000214576721 test loss: 1.0354036092758179 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3375 train loss: 0.9271815419197083 train acc: 0.9775000214576721 test loss: 1.0273889303207397 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3376 train loss: 0.9272967576980591 train acc: 0.9775000214576721 test loss: 1.028426170349121 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3377 train loss: 0.9271789789199829 train acc: 0.9775000214576721 test loss: 1.0306830406188965 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3378 train loss: 0.927176833152771 train acc: 0.9775000214576721 test loss: 1.0390005111694336 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3379 train loss: 0.9273107647895813 train acc: 0.9775000214576721 test loss: 1.0274254083633423 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3380 train loss: 0.9272670745849609 train acc: 0.9775000214576721 test loss: 1.028649091720581 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3381 train loss: 0.9272701144218445 train acc: 0.9775000214576721 test loss: 1.0398374795913696 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3382 train loss: 0.9273194074630737 train acc: 0.9775000214576721 test loss: 1.025753140449524 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3383 train loss: 0.9272602796554565 train acc: 0.9775000214576721 test loss: 1.0324307680130005 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3384 train loss: 0.9273260235786438 train acc: 0.9775000214576721 test loss: 1.023406744003296 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3385 train loss: 0.9271954894065857 train acc: 0.9775000214576721 test loss: 1.042017936706543 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3386 train loss: 0.9272840619087219 train acc: 0.9775000214576721 test loss: 1.0403432846069336 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3387 train loss: 0.9273137450218201 train acc: 0.9775000214576721 test loss: 1.0379561185836792 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3388 train loss: 0.9273136258125305 train acc: 0.9775000214576721 test loss: 1.0519492626190186 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3389 train loss: 0.9272646903991699 train acc: 0.9775000214576721 test loss: 1.0321297645568848 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3390 train loss: 0.9272969961166382 train acc: 0.9775000214576721 test loss: 1.0347546339035034 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3391 train loss: 0.9273351430892944 train acc: 0.9775000214576721 test loss: 1.0346770286560059 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3392 train loss: 0.9272592663764954 train acc: 0.9775000214576721 test loss: 1.0270955562591553 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3393 train loss: 0.9273177981376648 train acc: 0.9775000214576721 test loss: 1.0361462831497192 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3394 train loss: 0.9273194670677185 train acc: 0.9775000214576721 test loss: 1.0334042310714722 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3395 train loss: 0.9272301197052002 train acc: 0.9775000214576721 test loss: 1.0269107818603516 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3396 train loss: 0.9272103905677795 train acc: 0.9775000214576721 test loss: 1.032466173171997 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3397 train loss: 0.9271777272224426 train acc: 0.9775000214576721 test loss: 1.0388424396514893 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3398 train loss: 0.9273245334625244 train acc: 0.9775000214576721 test loss: 1.0316929817199707 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3399 train loss: 0.9273399114608765 train acc: 0.9775000214576721 test loss: 1.033968448638916 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3400 train loss: 0.9272022247314453 train acc: 0.9775000214576721 test loss: 1.030867338180542 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3401 train loss: 0.9272080063819885 train acc: 0.9775000214576721 test loss: 1.0259957313537598 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3402 train loss: 0.9273093342781067 train acc: 0.9775000214576721 test loss: 1.0352842807769775 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3403 train loss: 0.927177906036377 train acc: 0.9775000214576721 test loss: 1.0284994840621948 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3404 train loss: 0.9272798299789429 train acc: 0.9775000214576721 test loss: 1.0367335081100464 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3405 train loss: 0.9272541999816895 train acc: 0.9775000214576721 test loss: 1.0298517942428589 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3406 train loss: 0.9272037744522095 train acc: 0.9775000214576721 test loss: 1.0270240306854248 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3407 train loss: 0.9272610545158386 train acc: 0.9775000214576721 test loss: 1.0275447368621826 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3408 train loss: 0.9273194670677185 train acc: 0.9775000214576721 test loss: 1.0338910818099976 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3409 train loss: 0.9272878766059875 train acc: 0.9775000214576721 test loss: 1.0437026023864746 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3410 train loss: 0.9271814227104187 train acc: 0.9775000214576721 test loss: 1.0372425317764282 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3411 train loss: 0.9272655248641968 train acc: 0.9775000214576721 test loss: 1.0410590171813965 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3412 train loss: 0.9272739291191101 train acc: 0.9775000214576721 test loss: 1.0361783504486084 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3413 train loss: 0.9272752404212952 train acc: 0.9775000214576721 test loss: 1.027989149093628 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3414 train loss: 0.9272752404212952 train acc: 0.9775000214576721 test loss: 1.0343139171600342 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3415 train loss: 0.9272664785385132 train acc: 0.9775000214576721 test loss: 1.0331857204437256 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3416 train loss: 0.9272515773773193 train acc: 0.9775000214576721 test loss: 1.0299242734909058 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3417 train loss: 0.9272459149360657 train acc: 0.9775000214576721 test loss: 1.0331486463546753 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3418 train loss: 0.927287757396698 train acc: 0.9775000214576721 test loss: 1.0420774221420288 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3419 train loss: 0.9271812438964844 train acc: 0.9775000214576721 test loss: 1.0273613929748535 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3420 train loss: 0.9273106455802917 train acc: 0.9775000214576721 test loss: 1.038201093673706 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3421 train loss: 0.9272611737251282 train acc: 0.9775000214576721 test loss: 1.0279163122177124 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3422 train loss: 0.927217423915863 train acc: 0.9775000214576721 test loss: 1.0437391996383667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3423 train loss: 0.9272115230560303 train acc: 0.9775000214576721 test loss: 1.0313972234725952 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3424 train loss: 0.9272440075874329 train acc: 0.9775000214576721 test loss: 1.03557288646698 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3425 train loss: 0.9273292422294617 train acc: 0.9775000214576721 test loss: 1.0255942344665527 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3426 train loss: 0.927338719367981 train acc: 0.9775000214576721 test loss: 1.030646800994873 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3427 train loss: 0.9273028373718262 train acc: 0.9775000214576721 test loss: 1.0287829637527466 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3428 train loss: 0.9272555708885193 train acc: 0.9775000214576721 test loss: 1.0278596878051758 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3429 train loss: 0.9273440837860107 train acc: 0.9775000214576721 test loss: 1.03242027759552 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3430 train loss: 0.9273371696472168 train acc: 0.9775000214576721 test loss: 1.0355706214904785 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3431 train loss: 0.9273236989974976 train acc: 0.9775000214576721 test loss: 1.0340428352355957 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3432 train loss: 0.9273253679275513 train acc: 0.9775000214576721 test loss: 1.0289386510849 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3433 train loss: 0.9272811412811279 train acc: 0.9775000214576721 test loss: 1.0287282466888428 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3434 train loss: 0.9273011684417725 train acc: 0.9775000214576721 test loss: 1.025033712387085 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3435 train loss: 0.9272656440734863 train acc: 0.9775000214576721 test loss: 1.0251847505569458 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3436 train loss: 0.9271978735923767 train acc: 0.9775000214576721 test loss: 1.0290991067886353 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3437 train loss: 0.9272834062576294 train acc: 0.9775000214576721 test loss: 1.034926176071167 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3438 train loss: 0.9273130297660828 train acc: 0.9775000214576721 test loss: 1.0471961498260498 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3439 train loss: 0.9273293018341064 train acc: 0.9775000214576721 test loss: 1.0283180475234985 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3440 train loss: 0.9272874593734741 train acc: 0.9775000214576721 test loss: 1.0346035957336426 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3441 train loss: 0.9272459149360657 train acc: 0.9775000214576721 test loss: 1.0411373376846313 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3442 train loss: 0.9271883368492126 train acc: 0.9775000214576721 test loss: 1.0275312662124634 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3443 train loss: 0.9272552728652954 train acc: 0.9775000214576721 test loss: 1.029293179512024 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3444 train loss: 0.9273017644882202 train acc: 0.9775000214576721 test loss: 1.0305852890014648 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3445 train loss: 0.9271825551986694 train acc: 0.9775000214576721 test loss: 1.0294698476791382 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3446 train loss: 0.9271938800811768 train acc: 0.9775000214576721 test loss: 1.0292538404464722 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3447 train loss: 0.9271987080574036 train acc: 0.9775000214576721 test loss: 1.0376665592193604 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3448 train loss: 0.927285373210907 train acc: 0.9775000214576721 test loss: 1.0292720794677734 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3449 train loss: 0.9273163080215454 train acc: 0.9775000214576721 test loss: 1.0483810901641846 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 3450 train loss: 0.9272157549858093 train acc: 0.9775000214576721 test loss: 1.0265637636184692 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3451 train loss: 0.9272841811180115 train acc: 0.9775000214576721 test loss: 1.0301071405410767 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3452 train loss: 0.927193284034729 train acc: 0.9775000214576721 test loss: 1.0330109596252441 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3453 train loss: 0.927314281463623 train acc: 0.9775000214576721 test loss: 1.0308794975280762 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3454 train loss: 0.9272563457489014 train acc: 0.9775000214576721 test loss: 1.0228545665740967 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3455 train loss: 0.9271932244300842 train acc: 0.9775000214576721 test loss: 1.0327447652816772 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3456 train loss: 0.9273363351821899 train acc: 0.9775000214576721 test loss: 1.033321738243103 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3457 train loss: 0.9271953701972961 train acc: 0.9775000214576721 test loss: 1.0347648859024048 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3458 train loss: 0.9272202849388123 train acc: 0.9775000214576721 test loss: 1.025801420211792 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3459 train loss: 0.9271843433380127 train acc: 0.9775000214576721 test loss: 1.0250953435897827 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3460 train loss: 0.92732834815979 train acc: 0.9775000214576721 test loss: 1.0331579446792603 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3461 train loss: 0.9271966814994812 train acc: 0.9775000214576721 test loss: 1.0327403545379639 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3462 train loss: 0.9272043108940125 train acc: 0.9775000214576721 test loss: 1.0326350927352905 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3463 train loss: 0.9273216724395752 train acc: 0.9775000214576721 test loss: 1.0293381214141846 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3464 train loss: 0.927255392074585 train acc: 0.9775000214576721 test loss: 1.042750597000122 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3465 train loss: 0.9272306561470032 train acc: 0.9775000214576721 test loss: 1.0410809516906738 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3466 train loss: 0.9272672533988953 train acc: 0.9775000214576721 test loss: 1.0272910594940186 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3467 train loss: 0.9273113012313843 train acc: 0.9775000214576721 test loss: 1.0278046131134033 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3468 train loss: 0.9273340702056885 train acc: 0.9775000214576721 test loss: 1.02522611618042 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3469 train loss: 0.9272265434265137 train acc: 0.9775000214576721 test loss: 1.0273754596710205 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3470 train loss: 0.9272158741950989 train acc: 0.9775000214576721 test loss: 1.0268152952194214 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3471 train loss: 0.9273359775543213 train acc: 0.9775000214576721 test loss: 1.0340594053268433 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3472 train loss: 0.9272074699401855 train acc: 0.9775000214576721 test loss: 1.0260436534881592 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3473 train loss: 0.9272119402885437 train acc: 0.9775000214576721 test loss: 1.0383318662643433 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3474 train loss: 0.9272929430007935 train acc: 0.9775000214576721 test loss: 1.031229019165039 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3475 train loss: 0.9273466467857361 train acc: 0.9775000214576721 test loss: 1.0329833030700684 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3476 train loss: 0.9273384213447571 train acc: 0.9775000214576721 test loss: 1.0433704853057861 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3477 train loss: 0.9273113012313843 train acc: 0.9775000214576721 test loss: 1.036423921585083 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3478 train loss: 0.9273369312286377 train acc: 0.9775000214576721 test loss: 1.0243148803710938 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3479 train loss: 0.9272977709770203 train acc: 0.9775000214576721 test loss: 1.0251109600067139 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3480 train loss: 0.9272671341896057 train acc: 0.9775000214576721 test loss: 1.0329898595809937 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3481 train loss: 0.9272258281707764 train acc: 0.9775000214576721 test loss: 1.0319541692733765 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3482 train loss: 0.9272830486297607 train acc: 0.9775000214576721 test loss: 1.0383161306381226 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3483 train loss: 0.9272894859313965 train acc: 0.9775000214576721 test loss: 1.0318683385849 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3484 train loss: 0.9271978735923767 train acc: 0.9775000214576721 test loss: 1.0334985256195068 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3485 train loss: 0.9272198677062988 train acc: 0.9775000214576721 test loss: 1.0260416269302368 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3486 train loss: 0.927317202091217 train acc: 0.9775000214576721 test loss: 1.0349335670471191 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3487 train loss: 0.9273204207420349 train acc: 0.9775000214576721 test loss: 1.0337939262390137 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3488 train loss: 0.9273071885108948 train acc: 0.9775000214576721 test loss: 1.0286847352981567 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3489 train loss: 0.9273055791854858 train acc: 0.9775000214576721 test loss: 1.0323315858840942 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3490 train loss: 0.9272183775901794 train acc: 0.9775000214576721 test loss: 1.0273884534835815 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3491 train loss: 0.9273386597633362 train acc: 0.9775000214576721 test loss: 1.0269899368286133 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3492 train loss: 0.9272838830947876 train acc: 0.9775000214576721 test loss: 1.0269291400909424 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3493 train loss: 0.9273288249969482 train acc: 0.9775000214576721 test loss: 1.0338714122772217 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3494 train loss: 0.9273166656494141 train acc: 0.9775000214576721 test loss: 1.0362274646759033 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3495 train loss: 0.9273270964622498 train acc: 0.9775000214576721 test loss: 1.0332717895507812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3496 train loss: 0.9272201657295227 train acc: 0.9775000214576721 test loss: 1.024687647819519 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3497 train loss: 0.927286684513092 train acc: 0.9775000214576721 test loss: 1.026002049446106 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3498 train loss: 0.9271928668022156 train acc: 0.9775000214576721 test loss: 1.032904028892517 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3499 train loss: 0.9271870255470276 train acc: 0.9775000214576721 test loss: 1.0379904508590698 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3500 train loss: 0.9272611737251282 train acc: 0.9775000214576721 test loss: 1.0397087335586548 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3501 train loss: 0.927186131477356 train acc: 0.9775000214576721 test loss: 1.0295617580413818 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3502 train loss: 0.9271900653839111 train acc: 0.9775000214576721 test loss: 1.039319396018982 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3503 train loss: 0.9272239804267883 train acc: 0.9775000214576721 test loss: 1.0379472970962524 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3504 train loss: 0.9273159503936768 train acc: 0.9775000214576721 test loss: 1.0365010499954224 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3505 train loss: 0.9273351430892944 train acc: 0.9775000214576721 test loss: 1.0342353582382202 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3506 train loss: 0.9272909760475159 train acc: 0.9775000214576721 test loss: 1.0322754383087158 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3507 train loss: 0.9273389577865601 train acc: 0.9775000214576721 test loss: 1.0266833305358887 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3508 train loss: 0.9272258877754211 train acc: 0.9775000214576721 test loss: 1.040977954864502 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3509 train loss: 0.9273175597190857 train acc: 0.9775000214576721 test loss: 1.0281174182891846 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3510 train loss: 0.9271908402442932 train acc: 0.9775000214576721 test loss: 1.0273131132125854 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3511 train loss: 0.9272310733795166 train acc: 0.9775000214576721 test loss: 1.0338473320007324 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3512 train loss: 0.9272580742835999 train acc: 0.9775000214576721 test loss: 1.0262446403503418 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3513 train loss: 0.9273374080657959 train acc: 0.9775000214576721 test loss: 1.0278339385986328 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3514 train loss: 0.9271981120109558 train acc: 0.9775000214576721 test loss: 1.0343313217163086 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3515 train loss: 0.9271820783615112 train acc: 0.9775000214576721 test loss: 1.0346516370773315 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3516 train loss: 0.9273043870925903 train acc: 0.9775000214576721 test loss: 1.0299632549285889 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3517 train loss: 0.9272924065589905 train acc: 0.9775000214576721 test loss: 1.027303695678711 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3518 train loss: 0.9272585511207581 train acc: 0.9775000214576721 test loss: 1.0333845615386963 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3519 train loss: 0.9271828532218933 train acc: 0.9775000214576721 test loss: 1.0248469114303589 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3520 train loss: 0.9272127747535706 train acc: 0.9775000214576721 test loss: 1.0341905355453491 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3521 train loss: 0.9273221492767334 train acc: 0.9775000214576721 test loss: 1.0291024446487427 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3522 train loss: 0.9271763563156128 train acc: 0.9775000214576721 test loss: 1.0289714336395264 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3523 train loss: 0.9272106289863586 train acc: 0.9775000214576721 test loss: 1.025212049484253 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3524 train loss: 0.9272968769073486 train acc: 0.9775000214576721 test loss: 1.027217149734497 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3525 train loss: 0.9273485541343689 train acc: 0.9775000214576721 test loss: 1.0352023839950562 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3526 train loss: 0.927324652671814 train acc: 0.9775000214576721 test loss: 1.0310752391815186 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3527 train loss: 0.9271896481513977 train acc: 0.9775000214576721 test loss: 1.0257829427719116 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3528 train loss: 0.9272169470787048 train acc: 0.9775000214576721 test loss: 1.0336341857910156 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3529 train loss: 0.9271891117095947 train acc: 0.9775000214576721 test loss: 1.033705472946167 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3530 train loss: 0.9272097945213318 train acc: 0.9775000214576721 test loss: 1.0302860736846924 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3531 train loss: 0.9271894693374634 train acc: 0.9775000214576721 test loss: 1.0369811058044434 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3532 train loss: 0.9273359775543213 train acc: 0.9775000214576721 test loss: 1.031267523765564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3533 train loss: 0.9273046851158142 train acc: 0.9775000214576721 test loss: 1.0248850584030151 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3534 train loss: 0.9273319840431213 train acc: 0.9775000214576721 test loss: 1.0298393964767456 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3535 train loss: 0.9272856116294861 train acc: 0.9775000214576721 test loss: 1.0292357206344604 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3536 train loss: 0.9272084832191467 train acc: 0.9775000214576721 test loss: 1.0367827415466309 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3537 train loss: 0.9273645877838135 train acc: 0.9775000214576721 test loss: 1.0409855842590332 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3538 train loss: 0.9272889494895935 train acc: 0.9775000214576721 test loss: 1.025699257850647 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3539 train loss: 0.92732834815979 train acc: 0.9775000214576721 test loss: 1.0263807773590088 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3540 train loss: 0.9273391962051392 train acc: 0.9775000214576721 test loss: 1.0273756980895996 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3541 train loss: 0.9273363351821899 train acc: 0.9775000214576721 test loss: 1.03078293800354 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3542 train loss: 0.9273308515548706 train acc: 0.9775000214576721 test loss: 1.0278878211975098 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3543 train loss: 0.9273335337638855 train acc: 0.9775000214576721 test loss: 1.0253313779830933 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3544 train loss: 0.9273057579994202 train acc: 0.9775000214576721 test loss: 1.028733491897583 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3545 train loss: 0.9272847175598145 train acc: 0.9775000214576721 test loss: 1.0248571634292603 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3546 train loss: 0.9273093342781067 train acc: 0.9775000214576721 test loss: 1.031955361366272 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3547 train loss: 0.9271790385246277 train acc: 0.9775000214576721 test loss: 1.031905174255371 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3548 train loss: 0.9272091388702393 train acc: 0.9775000214576721 test loss: 1.0290638208389282 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3549 train loss: 0.9272305965423584 train acc: 0.9775000214576721 test loss: 1.0278100967407227 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3550 train loss: 0.9273263812065125 train acc: 0.9775000214576721 test loss: 1.0254871845245361 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3551 train loss: 0.9271885752677917 train acc: 0.9775000214576721 test loss: 1.0352836847305298 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3552 train loss: 0.9272122979164124 train acc: 0.9775000214576721 test loss: 1.0379070043563843 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3553 train loss: 0.9272547960281372 train acc: 0.9775000214576721 test loss: 1.031903862953186 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3554 train loss: 0.9273177981376648 train acc: 0.9775000214576721 test loss: 1.0254863500595093 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3555 train loss: 0.927327573299408 train acc: 0.9775000214576721 test loss: 1.0286641120910645 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3556 train loss: 0.9272957444190979 train acc: 0.9775000214576721 test loss: 1.038926124572754 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3557 train loss: 0.9273370504379272 train acc: 0.9775000214576721 test loss: 1.0381276607513428 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3558 train loss: 0.9273192882537842 train acc: 0.9775000214576721 test loss: 1.0380544662475586 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3559 train loss: 0.9272633194923401 train acc: 0.9775000214576721 test loss: 1.028064250946045 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3560 train loss: 0.9273332357406616 train acc: 0.9775000214576721 test loss: 1.0420024394989014 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3561 train loss: 0.9271972179412842 train acc: 0.9775000214576721 test loss: 1.0239671468734741 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3562 train loss: 0.9272239804267883 train acc: 0.9775000214576721 test loss: 1.0249214172363281 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3563 train loss: 0.9271810054779053 train acc: 0.9775000214576721 test loss: 1.034435749053955 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3564 train loss: 0.9273495674133301 train acc: 0.9775000214576721 test loss: 1.0343067646026611 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3565 train loss: 0.9272574782371521 train acc: 0.9775000214576721 test loss: 1.0377601385116577 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3566 train loss: 0.927175760269165 train acc: 0.9775000214576721 test loss: 1.0307865142822266 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3567 train loss: 0.9272990226745605 train acc: 0.9775000214576721 test loss: 1.0270984172821045 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3568 train loss: 0.9273303151130676 train acc: 0.9775000214576721 test loss: 1.0294508934020996 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3569 train loss: 0.927284836769104 train acc: 0.9775000214576721 test loss: 1.0329235792160034 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3570 train loss: 0.9272305965423584 train acc: 0.9775000214576721 test loss: 1.0364662408828735 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3571 train loss: 0.9272773265838623 train acc: 0.9775000214576721 test loss: 1.0308270454406738 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3572 train loss: 0.9271780252456665 train acc: 0.9775000214576721 test loss: 1.037756323814392 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3573 train loss: 0.9272502064704895 train acc: 0.9775000214576721 test loss: 1.0274295806884766 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3574 train loss: 0.92726731300354 train acc: 0.9775000214576721 test loss: 1.0335934162139893 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3575 train loss: 0.927318274974823 train acc: 0.9775000214576721 test loss: 1.0275994539260864 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3576 train loss: 0.9272507429122925 train acc: 0.9775000214576721 test loss: 1.0263679027557373 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3577 train loss: 0.9272899031639099 train acc: 0.9775000214576721 test loss: 1.0295953750610352 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3578 train loss: 0.9273349642753601 train acc: 0.9775000214576721 test loss: 1.0237185955047607 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3579 train loss: 0.9272379875183105 train acc: 0.9775000214576721 test loss: 1.0273557901382446 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3580 train loss: 0.9273245334625244 train acc: 0.9775000214576721 test loss: 1.0292329788208008 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3581 train loss: 0.9271786212921143 train acc: 0.9775000214576721 test loss: 1.0330365896224976 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3582 train loss: 0.9271957874298096 train acc: 0.9775000214576721 test loss: 1.0308793783187866 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3583 train loss: 0.927337646484375 train acc: 0.9775000214576721 test loss: 1.0307469367980957 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3584 train loss: 0.9272155165672302 train acc: 0.9775000214576721 test loss: 1.026639461517334 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3585 train loss: 0.9273017644882202 train acc: 0.9775000214576721 test loss: 1.0272996425628662 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3586 train loss: 0.9272133111953735 train acc: 0.9775000214576721 test loss: 1.0374797582626343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3587 train loss: 0.9273115396499634 train acc: 0.9775000214576721 test loss: 1.0339497327804565 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3588 train loss: 0.927324116230011 train acc: 0.9775000214576721 test loss: 1.0311075448989868 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3589 train loss: 0.9272691607475281 train acc: 0.9775000214576721 test loss: 1.0238349437713623 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3590 train loss: 0.9271976947784424 train acc: 0.9775000214576721 test loss: 1.041256070137024 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3591 train loss: 0.9271766543388367 train acc: 0.9775000214576721 test loss: 1.0282343626022339 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3592 train loss: 0.9272992014884949 train acc: 0.9775000214576721 test loss: 1.0313011407852173 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3593 train loss: 0.9272469878196716 train acc: 0.9775000214576721 test loss: 1.0263296365737915 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3594 train loss: 0.9272085428237915 train acc: 0.9775000214576721 test loss: 1.0499870777130127 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3595 train loss: 0.9271948933601379 train acc: 0.9775000214576721 test loss: 1.038749098777771 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3596 train loss: 0.9272682070732117 train acc: 0.9775000214576721 test loss: 1.0306198596954346 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3597 train loss: 0.9271994233131409 train acc: 0.9775000214576721 test loss: 1.0301035642623901 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3598 train loss: 0.9273087978363037 train acc: 0.9775000214576721 test loss: 1.0362602472305298 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3599 train loss: 0.9273227453231812 train acc: 0.9775000214576721 test loss: 1.0258808135986328 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3600 train loss: 0.9271973371505737 train acc: 0.9775000214576721 test loss: 1.031928539276123 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3601 train loss: 0.9272951483726501 train acc: 0.9775000214576721 test loss: 1.051884412765503 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 3602 train loss: 0.9271843433380127 train acc: 0.9775000214576721 test loss: 1.026698350906372 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3603 train loss: 0.9273016452789307 train acc: 0.9775000214576721 test loss: 1.0332385301589966 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3604 train loss: 0.9272540211677551 train acc: 0.9775000214576721 test loss: 1.0324252843856812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3605 train loss: 0.927186906337738 train acc: 0.9775000214576721 test loss: 1.0314842462539673 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3606 train loss: 0.9272344708442688 train acc: 0.9775000214576721 test loss: 1.0369350910186768 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3607 train loss: 0.9272027015686035 train acc: 0.9775000214576721 test loss: 1.0286757946014404 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3608 train loss: 0.9272493720054626 train acc: 0.9775000214576721 test loss: 1.031077265739441 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3609 train loss: 0.9272680878639221 train acc: 0.9775000214576721 test loss: 1.0269759893417358 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3610 train loss: 0.927327036857605 train acc: 0.9775000214576721 test loss: 1.0343780517578125 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3611 train loss: 0.9272155165672302 train acc: 0.9775000214576721 test loss: 1.029050588607788 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3612 train loss: 0.9273286461830139 train acc: 0.9775000214576721 test loss: 1.0332610607147217 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3613 train loss: 0.9272797107696533 train acc: 0.9775000214576721 test loss: 1.0251891613006592 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3614 train loss: 0.9271763563156128 train acc: 0.9775000214576721 test loss: 1.0244985818862915 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3615 train loss: 0.927321195602417 train acc: 0.9775000214576721 test loss: 1.030375599861145 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3616 train loss: 0.9273257255554199 train acc: 0.9775000214576721 test loss: 1.030015468597412 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3617 train loss: 0.9272013902664185 train acc: 0.9775000214576721 test loss: 1.0248175859451294 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3618 train loss: 0.9272249341011047 train acc: 0.9775000214576721 test loss: 1.0307207107543945 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3619 train loss: 0.9272161722183228 train acc: 0.9775000214576721 test loss: 1.0276544094085693 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3620 train loss: 0.927339494228363 train acc: 0.9775000214576721 test loss: 1.0338099002838135 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3621 train loss: 0.9273307919502258 train acc: 0.9775000214576721 test loss: 1.0341826677322388 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3622 train loss: 0.927308201789856 train acc: 0.9775000214576721 test loss: 1.049471378326416 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3623 train loss: 0.9272148013114929 train acc: 0.9775000214576721 test loss: 1.033582329750061 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3624 train loss: 0.9272422194480896 train acc: 0.9775000214576721 test loss: 1.032347321510315 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3625 train loss: 0.927228569984436 train acc: 0.9775000214576721 test loss: 1.028810977935791 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3626 train loss: 0.9273317456245422 train acc: 0.9775000214576721 test loss: 1.029788613319397 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3627 train loss: 0.9273297786712646 train acc: 0.9775000214576721 test loss: 1.0363211631774902 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3628 train loss: 0.9272436499595642 train acc: 0.9775000214576721 test loss: 1.0322785377502441 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3629 train loss: 0.927316427230835 train acc: 0.9775000214576721 test loss: 1.0407413244247437 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3630 train loss: 0.9271864295005798 train acc: 0.9775000214576721 test loss: 1.0343033075332642 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3631 train loss: 0.9272050261497498 train acc: 0.9775000214576721 test loss: 1.0328459739685059 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3632 train loss: 0.9273318648338318 train acc: 0.9775000214576721 test loss: 1.031821608543396 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3633 train loss: 0.9273085594177246 train acc: 0.9775000214576721 test loss: 1.0314695835113525 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3634 train loss: 0.9273379445075989 train acc: 0.9775000214576721 test loss: 1.0393010377883911 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3635 train loss: 0.9271774888038635 train acc: 0.9775000214576721 test loss: 1.0292739868164062 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3636 train loss: 0.9273028373718262 train acc: 0.9775000214576721 test loss: 1.0332589149475098 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3637 train loss: 0.9273043274879456 train acc: 0.9775000214576721 test loss: 1.037064552307129 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3638 train loss: 0.9272065758705139 train acc: 0.9775000214576721 test loss: 1.0268590450286865 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3639 train loss: 0.927308201789856 train acc: 0.9775000214576721 test loss: 1.0269511938095093 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3640 train loss: 0.9272592067718506 train acc: 0.9775000214576721 test loss: 1.026870608329773 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3641 train loss: 0.9271900653839111 train acc: 0.9775000214576721 test loss: 1.034462332725525 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3642 train loss: 0.9272616505622864 train acc: 0.9775000214576721 test loss: 1.0310362577438354 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3643 train loss: 0.9272341132164001 train acc: 0.9775000214576721 test loss: 1.0263776779174805 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3644 train loss: 0.9272177815437317 train acc: 0.9775000214576721 test loss: 1.0289965867996216 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3645 train loss: 0.927276611328125 train acc: 0.9775000214576721 test loss: 1.0303592681884766 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3646 train loss: 0.9271815419197083 train acc: 0.9775000214576721 test loss: 1.029353141784668 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3647 train loss: 0.9271889328956604 train acc: 0.9775000214576721 test loss: 1.037320613861084 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3648 train loss: 0.9272997975349426 train acc: 0.9775000214576721 test loss: 1.0323508977890015 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3649 train loss: 0.9273123741149902 train acc: 0.9775000214576721 test loss: 1.0264302492141724 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3650 train loss: 0.9272695779800415 train acc: 0.9775000214576721 test loss: 1.0346039533615112 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3651 train loss: 0.9271886944770813 train acc: 0.9775000214576721 test loss: 1.0452799797058105 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3652 train loss: 0.9272953867912292 train acc: 0.9775000214576721 test loss: 1.0312703847885132 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3653 train loss: 0.9272634983062744 train acc: 0.9775000214576721 test loss: 1.0265580415725708 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3654 train loss: 0.9271917939186096 train acc: 0.9775000214576721 test loss: 1.0331497192382812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3655 train loss: 0.9273355007171631 train acc: 0.9775000214576721 test loss: 1.0319488048553467 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3656 train loss: 0.9273109436035156 train acc: 0.9775000214576721 test loss: 1.0364530086517334 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3657 train loss: 0.927334725856781 train acc: 0.9775000214576721 test loss: 1.0349854230880737 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3658 train loss: 0.9273296594619751 train acc: 0.9775000214576721 test loss: 1.025815725326538 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3659 train loss: 0.9272722601890564 train acc: 0.9775000214576721 test loss: 1.0405192375183105 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3660 train loss: 0.9272902011871338 train acc: 0.9775000214576721 test loss: 1.0324946641921997 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3661 train loss: 0.9272469878196716 train acc: 0.9775000214576721 test loss: 1.0281270742416382 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3662 train loss: 0.9273191094398499 train acc: 0.9775000214576721 test loss: 1.0418601036071777 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3663 train loss: 0.9271852970123291 train acc: 0.9775000214576721 test loss: 1.027686357498169 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3664 train loss: 0.9271771311759949 train acc: 0.9775000214576721 test loss: 1.0308696031570435 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3665 train loss: 0.9273172616958618 train acc: 0.9775000214576721 test loss: 1.0397591590881348 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3666 train loss: 0.9271786212921143 train acc: 0.9775000214576721 test loss: 1.0364201068878174 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3667 train loss: 0.9273057579994202 train acc: 0.9775000214576721 test loss: 1.026167631149292 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3668 train loss: 0.9272185564041138 train acc: 0.9775000214576721 test loss: 1.035426139831543 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3669 train loss: 0.9271824359893799 train acc: 0.9775000214576721 test loss: 1.029997706413269 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3670 train loss: 0.9272834658622742 train acc: 0.9775000214576721 test loss: 1.0397119522094727 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3671 train loss: 0.9273133873939514 train acc: 0.9775000214576721 test loss: 1.0273942947387695 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3672 train loss: 0.9273231625556946 train acc: 0.9775000214576721 test loss: 1.035499095916748 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3673 train loss: 0.9272197484970093 train acc: 0.9775000214576721 test loss: 1.027838945388794 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3674 train loss: 0.9273194074630737 train acc: 0.9775000214576721 test loss: 1.0271902084350586 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3675 train loss: 0.9273056983947754 train acc: 0.9775000214576721 test loss: 1.028538703918457 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3676 train loss: 0.9272828102111816 train acc: 0.9775000214576721 test loss: 1.0393610000610352 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3677 train loss: 0.9271807670593262 train acc: 0.9775000214576721 test loss: 1.0290671586990356 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3678 train loss: 0.9273208379745483 train acc: 0.9775000214576721 test loss: 1.0287741422653198 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3679 train loss: 0.9272856712341309 train acc: 0.9775000214576721 test loss: 1.0310298204421997 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3680 train loss: 0.9272034168243408 train acc: 0.9775000214576721 test loss: 1.037986159324646 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3681 train loss: 0.927233874797821 train acc: 0.9775000214576721 test loss: 1.0366452932357788 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3682 train loss: 0.9272388219833374 train acc: 0.9775000214576721 test loss: 1.0258128643035889 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3683 train loss: 0.9271780252456665 train acc: 0.9775000214576721 test loss: 1.0360788106918335 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3684 train loss: 0.927253007888794 train acc: 0.9775000214576721 test loss: 1.0426983833312988 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3685 train loss: 0.9272640347480774 train acc: 0.9775000214576721 test loss: 1.0383377075195312 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3686 train loss: 0.927308976650238 train acc: 0.9775000214576721 test loss: 1.029282808303833 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3687 train loss: 0.9272744655609131 train acc: 0.9775000214576721 test loss: 1.0258129835128784 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3688 train loss: 0.9272960424423218 train acc: 0.9775000214576721 test loss: 1.0371816158294678 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3689 train loss: 0.9271867275238037 train acc: 0.9775000214576721 test loss: 1.030981183052063 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3690 train loss: 0.9272867441177368 train acc: 0.9775000214576721 test loss: 1.038788080215454 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3691 train loss: 0.9272348284721375 train acc: 0.9775000214576721 test loss: 1.0345896482467651 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3692 train loss: 0.9272842407226562 train acc: 0.9775000214576721 test loss: 1.0342662334442139 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3693 train loss: 0.9273327589035034 train acc: 0.9775000214576721 test loss: 1.0402125120162964 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3694 train loss: 0.9273210167884827 train acc: 0.9775000214576721 test loss: 1.0264708995819092 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3695 train loss: 0.927177369594574 train acc: 0.9775000214576721 test loss: 1.0359597206115723 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3696 train loss: 0.92728590965271 train acc: 0.9775000214576721 test loss: 1.0349739789962769 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3697 train loss: 0.9271763563156128 train acc: 0.9775000214576721 test loss: 1.031957983970642 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3698 train loss: 0.9272641539573669 train acc: 0.9775000214576721 test loss: 1.0252931118011475 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3699 train loss: 0.927310585975647 train acc: 0.9775000214576721 test loss: 1.0305038690567017 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3700 train loss: 0.9272502064704895 train acc: 0.9775000214576721 test loss: 1.0381762981414795 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3701 train loss: 0.9271826148033142 train acc: 0.9775000214576721 test loss: 1.0293456315994263 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3702 train loss: 0.9272215962409973 train acc: 0.9775000214576721 test loss: 1.051580548286438 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 3703 train loss: 0.9273379445075989 train acc: 0.9775000214576721 test loss: 1.0329859256744385 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3704 train loss: 0.9273101091384888 train acc: 0.9775000214576721 test loss: 1.0408843755722046 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3705 train loss: 0.927194356918335 train acc: 0.9775000214576721 test loss: 1.0240896940231323 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3706 train loss: 0.9271995425224304 train acc: 0.9775000214576721 test loss: 1.0376170873641968 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3707 train loss: 0.9271804094314575 train acc: 0.9775000214576721 test loss: 1.028157353401184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3708 train loss: 0.9272447824478149 train acc: 0.9775000214576721 test loss: 1.0418190956115723 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3709 train loss: 0.9271788001060486 train acc: 0.9775000214576721 test loss: 1.0308476686477661 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3710 train loss: 0.9272856712341309 train acc: 0.9775000214576721 test loss: 1.0327662229537964 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3711 train loss: 0.9272894859313965 train acc: 0.9775000214576721 test loss: 1.0285313129425049 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3712 train loss: 0.9272005558013916 train acc: 0.9775000214576721 test loss: 1.039576768875122 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3713 train loss: 0.9272668957710266 train acc: 0.9775000214576721 test loss: 1.0303250551223755 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3714 train loss: 0.927340567111969 train acc: 0.9775000214576721 test loss: 1.0286015272140503 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3715 train loss: 0.9272396564483643 train acc: 0.9775000214576721 test loss: 1.028030276298523 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3716 train loss: 0.9273163080215454 train acc: 0.9775000214576721 test loss: 1.0313078165054321 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3717 train loss: 0.9273360967636108 train acc: 0.9775000214576721 test loss: 1.0305509567260742 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3718 train loss: 0.9273183345794678 train acc: 0.9775000214576721 test loss: 1.032422423362732 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3719 train loss: 0.9273505210876465 train acc: 0.9775000214576721 test loss: 1.036073923110962 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3720 train loss: 0.9272565245628357 train acc: 0.9775000214576721 test loss: 1.0452756881713867 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3721 train loss: 0.9272728562355042 train acc: 0.9775000214576721 test loss: 1.035156488418579 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3722 train loss: 0.9273430705070496 train acc: 0.9775000214576721 test loss: 1.0432599782943726 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3723 train loss: 0.9272679686546326 train acc: 0.9775000214576721 test loss: 1.0277334451675415 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3724 train loss: 0.9271863698959351 train acc: 0.9775000214576721 test loss: 1.0335496664047241 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3725 train loss: 0.9272088408470154 train acc: 0.9775000214576721 test loss: 1.0347390174865723 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3726 train loss: 0.9273160696029663 train acc: 0.9775000214576721 test loss: 1.030717134475708 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3727 train loss: 0.9272828102111816 train acc: 0.9775000214576721 test loss: 1.0270458459854126 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3728 train loss: 0.9273173809051514 train acc: 0.9775000214576721 test loss: 1.034104824066162 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3729 train loss: 0.9272971153259277 train acc: 0.9775000214576721 test loss: 1.034555435180664 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3730 train loss: 0.9271831512451172 train acc: 0.9775000214576721 test loss: 1.0268034934997559 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3731 train loss: 0.9273149371147156 train acc: 0.9775000214576721 test loss: 1.031206488609314 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3732 train loss: 0.9272428750991821 train acc: 0.9775000214576721 test loss: 1.0268230438232422 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3733 train loss: 0.9272752404212952 train acc: 0.9775000214576721 test loss: 1.0329692363739014 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3734 train loss: 0.9271823167800903 train acc: 0.9775000214576721 test loss: 1.0377670526504517 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3735 train loss: 0.9272189140319824 train acc: 0.9775000214576721 test loss: 1.0325535535812378 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3736 train loss: 0.9272299408912659 train acc: 0.9775000214576721 test loss: 1.0391407012939453 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3737 train loss: 0.9272867441177368 train acc: 0.9775000214576721 test loss: 1.0334919691085815 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3738 train loss: 0.9273367524147034 train acc: 0.9775000214576721 test loss: 1.0278289318084717 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3739 train loss: 0.9272424578666687 train acc: 0.9775000214576721 test loss: 1.0425838232040405 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3740 train loss: 0.9272497296333313 train acc: 0.9775000214576721 test loss: 1.0355144739151 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3741 train loss: 0.9271867275238037 train acc: 0.9775000214576721 test loss: 1.029962182044983 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3742 train loss: 0.9272003769874573 train acc: 0.9775000214576721 test loss: 1.028725504875183 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3743 train loss: 0.927288830280304 train acc: 0.9775000214576721 test loss: 1.028652310371399 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3744 train loss: 0.9273237586021423 train acc: 0.9775000214576721 test loss: 1.0246975421905518 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3745 train loss: 0.9271833896636963 train acc: 0.9775000214576721 test loss: 1.0381823778152466 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3746 train loss: 0.9273250102996826 train acc: 0.9775000214576721 test loss: 1.0288242101669312 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3747 train loss: 0.9272140264511108 train acc: 0.9775000214576721 test loss: 1.028689980506897 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3748 train loss: 0.9273145198822021 train acc: 0.9775000214576721 test loss: 1.035476565361023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3749 train loss: 0.9272007942199707 train acc: 0.9775000214576721 test loss: 1.024524211883545 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3750 train loss: 0.9271795749664307 train acc: 0.9775000214576721 test loss: 1.0307071208953857 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3751 train loss: 0.9272031188011169 train acc: 0.9775000214576721 test loss: 1.0237971544265747 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3752 train loss: 0.9272804856300354 train acc: 0.9775000214576721 test loss: 1.0389896631240845 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3753 train loss: 0.9272063970565796 train acc: 0.9775000214576721 test loss: 1.0316249132156372 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3754 train loss: 0.9272105693817139 train acc: 0.9775000214576721 test loss: 1.024891972541809 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3755 train loss: 0.9273335337638855 train acc: 0.9775000214576721 test loss: 1.0362638235092163 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3756 train loss: 0.9272314310073853 train acc: 0.9775000214576721 test loss: 1.0264554023742676 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3757 train loss: 0.9272189140319824 train acc: 0.9775000214576721 test loss: 1.0331658124923706 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3758 train loss: 0.9271782636642456 train acc: 0.9775000214576721 test loss: 1.0291231870651245 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3759 train loss: 0.9272993206977844 train acc: 0.9775000214576721 test loss: 1.0387473106384277 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3760 train loss: 0.9272009134292603 train acc: 0.9775000214576721 test loss: 1.0291165113449097 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3761 train loss: 0.9273318648338318 train acc: 0.9775000214576721 test loss: 1.0374360084533691 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3762 train loss: 0.9273163080215454 train acc: 0.9775000214576721 test loss: 1.0330896377563477 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3763 train loss: 0.9271940588951111 train acc: 0.9775000214576721 test loss: 1.0338491201400757 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3764 train loss: 0.9272444844245911 train acc: 0.9775000214576721 test loss: 1.037185549736023 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3765 train loss: 0.9271788001060486 train acc: 0.9775000214576721 test loss: 1.0348507165908813 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3766 train loss: 0.9271806478500366 train acc: 0.9775000214576721 test loss: 1.0259929895401 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3767 train loss: 0.9271812438964844 train acc: 0.9775000214576721 test loss: 1.0301735401153564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3768 train loss: 0.9271751642227173 train acc: 0.9775000214576721 test loss: 1.0420187711715698 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3769 train loss: 0.9272076487541199 train acc: 0.9775000214576721 test loss: 1.024501919746399 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3770 train loss: 0.9272627830505371 train acc: 0.9775000214576721 test loss: 1.0260932445526123 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3771 train loss: 0.9272410869598389 train acc: 0.9775000214576721 test loss: 1.0316203832626343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3772 train loss: 0.9271766543388367 train acc: 0.9775000214576721 test loss: 1.0278847217559814 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3773 train loss: 0.9272104501724243 train acc: 0.9775000214576721 test loss: 1.0304272174835205 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3774 train loss: 0.9273207187652588 train acc: 0.9775000214576721 test loss: 1.0238375663757324 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3775 train loss: 0.9273192882537842 train acc: 0.9775000214576721 test loss: 1.0258435010910034 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3776 train loss: 0.9271834492683411 train acc: 0.9775000214576721 test loss: 1.0258740186691284 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3777 train loss: 0.9273281097412109 train acc: 0.9775000214576721 test loss: 1.0345520973205566 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3778 train loss: 0.9272155165672302 train acc: 0.9775000214576721 test loss: 1.0317373275756836 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3779 train loss: 0.9272093176841736 train acc: 0.9775000214576721 test loss: 1.033777117729187 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3780 train loss: 0.9272134304046631 train acc: 0.9775000214576721 test loss: 1.032041311264038 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3781 train loss: 0.9272592067718506 train acc: 0.9775000214576721 test loss: 1.0369631052017212 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3782 train loss: 0.9272041320800781 train acc: 0.9775000214576721 test loss: 1.0279430150985718 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3783 train loss: 0.9273270964622498 train acc: 0.9775000214576721 test loss: 1.04318368434906 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3784 train loss: 0.9271800518035889 train acc: 0.9775000214576721 test loss: 1.0315840244293213 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3785 train loss: 0.9271740913391113 train acc: 0.9775000214576721 test loss: 1.0389190912246704 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3786 train loss: 0.927182137966156 train acc: 0.9775000214576721 test loss: 1.0346373319625854 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3787 train loss: 0.9271795153617859 train acc: 0.9775000214576721 test loss: 1.042872428894043 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3788 train loss: 0.9272692799568176 train acc: 0.9775000214576721 test loss: 1.0345577001571655 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3789 train loss: 0.9272716641426086 train acc: 0.9775000214576721 test loss: 1.035158634185791 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3790 train loss: 0.9273360967636108 train acc: 0.9775000214576721 test loss: 1.0298666954040527 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3791 train loss: 0.9272291660308838 train acc: 0.9775000214576721 test loss: 1.027530550956726 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3792 train loss: 0.9273272752761841 train acc: 0.9775000214576721 test loss: 1.0424304008483887 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3793 train loss: 0.9271842837333679 train acc: 0.9775000214576721 test loss: 1.027500033378601 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3794 train loss: 0.9273131489753723 train acc: 0.9775000214576721 test loss: 1.0320185422897339 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3795 train loss: 0.9272286295890808 train acc: 0.9775000214576721 test loss: 1.0267585515975952 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3796 train loss: 0.9272913932800293 train acc: 0.9775000214576721 test loss: 1.0330829620361328 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3797 train loss: 0.9271965026855469 train acc: 0.9775000214576721 test loss: 1.035771131515503 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3798 train loss: 0.9271833896636963 train acc: 0.9775000214576721 test loss: 1.023524522781372 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3799 train loss: 0.9272027015686035 train acc: 0.9775000214576721 test loss: 1.0425161123275757 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3800 train loss: 0.9272717833518982 train acc: 0.9775000214576721 test loss: 1.0297751426696777 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3801 train loss: 0.9272552728652954 train acc: 0.9775000214576721 test loss: 1.026241660118103 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3802 train loss: 0.927196204662323 train acc: 0.9775000214576721 test loss: 1.0445199012756348 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3803 train loss: 0.9272955060005188 train acc: 0.9775000214576721 test loss: 1.0273281335830688 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3804 train loss: 0.9272521138191223 train acc: 0.9775000214576721 test loss: 1.0386240482330322 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3805 train loss: 0.9273344278335571 train acc: 0.9775000214576721 test loss: 1.0338791608810425 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3806 train loss: 0.9271878004074097 train acc: 0.9775000214576721 test loss: 1.0260560512542725 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3807 train loss: 0.9271969795227051 train acc: 0.9775000214576721 test loss: 1.0240105390548706 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3808 train loss: 0.9273101091384888 train acc: 0.9775000214576721 test loss: 1.0346776247024536 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3809 train loss: 0.9273335337638855 train acc: 0.9775000214576721 test loss: 1.0247173309326172 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3810 train loss: 0.927304208278656 train acc: 0.9775000214576721 test loss: 1.0408729314804077 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3811 train loss: 0.9272967576980591 train acc: 0.9775000214576721 test loss: 1.0387885570526123 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3812 train loss: 0.9272960424423218 train acc: 0.9775000214576721 test loss: 1.0309637784957886 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3813 train loss: 0.9272852540016174 train acc: 0.9775000214576721 test loss: 1.0259095430374146 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3814 train loss: 0.9272724986076355 train acc: 0.9775000214576721 test loss: 1.0358530282974243 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3815 train loss: 0.9273260235786438 train acc: 0.9775000214576721 test loss: 1.0367692708969116 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3816 train loss: 0.9273273348808289 train acc: 0.9775000214576721 test loss: 1.0307577848434448 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3817 train loss: 0.9273174405097961 train acc: 0.9775000214576721 test loss: 1.025162696838379 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3818 train loss: 0.9271934628486633 train acc: 0.9775000214576721 test loss: 1.0301077365875244 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3819 train loss: 0.9271752834320068 train acc: 0.9775000214576721 test loss: 1.0391676425933838 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3820 train loss: 0.9272008538246155 train acc: 0.9775000214576721 test loss: 1.0330226421356201 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3821 train loss: 0.9272604584693909 train acc: 0.9775000214576721 test loss: 1.0313142538070679 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3822 train loss: 0.9271796345710754 train acc: 0.9775000214576721 test loss: 1.0274642705917358 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3823 train loss: 0.9271981716156006 train acc: 0.9775000214576721 test loss: 1.0292681455612183 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3824 train loss: 0.9271920919418335 train acc: 0.9775000214576721 test loss: 1.031449317932129 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3825 train loss: 0.9273191690444946 train acc: 0.9775000214576721 test loss: 1.0327792167663574 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3826 train loss: 0.9272706508636475 train acc: 0.9775000214576721 test loss: 1.0278550386428833 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3827 train loss: 0.9272578358650208 train acc: 0.9775000214576721 test loss: 1.0323444604873657 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3828 train loss: 0.9271844625473022 train acc: 0.9775000214576721 test loss: 1.0338701009750366 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3829 train loss: 0.9272354245185852 train acc: 0.9775000214576721 test loss: 1.0268498659133911 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3830 train loss: 0.9271950721740723 train acc: 0.9775000214576721 test loss: 1.0282471179962158 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3831 train loss: 0.9273365139961243 train acc: 0.9775000214576721 test loss: 1.0306020975112915 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3832 train loss: 0.927192211151123 train acc: 0.9775000214576721 test loss: 1.0263334512710571 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3833 train loss: 0.9272316098213196 train acc: 0.9775000214576721 test loss: 1.0320535898208618 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3834 train loss: 0.9273253679275513 train acc: 0.9775000214576721 test loss: 1.0413192510604858 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3835 train loss: 0.9272229075431824 train acc: 0.9775000214576721 test loss: 1.0285718441009521 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3836 train loss: 0.927289605140686 train acc: 0.9775000214576721 test loss: 1.0498439073562622 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3837 train loss: 0.927326500415802 train acc: 0.9775000214576721 test loss: 1.0526015758514404 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3838 train loss: 0.927336573600769 train acc: 0.9775000214576721 test loss: 1.0251116752624512 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3839 train loss: 0.9273313283920288 train acc: 0.9775000214576721 test loss: 1.0356460809707642 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3840 train loss: 0.9271826148033142 train acc: 0.9775000214576721 test loss: 1.0330147743225098 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3841 train loss: 0.9272878766059875 train acc: 0.9775000214576721 test loss: 1.0292747020721436 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3842 train loss: 0.9272648692131042 train acc: 0.9775000214576721 test loss: 1.033339500427246 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3843 train loss: 0.9271883964538574 train acc: 0.9775000214576721 test loss: 1.0229709148406982 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3844 train loss: 0.9271945357322693 train acc: 0.9775000214576721 test loss: 1.034759283065796 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3845 train loss: 0.9272102117538452 train acc: 0.9775000214576721 test loss: 1.044228196144104 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3846 train loss: 0.9271833896636963 train acc: 0.9775000214576721 test loss: 1.0296787023544312 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3847 train loss: 0.9272555708885193 train acc: 0.9775000214576721 test loss: 1.0372053384780884 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3848 train loss: 0.9271919131278992 train acc: 0.9775000214576721 test loss: 1.0384000539779663 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3849 train loss: 0.9272245168685913 train acc: 0.9775000214576721 test loss: 1.032307505607605 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3850 train loss: 0.9272782206535339 train acc: 0.9775000214576721 test loss: 1.0386912822723389 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3851 train loss: 0.9273242354393005 train acc: 0.9775000214576721 test loss: 1.035244107246399 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3852 train loss: 0.927182674407959 train acc: 0.9775000214576721 test loss: 1.026401162147522 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3853 train loss: 0.9272536635398865 train acc: 0.9775000214576721 test loss: 1.0246883630752563 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3854 train loss: 0.9271790981292725 train acc: 0.9775000214576721 test loss: 1.0263124704360962 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3855 train loss: 0.9272350072860718 train acc: 0.9775000214576721 test loss: 1.0252888202667236 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3856 train loss: 0.9272748827934265 train acc: 0.9775000214576721 test loss: 1.0273898839950562 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3857 train loss: 0.9273071885108948 train acc: 0.9775000214576721 test loss: 1.0258793830871582 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3858 train loss: 0.9271751642227173 train acc: 0.9775000214576721 test loss: 1.0520367622375488 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3859 train loss: 0.9273273944854736 train acc: 0.9775000214576721 test loss: 1.0431324243545532 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3860 train loss: 0.9272723197937012 train acc: 0.9775000214576721 test loss: 1.0431652069091797 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3861 train loss: 0.9272881150245667 train acc: 0.9775000214576721 test loss: 1.0310685634613037 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3862 train loss: 0.9271993041038513 train acc: 0.9775000214576721 test loss: 1.0349568128585815 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3863 train loss: 0.927249550819397 train acc: 0.9775000214576721 test loss: 1.0370253324508667 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3864 train loss: 0.9272638559341431 train acc: 0.9775000214576721 test loss: 1.0327033996582031 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3865 train loss: 0.9272143840789795 train acc: 0.9775000214576721 test loss: 1.0295950174331665 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3866 train loss: 0.9272087216377258 train acc: 0.9775000214576721 test loss: 1.032161831855774 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3867 train loss: 0.9271842837333679 train acc: 0.9775000214576721 test loss: 1.0335593223571777 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3868 train loss: 0.9272632002830505 train acc: 0.9775000214576721 test loss: 1.0344786643981934 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3869 train loss: 0.927265465259552 train acc: 0.9775000214576721 test loss: 1.0266224145889282 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3870 train loss: 0.927222490310669 train acc: 0.9775000214576721 test loss: 1.032098650932312 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3871 train loss: 0.9272346496582031 train acc: 0.9775000214576721 test loss: 1.0316110849380493 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3872 train loss: 0.9272446632385254 train acc: 0.9775000214576721 test loss: 1.0275853872299194 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3873 train loss: 0.9273114204406738 train acc: 0.9775000214576721 test loss: 1.028077244758606 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3874 train loss: 0.9272129535675049 train acc: 0.9775000214576721 test loss: 1.0288196802139282 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3875 train loss: 0.9272182583808899 train acc: 0.9775000214576721 test loss: 1.0215660333633423 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3876 train loss: 0.9272989630699158 train acc: 0.9775000214576721 test loss: 1.0340182781219482 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3877 train loss: 0.9272174835205078 train acc: 0.9775000214576721 test loss: 1.0238579511642456 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3878 train loss: 0.9272262454032898 train acc: 0.9775000214576721 test loss: 1.0305002927780151 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3879 train loss: 0.9271963238716125 train acc: 0.9775000214576721 test loss: 1.0399186611175537 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3880 train loss: 0.9273238182067871 train acc: 0.9775000214576721 test loss: 1.0261059999465942 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3881 train loss: 0.9272475242614746 train acc: 0.9775000214576721 test loss: 1.0252902507781982 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3882 train loss: 0.9272035360336304 train acc: 0.9775000214576721 test loss: 1.03114914894104 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3883 train loss: 0.9273390769958496 train acc: 0.9775000214576721 test loss: 1.0260664224624634 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3884 train loss: 0.9272996783256531 train acc: 0.9775000214576721 test loss: 1.0363234281539917 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3885 train loss: 0.9272656440734863 train acc: 0.9775000214576721 test loss: 1.0311470031738281 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3886 train loss: 0.9272016882896423 train acc: 0.9775000214576721 test loss: 1.036452293395996 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3887 train loss: 0.927327036857605 train acc: 0.9775000214576721 test loss: 1.0355132818222046 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3888 train loss: 0.9271872043609619 train acc: 0.9775000214576721 test loss: 1.025874376296997 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3889 train loss: 0.927246630191803 train acc: 0.9775000214576721 test loss: 1.0311996936798096 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3890 train loss: 0.9273200035095215 train acc: 0.9775000214576721 test loss: 1.0332424640655518 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3891 train loss: 0.9272738695144653 train acc: 0.9775000214576721 test loss: 1.0321934223175049 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3892 train loss: 0.9272100925445557 train acc: 0.9775000214576721 test loss: 1.028680443763733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3893 train loss: 0.9272924065589905 train acc: 0.9775000214576721 test loss: 1.0299843549728394 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3894 train loss: 0.9273318648338318 train acc: 0.9775000214576721 test loss: 1.0373945236206055 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3895 train loss: 0.9272210001945496 train acc: 0.9775000214576721 test loss: 1.0246922969818115 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3896 train loss: 0.9272699952125549 train acc: 0.9775000214576721 test loss: 1.0259318351745605 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3897 train loss: 0.9271866083145142 train acc: 0.9775000214576721 test loss: 1.0332626104354858 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3898 train loss: 0.9272369146347046 train acc: 0.9775000214576721 test loss: 1.0288441181182861 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3899 train loss: 0.9272866249084473 train acc: 0.9775000214576721 test loss: 1.0259958505630493 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3900 train loss: 0.927229106426239 train acc: 0.9775000214576721 test loss: 1.0298954248428345 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3901 train loss: 0.9272509813308716 train acc: 0.9775000214576721 test loss: 1.0322898626327515 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3902 train loss: 0.9273232817649841 train acc: 0.9775000214576721 test loss: 1.0240782499313354 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3903 train loss: 0.9272760152816772 train acc: 0.9775000214576721 test loss: 1.0304678678512573 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3904 train loss: 0.9271855354309082 train acc: 0.9775000214576721 test loss: 1.0395902395248413 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3905 train loss: 0.9273397326469421 train acc: 0.9775000214576721 test loss: 1.0251820087432861 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3906 train loss: 0.9273080229759216 train acc: 0.9775000214576721 test loss: 1.0389639139175415 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3907 train loss: 0.92718106508255 train acc: 0.9775000214576721 test loss: 1.030994176864624 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3908 train loss: 0.9272001385688782 train acc: 0.9775000214576721 test loss: 1.0304723978042603 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3909 train loss: 0.9273116588592529 train acc: 0.9775000214576721 test loss: 1.0256359577178955 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3910 train loss: 0.9271769523620605 train acc: 0.9775000214576721 test loss: 1.0363632440567017 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3911 train loss: 0.9273272156715393 train acc: 0.9775000214576721 test loss: 1.0335139036178589 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3912 train loss: 0.9272546172142029 train acc: 0.9775000214576721 test loss: 1.0265774726867676 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3913 train loss: 0.9273021221160889 train acc: 0.9775000214576721 test loss: 1.0284451246261597 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3914 train loss: 0.9272972941398621 train acc: 0.9775000214576721 test loss: 1.0314724445343018 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3915 train loss: 0.9272459149360657 train acc: 0.9775000214576721 test loss: 1.0303527116775513 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3916 train loss: 0.9272196888923645 train acc: 0.9775000214576721 test loss: 1.0416375398635864 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3917 train loss: 0.9272043108940125 train acc: 0.9775000214576721 test loss: 1.0461162328720093 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3918 train loss: 0.927308976650238 train acc: 0.9775000214576721 test loss: 1.0304402112960815 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3919 train loss: 0.927298903465271 train acc: 0.9775000214576721 test loss: 1.0240118503570557 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3920 train loss: 0.9273055195808411 train acc: 0.9775000214576721 test loss: 1.0295146703720093 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3921 train loss: 0.9271758794784546 train acc: 0.9775000214576721 test loss: 1.0446772575378418 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3922 train loss: 0.9271948337554932 train acc: 0.9775000214576721 test loss: 1.0326308012008667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3923 train loss: 0.9271791577339172 train acc: 0.9775000214576721 test loss: 1.0292285680770874 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3924 train loss: 0.9273079633712769 train acc: 0.9775000214576721 test loss: 1.0289584398269653 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3925 train loss: 0.9272221326828003 train acc: 0.9775000214576721 test loss: 1.0306905508041382 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3926 train loss: 0.9271847009658813 train acc: 0.9775000214576721 test loss: 1.0299285650253296 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3927 train loss: 0.927316427230835 train acc: 0.9775000214576721 test loss: 1.031040906906128 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3928 train loss: 0.9272385239601135 train acc: 0.9775000214576721 test loss: 1.035438895225525 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3929 train loss: 0.9272485971450806 train acc: 0.9775000214576721 test loss: 1.032490611076355 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3930 train loss: 0.9272491335868835 train acc: 0.9775000214576721 test loss: 1.0257891416549683 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3931 train loss: 0.9273359775543213 train acc: 0.9775000214576721 test loss: 1.038460612297058 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3932 train loss: 0.9271826148033142 train acc: 0.9775000214576721 test loss: 1.0366803407669067 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3933 train loss: 0.9272932410240173 train acc: 0.9775000214576721 test loss: 1.0371352434158325 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3934 train loss: 0.9273340106010437 train acc: 0.9775000214576721 test loss: 1.0316119194030762 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3935 train loss: 0.9273214936256409 train acc: 0.9775000214576721 test loss: 1.025605320930481 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3936 train loss: 0.927268385887146 train acc: 0.9775000214576721 test loss: 1.0285310745239258 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3937 train loss: 0.9271913170814514 train acc: 0.9775000214576721 test loss: 1.035659909248352 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3938 train loss: 0.9272429943084717 train acc: 0.9775000214576721 test loss: 1.0273901224136353 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3939 train loss: 0.9271835088729858 train acc: 0.9775000214576721 test loss: 1.0277700424194336 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3940 train loss: 0.927213191986084 train acc: 0.9775000214576721 test loss: 1.0394539833068848 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3941 train loss: 0.9271881580352783 train acc: 0.9775000214576721 test loss: 1.0408509969711304 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3942 train loss: 0.9272169470787048 train acc: 0.9775000214576721 test loss: 1.0345046520233154 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3943 train loss: 0.9272736310958862 train acc: 0.9775000214576721 test loss: 1.0213651657104492 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3944 train loss: 0.9273138642311096 train acc: 0.9775000214576721 test loss: 1.0311449766159058 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3945 train loss: 0.9272767901420593 train acc: 0.9775000214576721 test loss: 1.033508539199829 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3946 train loss: 0.9273321032524109 train acc: 0.9775000214576721 test loss: 1.0315184593200684 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3947 train loss: 0.9271925091743469 train acc: 0.9775000214576721 test loss: 1.0396316051483154 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3948 train loss: 0.9272967576980591 train acc: 0.9775000214576721 test loss: 1.0330557823181152 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3949 train loss: 0.9271875023841858 train acc: 0.9775000214576721 test loss: 1.029457449913025 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3950 train loss: 0.9272369146347046 train acc: 0.9775000214576721 test loss: 1.0278416872024536 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3951 train loss: 0.9272860884666443 train acc: 0.9775000214576721 test loss: 1.0319509506225586 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3952 train loss: 0.9271891713142395 train acc: 0.9775000214576721 test loss: 1.0346684455871582 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3953 train loss: 0.9272699952125549 train acc: 0.9775000214576721 test loss: 1.0253376960754395 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3954 train loss: 0.9272300601005554 train acc: 0.9775000214576721 test loss: 1.0278534889221191 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3955 train loss: 0.9272282123565674 train acc: 0.9775000214576721 test loss: 1.023748755455017 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3956 train loss: 0.9271823763847351 train acc: 0.9775000214576721 test loss: 1.0324604511260986 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3957 train loss: 0.9272575378417969 train acc: 0.9775000214576721 test loss: 1.032871961593628 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3958 train loss: 0.9272130727767944 train acc: 0.9775000214576721 test loss: 1.0259532928466797 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3959 train loss: 0.9272169470787048 train acc: 0.9775000214576721 test loss: 1.0245909690856934 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3960 train loss: 0.9273262619972229 train acc: 0.9775000214576721 test loss: 1.0347206592559814 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3961 train loss: 0.9271784424781799 train acc: 0.9775000214576721 test loss: 1.0349048376083374 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3962 train loss: 0.9272653460502625 train acc: 0.9775000214576721 test loss: 1.0278362035751343 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3963 train loss: 0.9272920489311218 train acc: 0.9775000214576721 test loss: 1.0276943445205688 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3964 train loss: 0.9273284673690796 train acc: 0.9775000214576721 test loss: 1.0416676998138428 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3965 train loss: 0.9272360801696777 train acc: 0.9775000214576721 test loss: 1.0391159057617188 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3966 train loss: 0.9272254109382629 train acc: 0.9775000214576721 test loss: 1.0353844165802002 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3967 train loss: 0.9272444248199463 train acc: 0.9775000214576721 test loss: 1.0273241996765137 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3968 train loss: 0.927249014377594 train acc: 0.9775000214576721 test loss: 1.0388803482055664 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3969 train loss: 0.9273371696472168 train acc: 0.9775000214576721 test loss: 1.0315765142440796 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3970 train loss: 0.9271941184997559 train acc: 0.9775000214576721 test loss: 1.034737229347229 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3971 train loss: 0.9271836876869202 train acc: 0.9775000214576721 test loss: 1.0437873601913452 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3972 train loss: 0.927192747592926 train acc: 0.9775000214576721 test loss: 1.0417835712432861 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3973 train loss: 0.9272421002388 train acc: 0.9775000214576721 test loss: 1.0319244861602783 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3974 train loss: 0.9272058010101318 train acc: 0.9775000214576721 test loss: 1.026155948638916 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3975 train loss: 0.9273300170898438 train acc: 0.9775000214576721 test loss: 1.0346721410751343 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3976 train loss: 0.9272461533546448 train acc: 0.9775000214576721 test loss: 1.0260509252548218 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3977 train loss: 0.9273049235343933 train acc: 0.9775000214576721 test loss: 1.0518571138381958 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 3978 train loss: 0.9271791577339172 train acc: 0.9775000214576721 test loss: 1.0327370166778564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3979 train loss: 0.9273262619972229 train acc: 0.9775000214576721 test loss: 1.0283994674682617 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3980 train loss: 0.927265465259552 train acc: 0.9775000214576721 test loss: 1.0279189348220825 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3981 train loss: 0.9271847605705261 train acc: 0.9775000214576721 test loss: 1.0442440509796143 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3982 train loss: 0.9271766543388367 train acc: 0.9775000214576721 test loss: 1.0300143957138062 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3983 train loss: 0.9272752404212952 train acc: 0.9775000214576721 test loss: 1.027580976486206 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3984 train loss: 0.9272070527076721 train acc: 0.9775000214576721 test loss: 1.0252822637557983 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3985 train loss: 0.9272849559783936 train acc: 0.9775000214576721 test loss: 1.0298762321472168 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3986 train loss: 0.927187442779541 train acc: 0.9775000214576721 test loss: 1.0323249101638794 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3987 train loss: 0.9271789789199829 train acc: 0.9775000214576721 test loss: 1.0264623165130615 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3988 train loss: 0.9272714257240295 train acc: 0.9775000214576721 test loss: 1.026717185974121 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3989 train loss: 0.9272916316986084 train acc: 0.9775000214576721 test loss: 1.04194974899292 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 3990 train loss: 0.9272508025169373 train acc: 0.9775000214576721 test loss: 1.0341509580612183 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3991 train loss: 0.9273204207420349 train acc: 0.9775000214576721 test loss: 1.025957465171814 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3992 train loss: 0.9271894693374634 train acc: 0.9775000214576721 test loss: 1.033031940460205 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3993 train loss: 0.927236795425415 train acc: 0.9775000214576721 test loss: 1.0292176008224487 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3994 train loss: 0.9272707104682922 train acc: 0.9775000214576721 test loss: 1.0291324853897095 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3995 train loss: 0.9272210001945496 train acc: 0.9775000214576721 test loss: 1.0269628763198853 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3996 train loss: 0.9272152781486511 train acc: 0.9775000214576721 test loss: 1.027524709701538 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3997 train loss: 0.9273008108139038 train acc: 0.9775000214576721 test loss: 1.0315946340560913 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 3998 train loss: 0.9271903038024902 train acc: 0.9775000214576721 test loss: 1.0268235206604004 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 3999 train loss: 0.9273307919502258 train acc: 0.9775000214576721 test loss: 1.0245192050933838 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4000 train loss: 0.9271957278251648 train acc: 0.9775000214576721 test loss: 1.0324649810791016 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4001 train loss: 0.9271818399429321 train acc: 0.9775000214576721 test loss: 1.0365749597549438 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4002 train loss: 0.927283525466919 train acc: 0.9775000214576721 test loss: 1.031642198562622 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4003 train loss: 0.927183985710144 train acc: 0.9775000214576721 test loss: 1.0442103147506714 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4004 train loss: 0.9272289872169495 train acc: 0.9775000214576721 test loss: 1.028809666633606 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4005 train loss: 0.9272987246513367 train acc: 0.9775000214576721 test loss: 1.0301790237426758 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4006 train loss: 0.9271930456161499 train acc: 0.9775000214576721 test loss: 1.0413227081298828 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4007 train loss: 0.9272770881652832 train acc: 0.9775000214576721 test loss: 1.031601071357727 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4008 train loss: 0.9271767139434814 train acc: 0.9775000214576721 test loss: 1.0314059257507324 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4009 train loss: 0.9272348880767822 train acc: 0.9775000214576721 test loss: 1.0373597145080566 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4010 train loss: 0.9272589683532715 train acc: 0.9775000214576721 test loss: 1.038008451461792 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4011 train loss: 0.9272761344909668 train acc: 0.9775000214576721 test loss: 1.0282542705535889 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4012 train loss: 0.9272920489311218 train acc: 0.9775000214576721 test loss: 1.0295627117156982 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4013 train loss: 0.9271806478500366 train acc: 0.9775000214576721 test loss: 1.0246564149856567 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4014 train loss: 0.9272474646568298 train acc: 0.9775000214576721 test loss: 1.0233579874038696 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4015 train loss: 0.9271849989891052 train acc: 0.9775000214576721 test loss: 1.0260082483291626 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4016 train loss: 0.9272027015686035 train acc: 0.9775000214576721 test loss: 1.035344123840332 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4017 train loss: 0.9272539019584656 train acc: 0.9775000214576721 test loss: 1.0240687131881714 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4018 train loss: 0.9273171424865723 train acc: 0.9775000214576721 test loss: 1.0270730257034302 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4019 train loss: 0.927191972732544 train acc: 0.9775000214576721 test loss: 1.0244293212890625 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4020 train loss: 0.9271942973136902 train acc: 0.9775000214576721 test loss: 1.0249629020690918 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4021 train loss: 0.927192211151123 train acc: 0.9775000214576721 test loss: 1.0281180143356323 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4022 train loss: 0.9272814989089966 train acc: 0.9775000214576721 test loss: 1.0322047472000122 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4023 train loss: 0.9272875785827637 train acc: 0.9775000214576721 test loss: 1.0305365324020386 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4024 train loss: 0.9272799491882324 train acc: 0.9775000214576721 test loss: 1.0350444316864014 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4025 train loss: 0.9272653460502625 train acc: 0.9775000214576721 test loss: 1.026422381401062 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4026 train loss: 0.9271771907806396 train acc: 0.9775000214576721 test loss: 1.03095281124115 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4027 train loss: 0.927285373210907 train acc: 0.9775000214576721 test loss: 1.033097743988037 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4028 train loss: 0.9271869659423828 train acc: 0.9775000214576721 test loss: 1.0260063409805298 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4029 train loss: 0.927227795124054 train acc: 0.9775000214576721 test loss: 1.0322232246398926 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4030 train loss: 0.927194356918335 train acc: 0.9775000214576721 test loss: 1.0253826379776 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4031 train loss: 0.9272125363349915 train acc: 0.9775000214576721 test loss: 1.0337132215499878 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4032 train loss: 0.927177906036377 train acc: 0.9775000214576721 test loss: 1.0265215635299683 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4033 train loss: 0.927323579788208 train acc: 0.9775000214576721 test loss: 1.031273365020752 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4034 train loss: 0.9272670149803162 train acc: 0.9775000214576721 test loss: 1.0310852527618408 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4035 train loss: 0.9272733330726624 train acc: 0.9775000214576721 test loss: 1.0255595445632935 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4036 train loss: 0.9271923303604126 train acc: 0.9775000214576721 test loss: 1.0263005495071411 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4037 train loss: 0.927202045917511 train acc: 0.9775000214576721 test loss: 1.0335561037063599 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4038 train loss: 0.927237868309021 train acc: 0.9775000214576721 test loss: 1.0295970439910889 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4039 train loss: 0.9271777868270874 train acc: 0.9775000214576721 test loss: 1.031430721282959 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4040 train loss: 0.9272441864013672 train acc: 0.9775000214576721 test loss: 1.0248419046401978 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4041 train loss: 0.9272607564926147 train acc: 0.9775000214576721 test loss: 1.0285801887512207 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4042 train loss: 0.9271949529647827 train acc: 0.9775000214576721 test loss: 1.034561038017273 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4043 train loss: 0.927207887172699 train acc: 0.9775000214576721 test loss: 1.025016188621521 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4044 train loss: 0.9272719621658325 train acc: 0.9775000214576721 test loss: 1.031897783279419 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4045 train loss: 0.9271817803382874 train acc: 0.9775000214576721 test loss: 1.0401731729507446 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4046 train loss: 0.9272804260253906 train acc: 0.9775000214576721 test loss: 1.0296107530593872 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4047 train loss: 0.9271751642227173 train acc: 0.9775000214576721 test loss: 1.0321331024169922 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4048 train loss: 0.9273443818092346 train acc: 0.9775000214576721 test loss: 1.0243169069290161 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4049 train loss: 0.9273280501365662 train acc: 0.9775000214576721 test loss: 1.029206395149231 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4050 train loss: 0.9272364974021912 train acc: 0.9775000214576721 test loss: 1.0251879692077637 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4051 train loss: 0.9271802306175232 train acc: 0.9775000214576721 test loss: 1.0311154127120972 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4052 train loss: 0.9271965026855469 train acc: 0.9775000214576721 test loss: 1.0322842597961426 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4053 train loss: 0.9271833896636963 train acc: 0.9775000214576721 test loss: 1.034560203552246 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4054 train loss: 0.9272700548171997 train acc: 0.9775000214576721 test loss: 1.0278469324111938 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4055 train loss: 0.9272674322128296 train acc: 0.9775000214576721 test loss: 1.0322439670562744 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4056 train loss: 0.9272211194038391 train acc: 0.9775000214576721 test loss: 1.035417914390564 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4057 train loss: 0.927276074886322 train acc: 0.9775000214576721 test loss: 1.0350314378738403 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4058 train loss: 0.9272247552871704 train acc: 0.9775000214576721 test loss: 1.0257651805877686 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4059 train loss: 0.9271767139434814 train acc: 0.9775000214576721 test loss: 1.026936411857605 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4060 train loss: 0.9271772503852844 train acc: 0.9775000214576721 test loss: 1.0355249643325806 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4061 train loss: 0.9271855354309082 train acc: 0.9775000214576721 test loss: 1.0297398567199707 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4062 train loss: 0.9272035360336304 train acc: 0.9775000214576721 test loss: 1.0267716646194458 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4063 train loss: 0.9272743463516235 train acc: 0.9775000214576721 test loss: 1.0274600982666016 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4064 train loss: 0.9273220896720886 train acc: 0.9775000214576721 test loss: 1.0373915433883667 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4065 train loss: 0.9272413849830627 train acc: 0.9775000214576721 test loss: 1.037963628768921 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4066 train loss: 0.9272379279136658 train acc: 0.9775000214576721 test loss: 1.0320876836776733 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4067 train loss: 0.9273045063018799 train acc: 0.9775000214576721 test loss: 1.0293582677841187 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4068 train loss: 0.9271817803382874 train acc: 0.9775000214576721 test loss: 1.034040093421936 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4069 train loss: 0.9272791147232056 train acc: 0.9775000214576721 test loss: 1.0348377227783203 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4070 train loss: 0.9272964596748352 train acc: 0.9775000214576721 test loss: 1.0331149101257324 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4071 train loss: 0.9272896647453308 train acc: 0.9775000214576721 test loss: 1.0277096033096313 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4072 train loss: 0.9271877408027649 train acc: 0.9775000214576721 test loss: 1.031035304069519 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4073 train loss: 0.9271760582923889 train acc: 0.9775000214576721 test loss: 1.0246167182922363 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4074 train loss: 0.9273260235786438 train acc: 0.9775000214576721 test loss: 1.0238888263702393 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4075 train loss: 0.9271817803382874 train acc: 0.9775000214576721 test loss: 1.040956974029541 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4076 train loss: 0.9272460341453552 train acc: 0.9775000214576721 test loss: 1.0408897399902344 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4077 train loss: 0.927287220954895 train acc: 0.9775000214576721 test loss: 1.0417723655700684 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4078 train loss: 0.9271953105926514 train acc: 0.9775000214576721 test loss: 1.0306895971298218 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4079 train loss: 0.9272837042808533 train acc: 0.9775000214576721 test loss: 1.0290778875350952 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4080 train loss: 0.9271905422210693 train acc: 0.9775000214576721 test loss: 1.0248647928237915 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4081 train loss: 0.92731112241745 train acc: 0.9775000214576721 test loss: 1.0403568744659424 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4082 train loss: 0.9272744655609131 train acc: 0.9775000214576721 test loss: 1.027458906173706 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4083 train loss: 0.927316427230835 train acc: 0.9775000214576721 test loss: 1.0338035821914673 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4084 train loss: 0.9272987246513367 train acc: 0.9775000214576721 test loss: 1.040320634841919 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4085 train loss: 0.9271800518035889 train acc: 0.9775000214576721 test loss: 1.0315883159637451 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4086 train loss: 0.927317202091217 train acc: 0.9775000214576721 test loss: 1.0302643775939941 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4087 train loss: 0.9272233843803406 train acc: 0.9775000214576721 test loss: 1.0413662195205688 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4088 train loss: 0.9272749423980713 train acc: 0.9775000214576721 test loss: 1.0366491079330444 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4089 train loss: 0.9272841811180115 train acc: 0.9775000214576721 test loss: 1.026726484298706 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4090 train loss: 0.9272675514221191 train acc: 0.9775000214576721 test loss: 1.0271005630493164 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4091 train loss: 0.9272393584251404 train acc: 0.9775000214576721 test loss: 1.0234109163284302 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4092 train loss: 0.9272632002830505 train acc: 0.9775000214576721 test loss: 1.0298327207565308 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4093 train loss: 0.9273015856742859 train acc: 0.9775000214576721 test loss: 1.023645043373108 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4094 train loss: 0.9271860718727112 train acc: 0.9775000214576721 test loss: 1.0535279512405396 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4095 train loss: 0.9272889494895935 train acc: 0.9775000214576721 test loss: 1.0317418575286865 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4096 train loss: 0.9273224472999573 train acc: 0.9775000214576721 test loss: 1.0340560674667358 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4097 train loss: 0.927184522151947 train acc: 0.9775000214576721 test loss: 1.0276638269424438 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4098 train loss: 0.9271944165229797 train acc: 0.9775000214576721 test loss: 1.0267362594604492 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4099 train loss: 0.9271976351737976 train acc: 0.9775000214576721 test loss: 1.027798056602478 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4100 train loss: 0.9272699952125549 train acc: 0.9775000214576721 test loss: 1.0265306234359741 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4101 train loss: 0.9272472858428955 train acc: 0.9775000214576721 test loss: 1.033424973487854 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4102 train loss: 0.9271798133850098 train acc: 0.9775000214576721 test loss: 1.0259312391281128 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4103 train loss: 0.9271803498268127 train acc: 0.9775000214576721 test loss: 1.0351417064666748 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4104 train loss: 0.9272575378417969 train acc: 0.9775000214576721 test loss: 1.0306190252304077 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4105 train loss: 0.927330493927002 train acc: 0.9775000214576721 test loss: 1.0247771739959717 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4106 train loss: 0.927207350730896 train acc: 0.9775000214576721 test loss: 1.0288854837417603 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4107 train loss: 0.927227795124054 train acc: 0.9775000214576721 test loss: 1.026232361793518 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4108 train loss: 0.9271858930587769 train acc: 0.9775000214576721 test loss: 1.031041145324707 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4109 train loss: 0.9272844791412354 train acc: 0.9775000214576721 test loss: 1.0343533754348755 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4110 train loss: 0.9271806478500366 train acc: 0.9775000214576721 test loss: 1.0247708559036255 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4111 train loss: 0.9273161292076111 train acc: 0.9775000214576721 test loss: 1.0353221893310547 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4112 train loss: 0.9273175597190857 train acc: 0.9775000214576721 test loss: 1.0232027769088745 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4113 train loss: 0.9272133111953735 train acc: 0.9775000214576721 test loss: 1.0263819694519043 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4114 train loss: 0.9271965622901917 train acc: 0.9775000214576721 test loss: 1.037627100944519 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4115 train loss: 0.9272680878639221 train acc: 0.9775000214576721 test loss: 1.0365568399429321 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4116 train loss: 0.9272711277008057 train acc: 0.9775000214576721 test loss: 1.027235746383667 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4117 train loss: 0.9273180365562439 train acc: 0.9775000214576721 test loss: 1.0337733030319214 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4118 train loss: 0.9272724986076355 train acc: 0.9775000214576721 test loss: 1.032681941986084 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4119 train loss: 0.9273118376731873 train acc: 0.9775000214576721 test loss: 1.039878010749817 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4120 train loss: 0.9272294640541077 train acc: 0.9775000214576721 test loss: 1.0294890403747559 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4121 train loss: 0.9272240996360779 train acc: 0.9775000214576721 test loss: 1.0352400541305542 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4122 train loss: 0.9271805286407471 train acc: 0.9775000214576721 test loss: 1.0280098915100098 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4123 train loss: 0.927203357219696 train acc: 0.9775000214576721 test loss: 1.039261817932129 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4124 train loss: 0.9273345470428467 train acc: 0.9775000214576721 test loss: 1.0327658653259277 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4125 train loss: 0.9271801114082336 train acc: 0.9775000214576721 test loss: 1.0322374105453491 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4126 train loss: 0.9271791577339172 train acc: 0.9775000214576721 test loss: 1.037594199180603 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4127 train loss: 0.9273055791854858 train acc: 0.9775000214576721 test loss: 1.030971884727478 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4128 train loss: 0.9271878004074097 train acc: 0.9775000214576721 test loss: 1.0273669958114624 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4129 train loss: 0.9272264242172241 train acc: 0.9775000214576721 test loss: 1.0355526208877563 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4130 train loss: 0.9271954298019409 train acc: 0.9775000214576721 test loss: 1.0421234369277954 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4131 train loss: 0.9273133277893066 train acc: 0.9775000214576721 test loss: 1.0344130992889404 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4132 train loss: 0.9271860718727112 train acc: 0.9775000214576721 test loss: 1.028377652168274 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4133 train loss: 0.927325963973999 train acc: 0.9775000214576721 test loss: 1.0333322286605835 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4134 train loss: 0.9271798729896545 train acc: 0.9775000214576721 test loss: 1.0319064855575562 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4135 train loss: 0.9272109866142273 train acc: 0.9775000214576721 test loss: 1.0456881523132324 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4136 train loss: 0.9272713661193848 train acc: 0.9775000214576721 test loss: 1.0371887683868408 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4137 train loss: 0.92730313539505 train acc: 0.9775000214576721 test loss: 1.034336805343628 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4138 train loss: 0.9272189140319824 train acc: 0.9775000214576721 test loss: 1.0325560569763184 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4139 train loss: 0.9271793961524963 train acc: 0.9775000214576721 test loss: 1.0305804014205933 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4140 train loss: 0.9272886514663696 train acc: 0.9775000214576721 test loss: 1.0297166109085083 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4141 train loss: 0.9273262023925781 train acc: 0.9775000214576721 test loss: 1.045375943183899 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4142 train loss: 0.9271802306175232 train acc: 0.9775000214576721 test loss: 1.0324456691741943 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4143 train loss: 0.9273242354393005 train acc: 0.9775000214576721 test loss: 1.0257165431976318 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4144 train loss: 0.9272082448005676 train acc: 0.9775000214576721 test loss: 1.0263842344284058 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4145 train loss: 0.9272079467773438 train acc: 0.9775000214576721 test loss: 1.0237679481506348 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4146 train loss: 0.9272171854972839 train acc: 0.9775000214576721 test loss: 1.0337789058685303 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4147 train loss: 0.9272672533988953 train acc: 0.9775000214576721 test loss: 1.025743007659912 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4148 train loss: 0.9272872805595398 train acc: 0.9775000214576721 test loss: 1.0277878046035767 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4149 train loss: 0.9272657036781311 train acc: 0.9775000214576721 test loss: 1.041121244430542 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4150 train loss: 0.9272632002830505 train acc: 0.9775000214576721 test loss: 1.02875554561615 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4151 train loss: 0.9271806478500366 train acc: 0.9775000214576721 test loss: 1.0313104391098022 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4152 train loss: 0.927318811416626 train acc: 0.9775000214576721 test loss: 1.023954153060913 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4153 train loss: 0.927216649055481 train acc: 0.9775000214576721 test loss: 1.0335129499435425 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4154 train loss: 0.9273012280464172 train acc: 0.9775000214576721 test loss: 1.0361318588256836 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4155 train loss: 0.9273197054862976 train acc: 0.9775000214576721 test loss: 1.0266799926757812 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4156 train loss: 0.9272185564041138 train acc: 0.9775000214576721 test loss: 1.0356487035751343 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4157 train loss: 0.9272815585136414 train acc: 0.9775000214576721 test loss: 1.0338044166564941 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4158 train loss: 0.9272604584693909 train acc: 0.9775000214576721 test loss: 1.0255337953567505 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4159 train loss: 0.9273492693901062 train acc: 0.9775000214576721 test loss: 1.0355067253112793 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4160 train loss: 0.9273174405097961 train acc: 0.9775000214576721 test loss: 1.0272696018218994 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4161 train loss: 0.927336573600769 train acc: 0.9775000214576721 test loss: 1.0286645889282227 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4162 train loss: 0.9272551536560059 train acc: 0.9775000214576721 test loss: 1.0307785272598267 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4163 train loss: 0.9273348450660706 train acc: 0.9775000214576721 test loss: 1.033195972442627 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4164 train loss: 0.9273365139961243 train acc: 0.9775000214576721 test loss: 1.0354880094528198 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4165 train loss: 0.9273137450218201 train acc: 0.9775000214576721 test loss: 1.0281643867492676 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4166 train loss: 0.9273667335510254 train acc: 0.9775000214576721 test loss: 1.0284881591796875 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4167 train loss: 0.9273098707199097 train acc: 0.9775000214576721 test loss: 1.0329362154006958 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4168 train loss: 0.9272326827049255 train acc: 0.9775000214576721 test loss: 1.0390762090682983 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4169 train loss: 0.9272575378417969 train acc: 0.9775000214576721 test loss: 1.029710292816162 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4170 train loss: 0.9271880984306335 train acc: 0.9775000214576721 test loss: 1.0324379205703735 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4171 train loss: 0.9273027181625366 train acc: 0.9775000214576721 test loss: 1.0355991125106812 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4172 train loss: 0.9271829128265381 train acc: 0.9775000214576721 test loss: 1.0248653888702393 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4173 train loss: 0.9272663593292236 train acc: 0.9775000214576721 test loss: 1.0285677909851074 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4174 train loss: 0.9272595047950745 train acc: 0.9775000214576721 test loss: 1.0273891687393188 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4175 train loss: 0.9272353649139404 train acc: 0.9775000214576721 test loss: 1.0280455350875854 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4176 train loss: 0.9271828532218933 train acc: 0.9775000214576721 test loss: 1.024604082107544 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4177 train loss: 0.9271790385246277 train acc: 0.9775000214576721 test loss: 1.0364923477172852 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4178 train loss: 0.9273414015769958 train acc: 0.9775000214576721 test loss: 1.0350463390350342 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4179 train loss: 0.9271770715713501 train acc: 0.9775000214576721 test loss: 1.040014624595642 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4180 train loss: 0.9272316098213196 train acc: 0.9775000214576721 test loss: 1.0324070453643799 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4181 train loss: 0.9272012114524841 train acc: 0.9775000214576721 test loss: 1.0377854108810425 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4182 train loss: 0.9272427558898926 train acc: 0.9775000214576721 test loss: 1.039525032043457 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4183 train loss: 0.9271894693374634 train acc: 0.9775000214576721 test loss: 1.0399446487426758 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4184 train loss: 0.9271796941757202 train acc: 0.9775000214576721 test loss: 1.026605248451233 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4185 train loss: 0.9272521138191223 train acc: 0.9775000214576721 test loss: 1.0237982273101807 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4186 train loss: 0.9273222088813782 train acc: 0.9775000214576721 test loss: 1.0313591957092285 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4187 train loss: 0.9272691607475281 train acc: 0.9775000214576721 test loss: 1.0381388664245605 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4188 train loss: 0.9273145198822021 train acc: 0.9775000214576721 test loss: 1.0334413051605225 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4189 train loss: 0.9272634983062744 train acc: 0.9775000214576721 test loss: 1.0324718952178955 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4190 train loss: 0.9272462725639343 train acc: 0.9775000214576721 test loss: 1.0315319299697876 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4191 train loss: 0.9273014068603516 train acc: 0.9775000214576721 test loss: 1.0283313989639282 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4192 train loss: 0.9272695183753967 train acc: 0.9775000214576721 test loss: 1.0258253812789917 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4193 train loss: 0.9272975921630859 train acc: 0.9775000214576721 test loss: 1.037653923034668 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4194 train loss: 0.9273346662521362 train acc: 0.9775000214576721 test loss: 1.0352911949157715 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4195 train loss: 0.9271718859672546 train acc: 0.9775000214576721 test loss: 1.0385785102844238 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4196 train loss: 0.9272224307060242 train acc: 0.9775000214576721 test loss: 1.0335123538970947 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4197 train loss: 0.927299976348877 train acc: 0.9775000214576721 test loss: 1.0295274257659912 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4198 train loss: 0.92725670337677 train acc: 0.9775000214576721 test loss: 1.038942813873291 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4199 train loss: 0.9271758198738098 train acc: 0.9775000214576721 test loss: 1.0338945388793945 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4200 train loss: 0.9271847605705261 train acc: 0.9775000214576721 test loss: 1.0321182012557983 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4201 train loss: 0.9273270964622498 train acc: 0.9775000214576721 test loss: 1.0311226844787598 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4202 train loss: 0.9273083209991455 train acc: 0.9775000214576721 test loss: 1.0296540260314941 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4203 train loss: 0.9272488355636597 train acc: 0.9775000214576721 test loss: 1.0429118871688843 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4204 train loss: 0.9272099137306213 train acc: 0.9775000214576721 test loss: 1.0272672176361084 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4205 train loss: 0.927308201789856 train acc: 0.9775000214576721 test loss: 1.0238795280456543 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4206 train loss: 0.9271817803382874 train acc: 0.9775000214576721 test loss: 1.0337345600128174 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4207 train loss: 0.9272619485855103 train acc: 0.9775000214576721 test loss: 1.0243600606918335 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4208 train loss: 0.9273145794868469 train acc: 0.9775000214576721 test loss: 1.0292863845825195 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4209 train loss: 0.9271872043609619 train acc: 0.9775000214576721 test loss: 1.0308575630187988 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4210 train loss: 0.9271894693374634 train acc: 0.9775000214576721 test loss: 1.0381830930709839 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4211 train loss: 0.927182674407959 train acc: 0.9775000214576721 test loss: 1.0461747646331787 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4212 train loss: 0.9272369146347046 train acc: 0.9775000214576721 test loss: 1.0390331745147705 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4213 train loss: 0.9272013306617737 train acc: 0.9775000214576721 test loss: 1.0294373035430908 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4214 train loss: 0.9272749423980713 train acc: 0.9775000214576721 test loss: 1.0316218137741089 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4215 train loss: 0.9272840023040771 train acc: 0.9775000214576721 test loss: 1.0347329378128052 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4216 train loss: 0.9271932244300842 train acc: 0.9775000214576721 test loss: 1.0413473844528198 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4217 train loss: 0.9271948933601379 train acc: 0.9775000214576721 test loss: 1.038713812828064 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4218 train loss: 0.9272068738937378 train acc: 0.9775000214576721 test loss: 1.038323998451233 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4219 train loss: 0.9272001385688782 train acc: 0.9775000214576721 test loss: 1.0308607816696167 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4220 train loss: 0.9271852374076843 train acc: 0.9775000214576721 test loss: 1.0415351390838623 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4221 train loss: 0.9272111654281616 train acc: 0.9775000214576721 test loss: 1.031596302986145 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4222 train loss: 0.9272795915603638 train acc: 0.9775000214576721 test loss: 1.0293997526168823 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4223 train loss: 0.9271852374076843 train acc: 0.9775000214576721 test loss: 1.0307344198226929 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4224 train loss: 0.9272919297218323 train acc: 0.9775000214576721 test loss: 1.0327774286270142 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4225 train loss: 0.927283763885498 train acc: 0.9775000214576721 test loss: 1.041212797164917 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4226 train loss: 0.9271795749664307 train acc: 0.9775000214576721 test loss: 1.0248057842254639 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4227 train loss: 0.9272559881210327 train acc: 0.9775000214576721 test loss: 1.0303349494934082 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4228 train loss: 0.9272016882896423 train acc: 0.9775000214576721 test loss: 1.037378191947937 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4229 train loss: 0.9273262619972229 train acc: 0.9775000214576721 test loss: 1.031754732131958 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4230 train loss: 0.9273098707199097 train acc: 0.9775000214576721 test loss: 1.0471763610839844 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4231 train loss: 0.9272700548171997 train acc: 0.9775000214576721 test loss: 1.0356944799423218 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4232 train loss: 0.9272807240486145 train acc: 0.9775000214576721 test loss: 1.0434517860412598 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4233 train loss: 0.927164614200592 train acc: 0.9775000214576721 test loss: 1.03159499168396 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4234 train loss: 0.9272340536117554 train acc: 0.9775000214576721 test loss: 1.0441155433654785 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4235 train loss: 0.9271767139434814 train acc: 0.9775000214576721 test loss: 1.0259865522384644 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4236 train loss: 0.9272940754890442 train acc: 0.9775000214576721 test loss: 1.0307095050811768 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4237 train loss: 0.9271796345710754 train acc: 0.9775000214576721 test loss: 1.0276305675506592 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4238 train loss: 0.9273291826248169 train acc: 0.9775000214576721 test loss: 1.0318068265914917 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4239 train loss: 0.927236795425415 train acc: 0.9775000214576721 test loss: 1.0361089706420898 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4240 train loss: 0.927175760269165 train acc: 0.9775000214576721 test loss: 1.0346732139587402 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4241 train loss: 0.9273002743721008 train acc: 0.9775000214576721 test loss: 1.0255156755447388 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4242 train loss: 0.9272758364677429 train acc: 0.9775000214576721 test loss: 1.0403231382369995 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4243 train loss: 0.9271724224090576 train acc: 0.9775000214576721 test loss: 1.033901572227478 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4244 train loss: 0.9276208281517029 train acc: 0.9775000214576721 test loss: 1.0340933799743652 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4245 train loss: 0.9272350072860718 train acc: 0.9775000214576721 test loss: 1.0311686992645264 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4246 train loss: 0.9272221326828003 train acc: 0.9775000214576721 test loss: 1.0385018587112427 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4247 train loss: 0.9273343682289124 train acc: 0.9775000214576721 test loss: 1.0517032146453857 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4248 train loss: 0.9272192120552063 train acc: 0.9775000214576721 test loss: 1.0325621366500854 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4249 train loss: 0.9272359609603882 train acc: 0.9775000214576721 test loss: 1.0368860960006714 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4250 train loss: 0.9271542429924011 train acc: 0.9775000214576721 test loss: 1.0366952419281006 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4251 train loss: 0.927167534828186 train acc: 0.9775000214576721 test loss: 1.0341531038284302 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4252 train loss: 0.9272997379302979 train acc: 0.9775000214576721 test loss: 1.036999225616455 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4253 train loss: 0.9276058077812195 train acc: 0.9775000214576721 test loss: 1.0453532934188843 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4254 train loss: 0.9269389510154724 train acc: 0.9775000214576721 test loss: 1.0370477437973022 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4255 train loss: 0.9261096715927124 train acc: 0.9775000214576721 test loss: 1.0398015975952148 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4256 train loss: 0.9267029762268066 train acc: 0.9775000214576721 test loss: 1.041151523590088 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4257 train loss: 0.9250444769859314 train acc: 0.9800000190734863 test loss: 1.0408564805984497 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4258 train loss: 0.9248790144920349 train acc: 0.9800000190734863 test loss: 1.0450544357299805 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4259 train loss: 0.9256412982940674 train acc: 0.9800000190734863 test loss: 1.04323148727417 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4260 train loss: 0.9268150925636292 train acc: 0.9775000214576721 test loss: 1.0434023141860962 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4261 train loss: 0.9251047372817993 train acc: 0.9800000190734863 test loss: 1.0362943410873413 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4262 train loss: 0.9248934388160706 train acc: 0.9800000190734863 test loss: 1.0465699434280396 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4263 train loss: 0.9247016906738281 train acc: 0.9800000190734863 test loss: 1.0479871034622192 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4264 train loss: 0.9247687458992004 train acc: 0.9800000190734863 test loss: 1.0474450588226318 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4265 train loss: 0.9247863292694092 train acc: 0.9800000190734863 test loss: 1.0415130853652954 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4266 train loss: 0.9247269630432129 train acc: 0.9800000190734863 test loss: 1.0520117282867432 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4267 train loss: 0.9249351620674133 train acc: 0.9800000190734863 test loss: 1.0508023500442505 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4268 train loss: 0.924834132194519 train acc: 0.9800000190734863 test loss: 1.0581172704696655 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4269 train loss: 0.9250772595405579 train acc: 0.9800000190734863 test loss: 1.0491185188293457 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4270 train loss: 0.925203800201416 train acc: 0.9800000190734863 test loss: 1.0490787029266357 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4271 train loss: 0.9248393177986145 train acc: 0.9800000190734863 test loss: 1.0565381050109863 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4272 train loss: 0.9254445433616638 train acc: 0.9800000190734863 test loss: 1.052586317062378 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4273 train loss: 0.9248635172843933 train acc: 0.9800000190734863 test loss: 1.0493477582931519 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4274 train loss: 0.9248079657554626 train acc: 0.9800000190734863 test loss: 1.0396780967712402 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4275 train loss: 0.9247251749038696 train acc: 0.9800000190734863 test loss: 1.0433087348937988 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4276 train loss: 0.9247761368751526 train acc: 0.9800000190734863 test loss: 1.0424891710281372 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4277 train loss: 0.92486971616745 train acc: 0.9800000190734863 test loss: 1.0469120740890503 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4278 train loss: 0.9247897267341614 train acc: 0.9800000190734863 test loss: 1.0312966108322144 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4279 train loss: 0.924846887588501 train acc: 0.9800000190734863 test loss: 1.0324006080627441 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4280 train loss: 0.9250125885009766 train acc: 0.9800000190734863 test loss: 1.042667031288147 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4281 train loss: 0.9248403906822205 train acc: 0.9800000190734863 test loss: 1.044517159461975 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4282 train loss: 0.9247024655342102 train acc: 0.9800000190734863 test loss: 1.0423994064331055 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4283 train loss: 0.9247686266899109 train acc: 0.9800000190734863 test loss: 1.0443763732910156 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4284 train loss: 0.9247615337371826 train acc: 0.9800000190734863 test loss: 1.045459270477295 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4285 train loss: 0.9248241186141968 train acc: 0.9800000190734863 test loss: 1.0428948402404785 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4286 train loss: 0.9247349500656128 train acc: 0.9800000190734863 test loss: 1.0560718774795532 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4287 train loss: 0.9248082637786865 train acc: 0.9800000190734863 test loss: 1.0459256172180176 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4288 train loss: 0.9248935580253601 train acc: 0.9800000190734863 test loss: 1.0421546697616577 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4289 train loss: 0.9248912334442139 train acc: 0.9800000190734863 test loss: 1.0449354648590088 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4290 train loss: 0.9248121380805969 train acc: 0.9800000190734863 test loss: 1.0440481901168823 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4291 train loss: 0.92486971616745 train acc: 0.9800000190734863 test loss: 1.0445045232772827 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4292 train loss: 0.9248393177986145 train acc: 0.9800000190734863 test loss: 1.0446133613586426 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4293 train loss: 0.9247826337814331 train acc: 0.9800000190734863 test loss: 1.0414596796035767 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4294 train loss: 0.9248086810112 train acc: 0.9800000190734863 test loss: 1.0540846586227417 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4295 train loss: 0.9247445464134216 train acc: 0.9800000190734863 test loss: 1.048301339149475 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4296 train loss: 0.9247714877128601 train acc: 0.9800000190734863 test loss: 1.043405532836914 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4297 train loss: 0.9247885942459106 train acc: 0.9800000190734863 test loss: 1.043114423751831 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4298 train loss: 0.9247027635574341 train acc: 0.9800000190734863 test loss: 1.0452567338943481 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4299 train loss: 0.9246907234191895 train acc: 0.9800000190734863 test loss: 1.0407615900039673 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4300 train loss: 0.9247305393218994 train acc: 0.9800000190734863 test loss: 1.0411579608917236 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4301 train loss: 0.9247490167617798 train acc: 0.9800000190734863 test loss: 1.0421124696731567 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4302 train loss: 0.9248512387275696 train acc: 0.9800000190734863 test loss: 1.0440268516540527 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4303 train loss: 0.9247633218765259 train acc: 0.9800000190734863 test loss: 1.0433621406555176 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4304 train loss: 0.9247457981109619 train acc: 0.9800000190734863 test loss: 1.0489263534545898 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4305 train loss: 0.9247452020645142 train acc: 0.9800000190734863 test loss: 1.0447221994400024 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4306 train loss: 0.9247294664382935 train acc: 0.9800000190734863 test loss: 1.0558383464813232 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4307 train loss: 0.9247007966041565 train acc: 0.9800000190734863 test loss: 1.043266773223877 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4308 train loss: 0.9246902465820312 train acc: 0.9800000190734863 test loss: 1.0424103736877441 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4309 train loss: 0.9246882796287537 train acc: 0.9800000190734863 test loss: 1.0438917875289917 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4310 train loss: 0.9246983528137207 train acc: 0.9800000190734863 test loss: 1.047035574913025 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4311 train loss: 0.9247374534606934 train acc: 0.9800000190734863 test loss: 1.0448681116104126 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4312 train loss: 0.9247959852218628 train acc: 0.9800000190734863 test loss: 1.0529754161834717 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4313 train loss: 0.9247775077819824 train acc: 0.9800000190734863 test loss: 1.0443484783172607 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4314 train loss: 0.9247403144836426 train acc: 0.9800000190734863 test loss: 1.0488600730895996 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4315 train loss: 0.9247088432312012 train acc: 0.9800000190734863 test loss: 1.0444493293762207 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4316 train loss: 0.9247260093688965 train acc: 0.9800000190734863 test loss: 1.0429177284240723 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4317 train loss: 0.9247052669525146 train acc: 0.9800000190734863 test loss: 1.0453364849090576 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4318 train loss: 0.9247711300849915 train acc: 0.9800000190734863 test loss: 1.0469993352890015 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4319 train loss: 0.9247909784317017 train acc: 0.9800000190734863 test loss: 1.0448764562606812 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4320 train loss: 0.9247918725013733 train acc: 0.9800000190734863 test loss: 1.0436606407165527 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4321 train loss: 0.9247047305107117 train acc: 0.9800000190734863 test loss: 1.042963981628418 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4322 train loss: 0.9247190952301025 train acc: 0.9800000190734863 test loss: 1.0441159009933472 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4323 train loss: 0.9246866106987 train acc: 0.9800000190734863 test loss: 1.0533320903778076 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4324 train loss: 0.9247469305992126 train acc: 0.9800000190734863 test loss: 1.048901915550232 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4325 train loss: 0.9247693419456482 train acc: 0.9800000190734863 test loss: 1.044071078300476 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4326 train loss: 0.9247679114341736 train acc: 0.9800000190734863 test loss: 1.0438674688339233 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4327 train loss: 0.924805760383606 train acc: 0.9800000190734863 test loss: 1.0458412170410156 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4328 train loss: 0.9247077107429504 train acc: 0.9800000190734863 test loss: 1.0435442924499512 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4329 train loss: 0.9249148368835449 train acc: 0.9800000190734863 test loss: 1.0444053411483765 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4330 train loss: 0.9247485995292664 train acc: 0.9800000190734863 test loss: 1.0447922945022583 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4331 train loss: 0.9247044324874878 train acc: 0.9800000190734863 test loss: 1.0447793006896973 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4332 train loss: 0.9247670769691467 train acc: 0.9800000190734863 test loss: 1.0607004165649414 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4333 train loss: 0.9247104525566101 train acc: 0.9800000190734863 test loss: 1.0475696325302124 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4334 train loss: 0.9246798157691956 train acc: 0.9800000190734863 test loss: 1.0447672605514526 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4335 train loss: 0.924757182598114 train acc: 0.9800000190734863 test loss: 1.0437458753585815 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4336 train loss: 0.9247779846191406 train acc: 0.9800000190734863 test loss: 1.0449920892715454 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4337 train loss: 0.9248367547988892 train acc: 0.9800000190734863 test loss: 1.0543162822723389 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4338 train loss: 0.9247125387191772 train acc: 0.9800000190734863 test loss: 1.0495857000350952 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4339 train loss: 0.9247459173202515 train acc: 0.9800000190734863 test loss: 1.0436440706253052 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4340 train loss: 0.9247789978981018 train acc: 0.9800000190734863 test loss: 1.0480146408081055 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4341 train loss: 0.9247854351997375 train acc: 0.9800000190734863 test loss: 1.0477535724639893 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4342 train loss: 0.9247076511383057 train acc: 0.9800000190734863 test loss: 1.0421098470687866 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4343 train loss: 0.9246925115585327 train acc: 0.9800000190734863 test loss: 1.0524765253067017 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4344 train loss: 0.9246925115585327 train acc: 0.9800000190734863 test loss: 1.046887993812561 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4345 train loss: 0.9247692227363586 train acc: 0.9800000190734863 test loss: 1.045596718788147 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4346 train loss: 0.9247164130210876 train acc: 0.9800000190734863 test loss: 1.0447615385055542 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4347 train loss: 0.924757182598114 train acc: 0.9800000190734863 test loss: 1.050778865814209 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4348 train loss: 0.9248610734939575 train acc: 0.9800000190734863 test loss: 1.0474281311035156 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4349 train loss: 0.9246910214424133 train acc: 0.9800000190734863 test loss: 1.0441151857376099 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4350 train loss: 0.9247807264328003 train acc: 0.9800000190734863 test loss: 1.0424634218215942 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4351 train loss: 0.9247552752494812 train acc: 0.9800000190734863 test loss: 1.0450161695480347 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4352 train loss: 0.9247588515281677 train acc: 0.9800000190734863 test loss: 1.0458626747131348 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4353 train loss: 0.9247534275054932 train acc: 0.9800000190734863 test loss: 1.0421522855758667 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4354 train loss: 0.9247472286224365 train acc: 0.9800000190734863 test loss: 1.0474674701690674 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4355 train loss: 0.9247117638587952 train acc: 0.9800000190734863 test loss: 1.04413640499115 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4356 train loss: 0.9248055815696716 train acc: 0.9800000190734863 test loss: 1.0496814250946045 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4357 train loss: 0.9247539043426514 train acc: 0.9800000190734863 test loss: 1.046491265296936 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4358 train loss: 0.9246931672096252 train acc: 0.9800000190734863 test loss: 1.0485378503799438 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4359 train loss: 0.9247297644615173 train acc: 0.9800000190734863 test loss: 1.0454707145690918 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4360 train loss: 0.9247021675109863 train acc: 0.9800000190734863 test loss: 1.0462840795516968 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4361 train loss: 0.9246954917907715 train acc: 0.9800000190734863 test loss: 1.0445544719696045 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4362 train loss: 0.9248100519180298 train acc: 0.9800000190734863 test loss: 1.0444614887237549 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4363 train loss: 0.9248199462890625 train acc: 0.9800000190734863 test loss: 1.0438569784164429 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4364 train loss: 0.9246823191642761 train acc: 0.9800000190734863 test loss: 1.0450751781463623 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4365 train loss: 0.9247287511825562 train acc: 0.9800000190734863 test loss: 1.0442283153533936 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4366 train loss: 0.9247052073478699 train acc: 0.9800000190734863 test loss: 1.043615460395813 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4367 train loss: 0.9247077703475952 train acc: 0.9800000190734863 test loss: 1.0466127395629883 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4368 train loss: 0.9247280359268188 train acc: 0.9800000190734863 test loss: 1.0443042516708374 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4369 train loss: 0.9247174859046936 train acc: 0.9800000190734863 test loss: 1.0431665182113647 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4370 train loss: 0.9248039126396179 train acc: 0.9800000190734863 test loss: 1.0437588691711426 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4371 train loss: 0.9247071146965027 train acc: 0.9800000190734863 test loss: 1.0445979833602905 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4372 train loss: 0.9248068332672119 train acc: 0.9800000190734863 test loss: 1.0444395542144775 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4373 train loss: 0.9247313141822815 train acc: 0.9800000190734863 test loss: 1.0503947734832764 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4374 train loss: 0.9247986078262329 train acc: 0.9800000190734863 test loss: 1.0540084838867188 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4375 train loss: 0.9246927499771118 train acc: 0.9800000190734863 test loss: 1.0436086654663086 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4376 train loss: 0.9247698187828064 train acc: 0.9800000190734863 test loss: 1.048843502998352 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4377 train loss: 0.9247074723243713 train acc: 0.9800000190734863 test loss: 1.0437908172607422 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4378 train loss: 0.9246810078620911 train acc: 0.9800000190734863 test loss: 1.0435024499893188 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4379 train loss: 0.9247303605079651 train acc: 0.9800000190734863 test loss: 1.0452699661254883 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4380 train loss: 0.9248161911964417 train acc: 0.9800000190734863 test loss: 1.044244408607483 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4381 train loss: 0.9246852993965149 train acc: 0.9800000190734863 test loss: 1.0443953275680542 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4382 train loss: 0.9247207641601562 train acc: 0.9800000190734863 test loss: 1.0499664545059204 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4383 train loss: 0.9247649908065796 train acc: 0.9800000190734863 test loss: 1.0522339344024658 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4384 train loss: 0.9247194528579712 train acc: 0.9800000190734863 test loss: 1.0495104789733887 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4385 train loss: 0.9247779846191406 train acc: 0.9800000190734863 test loss: 1.0497231483459473 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4386 train loss: 0.9248270988464355 train acc: 0.9800000190734863 test loss: 1.038356065750122 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4387 train loss: 0.9248369336128235 train acc: 0.9800000190734863 test loss: 1.0453383922576904 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4388 train loss: 0.9246878623962402 train acc: 0.9800000190734863 test loss: 1.0466328859329224 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4389 train loss: 0.9248366355895996 train acc: 0.9800000190734863 test loss: 1.0428657531738281 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4390 train loss: 0.9247986078262329 train acc: 0.9800000190734863 test loss: 1.0508418083190918 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4391 train loss: 0.9248406291007996 train acc: 0.9800000190734863 test loss: 1.0428485870361328 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4392 train loss: 0.9246839284896851 train acc: 0.9800000190734863 test loss: 1.0404024124145508 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4393 train loss: 0.9248092770576477 train acc: 0.9800000190734863 test loss: 1.0431733131408691 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4394 train loss: 0.9246956706047058 train acc: 0.9800000190734863 test loss: 1.048397421836853 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4395 train loss: 0.924802303314209 train acc: 0.9800000190734863 test loss: 1.044465184211731 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4396 train loss: 0.924700915813446 train acc: 0.9800000190734863 test loss: 1.0497268438339233 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4397 train loss: 0.9247803688049316 train acc: 0.9800000190734863 test loss: 1.0435348749160767 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4398 train loss: 0.9250243306159973 train acc: 0.9800000190734863 test loss: 1.0421805381774902 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4399 train loss: 0.9247351288795471 train acc: 0.9800000190734863 test loss: 1.0422972440719604 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4400 train loss: 0.9247852563858032 train acc: 0.9800000190734863 test loss: 1.041153907775879 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4401 train loss: 0.9246949553489685 train acc: 0.9800000190734863 test loss: 1.0402811765670776 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4402 train loss: 0.924818217754364 train acc: 0.9800000190734863 test loss: 1.04612398147583 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4403 train loss: 0.9247661828994751 train acc: 0.9800000190734863 test loss: 1.044257402420044 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4404 train loss: 0.9247280955314636 train acc: 0.9800000190734863 test loss: 1.0453904867172241 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4405 train loss: 0.9248349070549011 train acc: 0.9800000190734863 test loss: 1.0497326850891113 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4406 train loss: 0.9247152209281921 train acc: 0.9800000190734863 test loss: 1.0513927936553955 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4407 train loss: 0.9246886968612671 train acc: 0.9800000190734863 test loss: 1.0434547662734985 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4408 train loss: 0.9247763752937317 train acc: 0.9800000190734863 test loss: 1.046882152557373 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4409 train loss: 0.9246913194656372 train acc: 0.9800000190734863 test loss: 1.0489811897277832 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4410 train loss: 0.9247182607650757 train acc: 0.9800000190734863 test loss: 1.0537759065628052 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4411 train loss: 0.9246839284896851 train acc: 0.9800000190734863 test loss: 1.0410704612731934 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4412 train loss: 0.9247300624847412 train acc: 0.9800000190734863 test loss: 1.0486500263214111 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4413 train loss: 0.924848735332489 train acc: 0.9800000190734863 test loss: 1.0437816381454468 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4414 train loss: 0.9248166084289551 train acc: 0.9800000190734863 test loss: 1.0468422174453735 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4415 train loss: 0.9247496724128723 train acc: 0.9800000190734863 test loss: 1.0426766872406006 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4416 train loss: 0.9247986078262329 train acc: 0.9800000190734863 test loss: 1.0495837926864624 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4417 train loss: 0.9247539043426514 train acc: 0.9800000190734863 test loss: 1.0468868017196655 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4418 train loss: 0.9248086810112 train acc: 0.9800000190734863 test loss: 1.043765664100647 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4419 train loss: 0.9247466325759888 train acc: 0.9800000190734863 test loss: 1.0364177227020264 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4420 train loss: 0.9247714877128601 train acc: 0.9800000190734863 test loss: 1.044085144996643 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4421 train loss: 0.924813449382782 train acc: 0.9800000190734863 test loss: 1.0442874431610107 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4422 train loss: 0.924756646156311 train acc: 0.9800000190734863 test loss: 1.0371997356414795 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4423 train loss: 0.9248800873756409 train acc: 0.9800000190734863 test loss: 1.043724536895752 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4424 train loss: 0.9247750043869019 train acc: 0.9800000190734863 test loss: 1.0518428087234497 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4425 train loss: 0.9246883988380432 train acc: 0.9800000190734863 test loss: 1.0397742986679077 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4426 train loss: 0.9247043132781982 train acc: 0.9800000190734863 test loss: 1.0445746183395386 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4427 train loss: 0.9246829152107239 train acc: 0.9800000190734863 test loss: 1.056169867515564 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4428 train loss: 0.9247103333473206 train acc: 0.9800000190734863 test loss: 1.0400210618972778 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4429 train loss: 0.9247511029243469 train acc: 0.9800000190734863 test loss: 1.043280005455017 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4430 train loss: 0.9247863292694092 train acc: 0.9800000190734863 test loss: 1.0446066856384277 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4431 train loss: 0.9246786236763 train acc: 0.9800000190734863 test loss: 1.0434465408325195 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4432 train loss: 0.9247634410858154 train acc: 0.9800000190734863 test loss: 1.0439435243606567 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4433 train loss: 0.9247009754180908 train acc: 0.9800000190734863 test loss: 1.0442888736724854 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4434 train loss: 0.9247028827667236 train acc: 0.9800000190734863 test loss: 1.0447379350662231 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4435 train loss: 0.9247196912765503 train acc: 0.9800000190734863 test loss: 1.0520963668823242 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4436 train loss: 0.9246785640716553 train acc: 0.9800000190734863 test loss: 1.0474073886871338 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4437 train loss: 0.9247782230377197 train acc: 0.9800000190734863 test loss: 1.0515257120132446 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4438 train loss: 0.9246922135353088 train acc: 0.9800000190734863 test loss: 1.0437129735946655 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4439 train loss: 0.9248254299163818 train acc: 0.9800000190734863 test loss: 1.0443719625473022 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4440 train loss: 0.9247079491615295 train acc: 0.9800000190734863 test loss: 1.0442774295806885 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4441 train loss: 0.9247975945472717 train acc: 0.9800000190734863 test loss: 1.0395441055297852 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4442 train loss: 0.9247027039527893 train acc: 0.9800000190734863 test loss: 1.0508898496627808 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4443 train loss: 0.9246964454650879 train acc: 0.9800000190734863 test loss: 1.0466506481170654 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4444 train loss: 0.9247192144393921 train acc: 0.9800000190734863 test loss: 1.0486754179000854 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4445 train loss: 0.9247742295265198 train acc: 0.9800000190734863 test loss: 1.045555830001831 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4446 train loss: 0.924721360206604 train acc: 0.9800000190734863 test loss: 1.0445828437805176 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4447 train loss: 0.9249734282493591 train acc: 0.9800000190734863 test loss: 1.0391165018081665 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4448 train loss: 0.9247434735298157 train acc: 0.9800000190734863 test loss: 1.0370200872421265 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4449 train loss: 0.924791157245636 train acc: 0.9800000190734863 test loss: 1.0394692420959473 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4450 train loss: 0.9247226119041443 train acc: 0.9800000190734863 test loss: 1.0490028858184814 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4451 train loss: 0.9246985912322998 train acc: 0.9800000190734863 test loss: 1.0500521659851074 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4452 train loss: 0.9246971011161804 train acc: 0.9800000190734863 test loss: 1.0431408882141113 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4453 train loss: 0.9247435927391052 train acc: 0.9800000190734863 test loss: 1.047340750694275 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4454 train loss: 0.924774169921875 train acc: 0.9800000190734863 test loss: 1.0425132513046265 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4455 train loss: 0.924781322479248 train acc: 0.9800000190734863 test loss: 1.0507984161376953 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4456 train loss: 0.9247720837593079 train acc: 0.9800000190734863 test loss: 1.0555379390716553 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4457 train loss: 0.9247962832450867 train acc: 0.9800000190734863 test loss: 1.0510112047195435 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4458 train loss: 0.9247732758522034 train acc: 0.9800000190734863 test loss: 1.0558114051818848 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4459 train loss: 0.924772322177887 train acc: 0.9800000190734863 test loss: 1.0436588525772095 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4460 train loss: 0.9247552752494812 train acc: 0.9800000190734863 test loss: 1.042110562324524 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4461 train loss: 0.9246976971626282 train acc: 0.9800000190734863 test loss: 1.0435101985931396 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4462 train loss: 0.924685001373291 train acc: 0.9800000190734863 test loss: 1.0406899452209473 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4463 train loss: 0.9247670769691467 train acc: 0.9800000190734863 test loss: 1.0516055822372437 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4464 train loss: 0.9247195720672607 train acc: 0.9800000190734863 test loss: 1.046442985534668 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4465 train loss: 0.9247016906738281 train acc: 0.9800000190734863 test loss: 1.0427627563476562 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4466 train loss: 0.9247328042984009 train acc: 0.9800000190734863 test loss: 1.0398598909378052 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4467 train loss: 0.9248362183570862 train acc: 0.9800000190734863 test loss: 1.0436809062957764 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4468 train loss: 0.924761950969696 train acc: 0.9800000190734863 test loss: 1.045928716659546 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4469 train loss: 0.9247804880142212 train acc: 0.9800000190734863 test loss: 1.0446712970733643 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4470 train loss: 0.924831748008728 train acc: 0.9800000190734863 test loss: 1.0498826503753662 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4471 train loss: 0.9247254133224487 train acc: 0.9800000190734863 test loss: 1.043483018875122 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4472 train loss: 0.9248411655426025 train acc: 0.9800000190734863 test loss: 1.0448431968688965 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4473 train loss: 0.924765408039093 train acc: 0.9800000190734863 test loss: 1.0434623956680298 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4474 train loss: 0.9248008728027344 train acc: 0.9800000190734863 test loss: 1.042912483215332 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4475 train loss: 0.9247885942459106 train acc: 0.9800000190734863 test loss: 1.0424998998641968 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4476 train loss: 0.9247840642929077 train acc: 0.9800000190734863 test loss: 1.0525271892547607 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4477 train loss: 0.9246962070465088 train acc: 0.9800000190734863 test loss: 1.0460577011108398 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4478 train loss: 0.9246878027915955 train acc: 0.9800000190734863 test loss: 1.0460737943649292 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4479 train loss: 0.9247720241546631 train acc: 0.9800000190734863 test loss: 1.0426764488220215 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4480 train loss: 0.9246779680252075 train acc: 0.9800000190734863 test loss: 1.0478124618530273 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4481 train loss: 0.9247376918792725 train acc: 0.9800000190734863 test loss: 1.0418692827224731 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4482 train loss: 0.924750566482544 train acc: 0.9800000190734863 test loss: 1.0393438339233398 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4483 train loss: 0.9247276186943054 train acc: 0.9800000190734863 test loss: 1.0361616611480713 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4484 train loss: 0.9248204231262207 train acc: 0.9800000190734863 test loss: 1.0400716066360474 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4485 train loss: 0.9248132109642029 train acc: 0.9800000190734863 test loss: 1.0443425178527832 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4486 train loss: 0.9248180985450745 train acc: 0.9800000190734863 test loss: 1.044441819190979 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4487 train loss: 0.9247157573699951 train acc: 0.9800000190734863 test loss: 1.0456647872924805 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4488 train loss: 0.9247063994407654 train acc: 0.9800000190734863 test loss: 1.043928623199463 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4489 train loss: 0.9247967600822449 train acc: 0.9800000190734863 test loss: 1.0437092781066895 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4490 train loss: 0.924812376499176 train acc: 0.9800000190734863 test loss: 1.0432151556015015 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4491 train loss: 0.924714982509613 train acc: 0.9800000190734863 test loss: 1.045686960220337 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4492 train loss: 0.9247087240219116 train acc: 0.9800000190734863 test loss: 1.0429010391235352 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4493 train loss: 0.9247930645942688 train acc: 0.9800000190734863 test loss: 1.0485389232635498 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4494 train loss: 0.9247903227806091 train acc: 0.9800000190734863 test loss: 1.0428730249404907 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4495 train loss: 0.9246971607208252 train acc: 0.9800000190734863 test loss: 1.0415362119674683 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4496 train loss: 0.924700140953064 train acc: 0.9800000190734863 test loss: 1.0489758253097534 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4497 train loss: 0.9246771335601807 train acc: 0.9800000190734863 test loss: 1.044812560081482 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4498 train loss: 0.9247443675994873 train acc: 0.9800000190734863 test loss: 1.0419526100158691 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4499 train loss: 0.9246799945831299 train acc: 0.9800000190734863 test loss: 1.0445237159729004 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4500 train loss: 0.9248082041740417 train acc: 0.9800000190734863 test loss: 1.0433522462844849 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4501 train loss: 0.9247994422912598 train acc: 0.9800000190734863 test loss: 1.0449527502059937 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4502 train loss: 0.9246780276298523 train acc: 0.9800000190734863 test loss: 1.0413060188293457 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4503 train loss: 0.9246805310249329 train acc: 0.9800000190734863 test loss: 1.0430550575256348 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4504 train loss: 0.9247900247573853 train acc: 0.9800000190734863 test loss: 1.0440764427185059 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4505 train loss: 0.9246856570243835 train acc: 0.9800000190734863 test loss: 1.0484505891799927 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4506 train loss: 0.9246832132339478 train acc: 0.9800000190734863 test loss: 1.0468319654464722 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4507 train loss: 0.9246823191642761 train acc: 0.9800000190734863 test loss: 1.0439733266830444 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4508 train loss: 0.9247163534164429 train acc: 0.9800000190734863 test loss: 1.0436779260635376 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4509 train loss: 0.924771785736084 train acc: 0.9800000190734863 test loss: 1.0488909482955933 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4510 train loss: 0.9247221350669861 train acc: 0.9800000190734863 test loss: 1.0423667430877686 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4511 train loss: 0.9246856570243835 train acc: 0.9800000190734863 test loss: 1.0436253547668457 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4512 train loss: 0.9247097969055176 train acc: 0.9800000190734863 test loss: 1.0464648008346558 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4513 train loss: 0.9248265027999878 train acc: 0.9800000190734863 test loss: 1.044823169708252 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4514 train loss: 0.9246923327445984 train acc: 0.9800000190734863 test loss: 1.0446702241897583 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4515 train loss: 0.9246938824653625 train acc: 0.9800000190734863 test loss: 1.0454609394073486 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4516 train loss: 0.9247488975524902 train acc: 0.9800000190734863 test loss: 1.0452479124069214 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4517 train loss: 0.9246917963027954 train acc: 0.9800000190734863 test loss: 1.0500432252883911 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4518 train loss: 0.9247831106185913 train acc: 0.9800000190734863 test loss: 1.0452179908752441 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4519 train loss: 0.9246834516525269 train acc: 0.9800000190734863 test loss: 1.0441991090774536 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4520 train loss: 0.9247422218322754 train acc: 0.9800000190734863 test loss: 1.0415306091308594 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4521 train loss: 0.9246867299079895 train acc: 0.9800000190734863 test loss: 1.042833685874939 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4522 train loss: 0.9247947931289673 train acc: 0.9800000190734863 test loss: 1.0459729433059692 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4523 train loss: 0.9247933030128479 train acc: 0.9800000190734863 test loss: 1.0420620441436768 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4524 train loss: 0.9247498512268066 train acc: 0.9800000190734863 test loss: 1.047873854637146 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4525 train loss: 0.9248006939888 train acc: 0.9800000190734863 test loss: 1.044765830039978 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4526 train loss: 0.9247167110443115 train acc: 0.9800000190734863 test loss: 1.0466256141662598 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4527 train loss: 0.9248130321502686 train acc: 0.9800000190734863 test loss: 1.0429152250289917 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4528 train loss: 0.9248044490814209 train acc: 0.9800000190734863 test loss: 1.0404000282287598 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4529 train loss: 0.9247863292694092 train acc: 0.9800000190734863 test loss: 1.0415551662445068 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4530 train loss: 0.9246870279312134 train acc: 0.9800000190734863 test loss: 1.0452402830123901 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4531 train loss: 0.9247185587882996 train acc: 0.9800000190734863 test loss: 1.0426888465881348 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4532 train loss: 0.9247596859931946 train acc: 0.9800000190734863 test loss: 1.0457344055175781 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4533 train loss: 0.9247785806655884 train acc: 0.9800000190734863 test loss: 1.041379690170288 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4534 train loss: 0.9246793389320374 train acc: 0.9800000190734863 test loss: 1.041202425956726 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4535 train loss: 0.9247005581855774 train acc: 0.9800000190734863 test loss: 1.0421288013458252 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4536 train loss: 0.9247990250587463 train acc: 0.9800000190734863 test loss: 1.0419920682907104 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4537 train loss: 0.9247999787330627 train acc: 0.9800000190734863 test loss: 1.0429322719573975 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4538 train loss: 0.9247166514396667 train acc: 0.9800000190734863 test loss: 1.0446425676345825 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4539 train loss: 0.9246820211410522 train acc: 0.9800000190734863 test loss: 1.0427619218826294 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4540 train loss: 0.9247022867202759 train acc: 0.9800000190734863 test loss: 1.0422481298446655 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4541 train loss: 0.9246863722801208 train acc: 0.9800000190734863 test loss: 1.0408527851104736 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4542 train loss: 0.9247567057609558 train acc: 0.9800000190734863 test loss: 1.037513017654419 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4543 train loss: 0.9246824383735657 train acc: 0.9800000190734863 test loss: 1.045079231262207 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4544 train loss: 0.9247007966041565 train acc: 0.9800000190734863 test loss: 1.0471758842468262 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4545 train loss: 0.9246806502342224 train acc: 0.9800000190734863 test loss: 1.046432375907898 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4546 train loss: 0.9246960282325745 train acc: 0.9800000190734863 test loss: 1.0512208938598633 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4547 train loss: 0.9247046709060669 train acc: 0.9800000190734863 test loss: 1.0427038669586182 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4548 train loss: 0.9247572422027588 train acc: 0.9800000190734863 test loss: 1.042863368988037 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4549 train loss: 0.9246772527694702 train acc: 0.9800000190734863 test loss: 1.040561556816101 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4550 train loss: 0.924682080745697 train acc: 0.9800000190734863 test loss: 1.0439316034317017 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4551 train loss: 0.9246870279312134 train acc: 0.9800000190734863 test loss: 1.0443311929702759 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4552 train loss: 0.9247627258300781 train acc: 0.9800000190734863 test loss: 1.0431427955627441 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4553 train loss: 0.9246913194656372 train acc: 0.9800000190734863 test loss: 1.044466495513916 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4554 train loss: 0.924761176109314 train acc: 0.9800000190734863 test loss: 1.045682430267334 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4555 train loss: 0.9247562289237976 train acc: 0.9800000190734863 test loss: 1.0421854257583618 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4556 train loss: 0.924815833568573 train acc: 0.9800000190734863 test loss: 1.0465006828308105 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4557 train loss: 0.9248338937759399 train acc: 0.9800000190734863 test loss: 1.0433827638626099 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4558 train loss: 0.9246765375137329 train acc: 0.9800000190734863 test loss: 1.0429902076721191 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4559 train loss: 0.9247722029685974 train acc: 0.9800000190734863 test loss: 1.0433354377746582 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4560 train loss: 0.9248133897781372 train acc: 0.9800000190734863 test loss: 1.0436269044876099 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4561 train loss: 0.924683690071106 train acc: 0.9800000190734863 test loss: 1.0524652004241943 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4562 train loss: 0.9247997403144836 train acc: 0.9800000190734863 test loss: 1.0442851781845093 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4563 train loss: 0.9247428178787231 train acc: 0.9800000190734863 test loss: 1.0399895906448364 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4564 train loss: 0.9246789813041687 train acc: 0.9800000190734863 test loss: 1.0451102256774902 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4565 train loss: 0.9246798753738403 train acc: 0.9800000190734863 test loss: 1.0447607040405273 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4566 train loss: 0.9248153567314148 train acc: 0.9800000190734863 test loss: 1.0399739742279053 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4567 train loss: 0.9246801137924194 train acc: 0.9800000190734863 test loss: 1.0427794456481934 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4568 train loss: 0.9247093200683594 train acc: 0.9800000190734863 test loss: 1.0409595966339111 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4569 train loss: 0.9247800707817078 train acc: 0.9800000190734863 test loss: 1.0427098274230957 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4570 train loss: 0.9247403740882874 train acc: 0.9800000190734863 test loss: 1.0442099571228027 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4571 train loss: 0.924772322177887 train acc: 0.9800000190734863 test loss: 1.0435402393341064 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4572 train loss: 0.9248058795928955 train acc: 0.9800000190734863 test loss: 1.0451282262802124 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4573 train loss: 0.9247658252716064 train acc: 0.9800000190734863 test loss: 1.0413575172424316 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4574 train loss: 0.9247148036956787 train acc: 0.9800000190734863 test loss: 1.0476490259170532 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4575 train loss: 0.9246796369552612 train acc: 0.9800000190734863 test loss: 1.043531060218811 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4576 train loss: 0.9247401356697083 train acc: 0.9800000190734863 test loss: 1.0448760986328125 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4577 train loss: 0.9246839880943298 train acc: 0.9800000190734863 test loss: 1.0417494773864746 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4578 train loss: 0.924679696559906 train acc: 0.9800000190734863 test loss: 1.0441116094589233 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4579 train loss: 0.9248058795928955 train acc: 0.9800000190734863 test loss: 1.0448664426803589 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4580 train loss: 0.9248304963111877 train acc: 0.9800000190734863 test loss: 1.0432676076889038 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4581 train loss: 0.9247564673423767 train acc: 0.9800000190734863 test loss: 1.0498671531677246 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4582 train loss: 0.9247552752494812 train acc: 0.9800000190734863 test loss: 1.0450584888458252 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4583 train loss: 0.9247265458106995 train acc: 0.9800000190734863 test loss: 1.0445917844772339 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4584 train loss: 0.9247888922691345 train acc: 0.9800000190734863 test loss: 1.0441746711730957 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4585 train loss: 0.9247284531593323 train acc: 0.9800000190734863 test loss: 1.041990041732788 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4586 train loss: 0.9247297644615173 train acc: 0.9800000190734863 test loss: 1.0406936407089233 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4587 train loss: 0.9246916174888611 train acc: 0.9800000190734863 test loss: 1.0454655885696411 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4588 train loss: 0.9246965646743774 train acc: 0.9800000190734863 test loss: 1.0358450412750244 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4589 train loss: 0.9247429370880127 train acc: 0.9800000190734863 test loss: 1.044333815574646 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4590 train loss: 0.9247143268585205 train acc: 0.9800000190734863 test loss: 1.0461982488632202 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4591 train loss: 0.9247013926506042 train acc: 0.9800000190734863 test loss: 1.0444064140319824 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4592 train loss: 0.9247850775718689 train acc: 0.9800000190734863 test loss: 1.0445445775985718 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4593 train loss: 0.9247448444366455 train acc: 0.9800000190734863 test loss: 1.0448074340820312 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4594 train loss: 0.9247688055038452 train acc: 0.9800000190734863 test loss: 1.043470025062561 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4595 train loss: 0.924802839756012 train acc: 0.9800000190734863 test loss: 1.0352044105529785 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4596 train loss: 0.9247850179672241 train acc: 0.9800000190734863 test loss: 1.043663740158081 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4597 train loss: 0.9247649312019348 train acc: 0.9800000190734863 test loss: 1.0450667142868042 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4598 train loss: 0.9247120022773743 train acc: 0.9800000190734863 test loss: 1.0410159826278687 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4599 train loss: 0.9247008562088013 train acc: 0.9800000190734863 test loss: 1.0462584495544434 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4600 train loss: 0.9247462749481201 train acc: 0.9800000190734863 test loss: 1.0424351692199707 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4601 train loss: 0.924827516078949 train acc: 0.9800000190734863 test loss: 1.0436336994171143 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4602 train loss: 0.9246945381164551 train acc: 0.9800000190734863 test loss: 1.0417195558547974 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4603 train loss: 0.9246912598609924 train acc: 0.9800000190734863 test loss: 1.0536737442016602 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4604 train loss: 0.9247611165046692 train acc: 0.9800000190734863 test loss: 1.0399969816207886 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4605 train loss: 0.9247953295707703 train acc: 0.9800000190734863 test loss: 1.0441762208938599 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4606 train loss: 0.924680769443512 train acc: 0.9800000190734863 test loss: 1.04184091091156 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4607 train loss: 0.9247098565101624 train acc: 0.9800000190734863 test loss: 1.043617606163025 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4608 train loss: 0.9247787594795227 train acc: 0.9800000190734863 test loss: 1.0425372123718262 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4609 train loss: 0.9246940016746521 train acc: 0.9800000190734863 test loss: 1.0450056791305542 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4610 train loss: 0.9247629642486572 train acc: 0.9800000190734863 test loss: 1.0396028757095337 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4611 train loss: 0.9247013330459595 train acc: 0.9800000190734863 test loss: 1.0499656200408936 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4612 train loss: 0.9246862530708313 train acc: 0.9800000190734863 test loss: 1.0414642095565796 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4613 train loss: 0.9246790409088135 train acc: 0.9800000190734863 test loss: 1.0417381525039673 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4614 train loss: 0.9247139692306519 train acc: 0.9800000190734863 test loss: 1.0441144704818726 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4615 train loss: 0.9247106909751892 train acc: 0.9800000190734863 test loss: 1.0439422130584717 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4616 train loss: 0.9248415231704712 train acc: 0.9800000190734863 test loss: 1.0440975427627563 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4617 train loss: 0.9247119426727295 train acc: 0.9800000190734863 test loss: 1.038131833076477 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4618 train loss: 0.9246784448623657 train acc: 0.9800000190734863 test loss: 1.0500898361206055 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4619 train loss: 0.9247234463691711 train acc: 0.9800000190734863 test loss: 1.0389022827148438 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4620 train loss: 0.9247837662696838 train acc: 0.9800000190734863 test loss: 1.0436595678329468 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4621 train loss: 0.9247027635574341 train acc: 0.9800000190734863 test loss: 1.0421319007873535 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4622 train loss: 0.9247840046882629 train acc: 0.9800000190734863 test loss: 1.0443055629730225 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4623 train loss: 0.9246855974197388 train acc: 0.9800000190734863 test loss: 1.0438957214355469 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4624 train loss: 0.9247368574142456 train acc: 0.9800000190734863 test loss: 1.052189826965332 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4625 train loss: 0.924685537815094 train acc: 0.9800000190734863 test loss: 1.0474977493286133 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4626 train loss: 0.9247719049453735 train acc: 0.9800000190734863 test loss: 1.0491418838500977 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4627 train loss: 0.9246814846992493 train acc: 0.9800000190734863 test loss: 1.034030556678772 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4628 train loss: 0.9247047901153564 train acc: 0.9800000190734863 test loss: 1.0400723218917847 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4629 train loss: 0.92470383644104 train acc: 0.9800000190734863 test loss: 1.0511058568954468 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4630 train loss: 0.9248077273368835 train acc: 0.9800000190734863 test loss: 1.0405209064483643 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4631 train loss: 0.924691915512085 train acc: 0.9800000190734863 test loss: 1.0459734201431274 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4632 train loss: 0.9248242974281311 train acc: 0.9800000190734863 test loss: 1.0422062873840332 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4633 train loss: 0.9248104691505432 train acc: 0.9800000190734863 test loss: 1.043389081954956 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4634 train loss: 0.9247224926948547 train acc: 0.9800000190734863 test loss: 1.0419642925262451 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4635 train loss: 0.9247611165046692 train acc: 0.9800000190734863 test loss: 1.0438942909240723 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4636 train loss: 0.9248002767562866 train acc: 0.9800000190734863 test loss: 1.0469850301742554 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4637 train loss: 0.9247265458106995 train acc: 0.9800000190734863 test loss: 1.045620322227478 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4638 train loss: 0.9246856570243835 train acc: 0.9800000190734863 test loss: 1.0448710918426514 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4639 train loss: 0.924679160118103 train acc: 0.9800000190734863 test loss: 1.0414342880249023 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4640 train loss: 0.9247855544090271 train acc: 0.9800000190734863 test loss: 1.0440806150436401 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4641 train loss: 0.9246880412101746 train acc: 0.9800000190734863 test loss: 1.044845461845398 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4642 train loss: 0.9248313903808594 train acc: 0.9800000190734863 test loss: 1.0426478385925293 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4643 train loss: 0.9246767163276672 train acc: 0.9800000190734863 test loss: 1.044260859489441 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4644 train loss: 0.9247369170188904 train acc: 0.9800000190734863 test loss: 1.0393661260604858 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4645 train loss: 0.9247603416442871 train acc: 0.9800000190734863 test loss: 1.048953652381897 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4646 train loss: 0.9248046875 train acc: 0.9800000190734863 test loss: 1.0430314540863037 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4647 train loss: 0.9247155785560608 train acc: 0.9800000190734863 test loss: 1.0438697338104248 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4648 train loss: 0.9248267412185669 train acc: 0.9800000190734863 test loss: 1.0423293113708496 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4649 train loss: 0.9246981143951416 train acc: 0.9800000190734863 test loss: 1.0492444038391113 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4650 train loss: 0.9246968626976013 train acc: 0.9800000190734863 test loss: 1.0440471172332764 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4651 train loss: 0.9247162342071533 train acc: 0.9800000190734863 test loss: 1.041345238685608 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4652 train loss: 0.924782395362854 train acc: 0.9800000190734863 test loss: 1.0432965755462646 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4653 train loss: 0.9247431755065918 train acc: 0.9800000190734863 test loss: 1.046144962310791 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4654 train loss: 0.9248203039169312 train acc: 0.9800000190734863 test loss: 1.0396193265914917 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4655 train loss: 0.9246962666511536 train acc: 0.9800000190734863 test loss: 1.0428024530410767 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4656 train loss: 0.9247847199440002 train acc: 0.9800000190734863 test loss: 1.0452920198440552 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4657 train loss: 0.9247277975082397 train acc: 0.9800000190734863 test loss: 1.0439796447753906 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4658 train loss: 0.9246944189071655 train acc: 0.9800000190734863 test loss: 1.0433980226516724 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4659 train loss: 0.9246990084648132 train acc: 0.9800000190734863 test loss: 1.0438294410705566 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4660 train loss: 0.9246962070465088 train acc: 0.9800000190734863 test loss: 1.0437287092208862 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4661 train loss: 0.9247133731842041 train acc: 0.9800000190734863 test loss: 1.0443363189697266 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4662 train loss: 0.9247692823410034 train acc: 0.9800000190734863 test loss: 1.0418215990066528 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4663 train loss: 0.9247174859046936 train acc: 0.9800000190734863 test loss: 1.0430536270141602 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4664 train loss: 0.9247276186943054 train acc: 0.9800000190734863 test loss: 1.0423400402069092 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4665 train loss: 0.9246776700019836 train acc: 0.9800000190734863 test loss: 1.0424401760101318 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4666 train loss: 0.9247585535049438 train acc: 0.9800000190734863 test loss: 1.038323163986206 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4667 train loss: 0.9247667789459229 train acc: 0.9800000190734863 test loss: 1.0430576801300049 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4668 train loss: 0.9247267246246338 train acc: 0.9800000190734863 test loss: 1.0439311265945435 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4669 train loss: 0.9247398376464844 train acc: 0.9800000190734863 test loss: 1.0437793731689453 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4670 train loss: 0.9247365593910217 train acc: 0.9800000190734863 test loss: 1.0449165105819702 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4671 train loss: 0.9247428774833679 train acc: 0.9800000190734863 test loss: 1.0417548418045044 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4672 train loss: 0.9247205853462219 train acc: 0.9800000190734863 test loss: 1.0457093715667725 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4673 train loss: 0.9246870279312134 train acc: 0.9800000190734863 test loss: 1.0421980619430542 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4674 train loss: 0.924689769744873 train acc: 0.9800000190734863 test loss: 1.0416604280471802 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4675 train loss: 0.9248092174530029 train acc: 0.9800000190734863 test loss: 1.0364876985549927 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4676 train loss: 0.9247692823410034 train acc: 0.9800000190734863 test loss: 1.0421279668807983 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4677 train loss: 0.924744188785553 train acc: 0.9800000190734863 test loss: 1.0486247539520264 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4678 train loss: 0.9247612953186035 train acc: 0.9800000190734863 test loss: 1.0442029237747192 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4679 train loss: 0.9247333407402039 train acc: 0.9800000190734863 test loss: 1.0453510284423828 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4680 train loss: 0.9247180819511414 train acc: 0.9800000190734863 test loss: 1.0432934761047363 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4681 train loss: 0.9247360229492188 train acc: 0.9800000190734863 test loss: 1.043199896812439 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4682 train loss: 0.9247872233390808 train acc: 0.9800000190734863 test loss: 1.0406490564346313 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4683 train loss: 0.9247324466705322 train acc: 0.9800000190734863 test loss: 1.0440664291381836 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4684 train loss: 0.9246922135353088 train acc: 0.9800000190734863 test loss: 1.047869086265564 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4685 train loss: 0.9247021675109863 train acc: 0.9800000190734863 test loss: 1.0437461137771606 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4686 train loss: 0.9247807860374451 train acc: 0.9800000190734863 test loss: 1.0421146154403687 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4687 train loss: 0.9246890544891357 train acc: 0.9800000190734863 test loss: 1.049252986907959 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4688 train loss: 0.9246761798858643 train acc: 0.9800000190734863 test loss: 1.0458661317825317 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4689 train loss: 0.9246973991394043 train acc: 0.9800000190734863 test loss: 1.0452945232391357 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4690 train loss: 0.9247260093688965 train acc: 0.9800000190734863 test loss: 1.0445737838745117 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4691 train loss: 0.9247338771820068 train acc: 0.9800000190734863 test loss: 1.038597822189331 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4692 train loss: 0.9247350096702576 train acc: 0.9800000190734863 test loss: 1.048973560333252 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4693 train loss: 0.9246752262115479 train acc: 0.9800000190734863 test loss: 1.0441975593566895 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4694 train loss: 0.9248014688491821 train acc: 0.9800000190734863 test loss: 1.0501189231872559 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4695 train loss: 0.9246799945831299 train acc: 0.9800000190734863 test loss: 1.042360782623291 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4696 train loss: 0.924801766872406 train acc: 0.9800000190734863 test loss: 1.0464446544647217 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4697 train loss: 0.9247486591339111 train acc: 0.9800000190734863 test loss: 1.0424776077270508 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4698 train loss: 0.9246798157691956 train acc: 0.9800000190734863 test loss: 1.043375015258789 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4699 train loss: 0.9247450232505798 train acc: 0.9800000190734863 test loss: 1.0364511013031006 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4700 train loss: 0.9247345924377441 train acc: 0.9800000190734863 test loss: 1.0427420139312744 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4701 train loss: 0.9248054623603821 train acc: 0.9800000190734863 test loss: 1.0468776226043701 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4702 train loss: 0.9247641563415527 train acc: 0.9800000190734863 test loss: 1.044245958328247 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4703 train loss: 0.9247356653213501 train acc: 0.9800000190734863 test loss: 1.0415077209472656 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4704 train loss: 0.9246779084205627 train acc: 0.9800000190734863 test loss: 1.0454424619674683 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4705 train loss: 0.9248126149177551 train acc: 0.9800000190734863 test loss: 1.0418007373809814 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4706 train loss: 0.9246814250946045 train acc: 0.9800000190734863 test loss: 1.0444128513336182 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4707 train loss: 0.9246838092803955 train acc: 0.9800000190734863 test loss: 1.0471410751342773 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4708 train loss: 0.924792468547821 train acc: 0.9800000190734863 test loss: 1.04780912399292 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4709 train loss: 0.9246896505355835 train acc: 0.9800000190734863 test loss: 1.047575831413269 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4710 train loss: 0.9247521758079529 train acc: 0.9800000190734863 test loss: 1.0497008562088013 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4711 train loss: 0.924724280834198 train acc: 0.9800000190734863 test loss: 1.0451292991638184 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4712 train loss: 0.9247310161590576 train acc: 0.9800000190734863 test loss: 1.049696922302246 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4713 train loss: 0.9246774911880493 train acc: 0.9800000190734863 test loss: 1.0434881448745728 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4714 train loss: 0.9248124957084656 train acc: 0.9800000190734863 test loss: 1.0453957319259644 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4715 train loss: 0.9247316718101501 train acc: 0.9800000190734863 test loss: 1.0447279214859009 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4716 train loss: 0.9246758818626404 train acc: 0.9800000190734863 test loss: 1.052636981010437 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4717 train loss: 0.9246874451637268 train acc: 0.9800000190734863 test loss: 1.045100212097168 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4718 train loss: 0.9248207211494446 train acc: 0.9800000190734863 test loss: 1.0416557788848877 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4719 train loss: 0.9247277975082397 train acc: 0.9800000190734863 test loss: 1.0416938066482544 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4720 train loss: 0.9247599244117737 train acc: 0.9800000190734863 test loss: 1.0390511751174927 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4721 train loss: 0.9247133731842041 train acc: 0.9800000190734863 test loss: 1.0478054285049438 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4722 train loss: 0.9247015118598938 train acc: 0.9800000190734863 test loss: 1.0444871187210083 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4723 train loss: 0.9247704148292542 train acc: 0.9800000190734863 test loss: 1.0456233024597168 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4724 train loss: 0.9247788786888123 train acc: 0.9800000190734863 test loss: 1.0479375123977661 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4725 train loss: 0.9247931838035583 train acc: 0.9800000190734863 test loss: 1.0432355403900146 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4726 train loss: 0.9247009754180908 train acc: 0.9800000190734863 test loss: 1.0471668243408203 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4727 train loss: 0.9247798919677734 train acc: 0.9800000190734863 test loss: 1.0392470359802246 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4728 train loss: 0.9246776103973389 train acc: 0.9800000190734863 test loss: 1.0428836345672607 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4729 train loss: 0.9247861504554749 train acc: 0.9800000190734863 test loss: 1.037165641784668 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4730 train loss: 0.9248179793357849 train acc: 0.9800000190734863 test loss: 1.0439655780792236 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4731 train loss: 0.9247162342071533 train acc: 0.9800000190734863 test loss: 1.0472731590270996 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4732 train loss: 0.9248027801513672 train acc: 0.9800000190734863 test loss: 1.0442478656768799 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4733 train loss: 0.9246871471405029 train acc: 0.9800000190734863 test loss: 1.0440926551818848 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4734 train loss: 0.9246886968612671 train acc: 0.9800000190734863 test loss: 1.0501328706741333 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4735 train loss: 0.9247003197669983 train acc: 0.9800000190734863 test loss: 1.0431712865829468 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4736 train loss: 0.9248113036155701 train acc: 0.9800000190734863 test loss: 1.0438653230667114 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4737 train loss: 0.9247177243232727 train acc: 0.9800000190734863 test loss: 1.0425246953964233 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4738 train loss: 0.9246841669082642 train acc: 0.9800000190734863 test loss: 1.0418721437454224 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4739 train loss: 0.9247625470161438 train acc: 0.9800000190734863 test loss: 1.0439165830612183 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4740 train loss: 0.9246907830238342 train acc: 0.9800000190734863 test loss: 1.0434231758117676 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4741 train loss: 0.9248260259628296 train acc: 0.9800000190734863 test loss: 1.0452686548233032 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4742 train loss: 0.9247852563858032 train acc: 0.9800000190734863 test loss: 1.0385562181472778 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4743 train loss: 0.9247462749481201 train acc: 0.9800000190734863 test loss: 1.0481067895889282 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4744 train loss: 0.9246824383735657 train acc: 0.9800000190734863 test loss: 1.0424615144729614 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4745 train loss: 0.9246975779533386 train acc: 0.9800000190734863 test loss: 1.0458290576934814 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4746 train loss: 0.9247030019760132 train acc: 0.9800000190734863 test loss: 1.0402369499206543 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4747 train loss: 0.924681544303894 train acc: 0.9800000190734863 test loss: 1.0428268909454346 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4748 train loss: 0.9246833920478821 train acc: 0.9800000190734863 test loss: 1.045394778251648 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4749 train loss: 0.924741804599762 train acc: 0.9800000190734863 test loss: 1.0483406782150269 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4750 train loss: 0.9247119426727295 train acc: 0.9800000190734863 test loss: 1.052945613861084 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4751 train loss: 0.9246758818626404 train acc: 0.9800000190734863 test loss: 1.041822075843811 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4752 train loss: 0.9247809648513794 train acc: 0.9800000190734863 test loss: 1.037764072418213 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4753 train loss: 0.9247915744781494 train acc: 0.9800000190734863 test loss: 1.048194169998169 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4754 train loss: 0.9247174263000488 train acc: 0.9800000190734863 test loss: 1.0436923503875732 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4755 train loss: 0.9247241020202637 train acc: 0.9800000190734863 test loss: 1.0400142669677734 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4756 train loss: 0.9246801137924194 train acc: 0.9800000190734863 test loss: 1.043246865272522 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4757 train loss: 0.9246924519538879 train acc: 0.9800000190734863 test loss: 1.0446566343307495 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4758 train loss: 0.92473965883255 train acc: 0.9800000190734863 test loss: 1.0418734550476074 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4759 train loss: 0.9246944189071655 train acc: 0.9800000190734863 test loss: 1.0410006046295166 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4760 train loss: 0.9246917963027954 train acc: 0.9800000190734863 test loss: 1.04422926902771 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4761 train loss: 0.9247044324874878 train acc: 0.9800000190734863 test loss: 1.0444846153259277 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4762 train loss: 0.924723207950592 train acc: 0.9800000190734863 test loss: 1.044211745262146 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4763 train loss: 0.9246859550476074 train acc: 0.9800000190734863 test loss: 1.0424009561538696 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4764 train loss: 0.9246783256530762 train acc: 0.9800000190734863 test loss: 1.0502803325653076 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4765 train loss: 0.9247122406959534 train acc: 0.9800000190734863 test loss: 1.0543532371520996 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4766 train loss: 0.9247077703475952 train acc: 0.9800000190734863 test loss: 1.048180103302002 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4767 train loss: 0.9247757792472839 train acc: 0.9800000190734863 test loss: 1.0504282712936401 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4768 train loss: 0.9246787428855896 train acc: 0.9800000190734863 test loss: 1.0439362525939941 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4769 train loss: 0.924731433391571 train acc: 0.9800000190734863 test loss: 1.0390782356262207 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4770 train loss: 0.9248024821281433 train acc: 0.9800000190734863 test loss: 1.0444306135177612 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4771 train loss: 0.9246783256530762 train acc: 0.9800000190734863 test loss: 1.043255090713501 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4772 train loss: 0.9248109459877014 train acc: 0.9800000190734863 test loss: 1.0381989479064941 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4773 train loss: 0.9247410297393799 train acc: 0.9800000190734863 test loss: 1.0436296463012695 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4774 train loss: 0.9246863722801208 train acc: 0.9800000190734863 test loss: 1.0510005950927734 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4775 train loss: 0.9248104095458984 train acc: 0.9800000190734863 test loss: 1.043758511543274 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4776 train loss: 0.924674391746521 train acc: 0.9800000190734863 test loss: 1.0534971952438354 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4777 train loss: 0.9247038960456848 train acc: 0.9800000190734863 test loss: 1.0430909395217896 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4778 train loss: 0.9246833324432373 train acc: 0.9800000190734863 test loss: 1.046757459640503 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4779 train loss: 0.9246861338615417 train acc: 0.9800000190734863 test loss: 1.0503979921340942 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4780 train loss: 0.9246906042098999 train acc: 0.9800000190734863 test loss: 1.0415476560592651 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4781 train loss: 0.9248120188713074 train acc: 0.9800000190734863 test loss: 1.0417966842651367 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4782 train loss: 0.9246925115585327 train acc: 0.9800000190734863 test loss: 1.041858434677124 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4783 train loss: 0.9246739149093628 train acc: 0.9800000190734863 test loss: 1.03892183303833 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4784 train loss: 0.9246804714202881 train acc: 0.9800000190734863 test loss: 1.0415408611297607 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4785 train loss: 0.9246790409088135 train acc: 0.9800000190734863 test loss: 1.0504319667816162 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4786 train loss: 0.9246751666069031 train acc: 0.9800000190734863 test loss: 1.042083740234375 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4787 train loss: 0.9248185157775879 train acc: 0.9800000190734863 test loss: 1.0413898229599 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4788 train loss: 0.9246829748153687 train acc: 0.9800000190734863 test loss: 1.042708158493042 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4789 train loss: 0.9247924089431763 train acc: 0.9800000190734863 test loss: 1.0448373556137085 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4790 train loss: 0.9248113036155701 train acc: 0.9800000190734863 test loss: 1.047438621520996 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4791 train loss: 0.9246818423271179 train acc: 0.9800000190734863 test loss: 1.0469582080841064 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4792 train loss: 0.9246885180473328 train acc: 0.9800000190734863 test loss: 1.0470001697540283 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4793 train loss: 0.9247077703475952 train acc: 0.9800000190734863 test loss: 1.0428156852722168 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4794 train loss: 0.9247277975082397 train acc: 0.9800000190734863 test loss: 1.0458019971847534 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4795 train loss: 0.9247031211853027 train acc: 0.9800000190734863 test loss: 1.035285472869873 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4796 train loss: 0.924696147441864 train acc: 0.9800000190734863 test loss: 1.0378135442733765 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4797 train loss: 0.9246787428855896 train acc: 0.9800000190734863 test loss: 1.0438510179519653 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4798 train loss: 0.924790620803833 train acc: 0.9800000190734863 test loss: 1.0431827306747437 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4799 train loss: 0.9247989058494568 train acc: 0.9800000190734863 test loss: 1.045732021331787 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4800 train loss: 0.9246851205825806 train acc: 0.9800000190734863 test loss: 1.03932523727417 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4801 train loss: 0.9246780872344971 train acc: 0.9800000190734863 test loss: 1.0427618026733398 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4802 train loss: 0.9246923923492432 train acc: 0.9800000190734863 test loss: 1.0423907041549683 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4803 train loss: 0.9247602820396423 train acc: 0.9800000190734863 test loss: 1.04343581199646 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4804 train loss: 0.9246982336044312 train acc: 0.9800000190734863 test loss: 1.0335094928741455 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4805 train loss: 0.9246779084205627 train acc: 0.9800000190734863 test loss: 1.0424363613128662 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4806 train loss: 0.9247375726699829 train acc: 0.9800000190734863 test loss: 1.0445523262023926 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4807 train loss: 0.9246786236763 train acc: 0.9800000190734863 test loss: 1.0485645532608032 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4808 train loss: 0.9246949553489685 train acc: 0.9800000190734863 test loss: 1.041744351387024 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4809 train loss: 0.9247134923934937 train acc: 0.9800000190734863 test loss: 1.0448681116104126 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4810 train loss: 0.9246819019317627 train acc: 0.9800000190734863 test loss: 1.0449473857879639 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4811 train loss: 0.9247187972068787 train acc: 0.9800000190734863 test loss: 1.047729253768921 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4812 train loss: 0.9247649908065796 train acc: 0.9800000190734863 test loss: 1.0379576683044434 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4813 train loss: 0.9247472882270813 train acc: 0.9800000190734863 test loss: 1.046593427658081 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4814 train loss: 0.9247734546661377 train acc: 0.9800000190734863 test loss: 1.046348214149475 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4815 train loss: 0.9247336387634277 train acc: 0.9800000190734863 test loss: 1.0436620712280273 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4816 train loss: 0.9248145818710327 train acc: 0.9800000190734863 test loss: 1.0431121587753296 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4817 train loss: 0.9246804118156433 train acc: 0.9800000190734863 test loss: 1.0453325510025024 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4818 train loss: 0.9248207807540894 train acc: 0.9800000190734863 test loss: 1.040034294128418 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4819 train loss: 0.9247599840164185 train acc: 0.9800000190734863 test loss: 1.0403149127960205 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4820 train loss: 0.9246981739997864 train acc: 0.9800000190734863 test loss: 1.0417181253433228 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4821 train loss: 0.9247275590896606 train acc: 0.9800000190734863 test loss: 1.046487808227539 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4822 train loss: 0.924680233001709 train acc: 0.9800000190734863 test loss: 1.041663408279419 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4823 train loss: 0.9247523546218872 train acc: 0.9800000190734863 test loss: 1.0437451601028442 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4824 train loss: 0.9247270822525024 train acc: 0.9800000190734863 test loss: 1.0421866178512573 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4825 train loss: 0.9247379899024963 train acc: 0.9800000190734863 test loss: 1.04427170753479 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4826 train loss: 0.9247173070907593 train acc: 0.9800000190734863 test loss: 1.0432430505752563 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4827 train loss: 0.9246912598609924 train acc: 0.9800000190734863 test loss: 1.038727879524231 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4828 train loss: 0.9247472882270813 train acc: 0.9800000190734863 test loss: 1.0412477254867554 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4829 train loss: 0.9246768355369568 train acc: 0.9800000190734863 test loss: 1.050869107246399 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4830 train loss: 0.9247075915336609 train acc: 0.9800000190734863 test loss: 1.0448249578475952 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4831 train loss: 0.9247299432754517 train acc: 0.9800000190734863 test loss: 1.0436205863952637 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4832 train loss: 0.9246812462806702 train acc: 0.9800000190734863 test loss: 1.0429866313934326 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4833 train loss: 0.9246951341629028 train acc: 0.9800000190734863 test loss: 1.037651538848877 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4834 train loss: 0.9247313737869263 train acc: 0.9800000190734863 test loss: 1.0397708415985107 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4835 train loss: 0.9248020648956299 train acc: 0.9800000190734863 test loss: 1.0415667295455933 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4836 train loss: 0.9247469902038574 train acc: 0.9800000190734863 test loss: 1.0430105924606323 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4837 train loss: 0.9248210787773132 train acc: 0.9800000190734863 test loss: 1.0426819324493408 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4838 train loss: 0.9246806502342224 train acc: 0.9800000190734863 test loss: 1.0480791330337524 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4839 train loss: 0.9246790409088135 train acc: 0.9800000190734863 test loss: 1.0434561967849731 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4840 train loss: 0.9246758818626404 train acc: 0.9800000190734863 test loss: 1.0372730493545532 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4841 train loss: 0.9247974157333374 train acc: 0.9800000190734863 test loss: 1.0424731969833374 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4842 train loss: 0.9247270226478577 train acc: 0.9800000190734863 test loss: 1.0422337055206299 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4843 train loss: 0.924679160118103 train acc: 0.9800000190734863 test loss: 1.0375112295150757 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4844 train loss: 0.924710750579834 train acc: 0.9800000190734863 test loss: 1.042510986328125 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4845 train loss: 0.9247229695320129 train acc: 0.9800000190734863 test loss: 1.0613374710083008 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4846 train loss: 0.9247593879699707 train acc: 0.9800000190734863 test loss: 1.039797306060791 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4847 train loss: 0.924691915512085 train acc: 0.9800000190734863 test loss: 1.0425273180007935 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4848 train loss: 0.9247021675109863 train acc: 0.9800000190734863 test loss: 1.048012375831604 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4849 train loss: 0.9247521758079529 train acc: 0.9800000190734863 test loss: 1.0404903888702393 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4850 train loss: 0.9248137474060059 train acc: 0.9800000190734863 test loss: 1.0423715114593506 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4851 train loss: 0.9246773719787598 train acc: 0.9800000190734863 test loss: 1.0431299209594727 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4852 train loss: 0.9247769117355347 train acc: 0.9800000190734863 test loss: 1.0450036525726318 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4853 train loss: 0.9246861934661865 train acc: 0.9800000190734863 test loss: 1.0405598878860474 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4854 train loss: 0.9247032999992371 train acc: 0.9800000190734863 test loss: 1.0422203540802002 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4855 train loss: 0.9246845841407776 train acc: 0.9800000190734863 test loss: 1.0402439832687378 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4856 train loss: 0.9246981739997864 train acc: 0.9800000190734863 test loss: 1.042443037033081 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4857 train loss: 0.9248088598251343 train acc: 0.9800000190734863 test loss: 1.0447970628738403 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4858 train loss: 0.9247757196426392 train acc: 0.9800000190734863 test loss: 1.0376659631729126 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4859 train loss: 0.924771249294281 train acc: 0.9800000190734863 test loss: 1.0343017578125 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4860 train loss: 0.9248006939888 train acc: 0.9800000190734863 test loss: 1.0536478757858276 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4861 train loss: 0.9247480034828186 train acc: 0.9800000190734863 test loss: 1.0444413423538208 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4862 train loss: 0.9248360991477966 train acc: 0.9800000190734863 test loss: 1.0469982624053955 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4863 train loss: 0.9246793985366821 train acc: 0.9800000190734863 test loss: 1.0394980907440186 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4864 train loss: 0.9247806072235107 train acc: 0.9800000190734863 test loss: 1.0440897941589355 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4865 train loss: 0.9247184991836548 train acc: 0.9800000190734863 test loss: 1.0444108247756958 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4866 train loss: 0.9247308373451233 train acc: 0.9800000190734863 test loss: 1.0406670570373535 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4867 train loss: 0.9248156547546387 train acc: 0.9800000190734863 test loss: 1.0427933931350708 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4868 train loss: 0.9247444868087769 train acc: 0.9800000190734863 test loss: 1.0441012382507324 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4869 train loss: 0.9247178435325623 train acc: 0.9800000190734863 test loss: 1.0549274682998657 best test loss: 1.0130972862243652 test acc: 0.8399999737739563\n",
      "Epoch 4870 train loss: 0.924682080745697 train acc: 0.9800000190734863 test loss: 1.042466163635254 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4871 train loss: 0.924775242805481 train acc: 0.9800000190734863 test loss: 1.0500211715698242 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4872 train loss: 0.9246916174888611 train acc: 0.9800000190734863 test loss: 1.0461080074310303 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4873 train loss: 0.9248019456863403 train acc: 0.9800000190734863 test loss: 1.0507928133010864 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4874 train loss: 0.9247632026672363 train acc: 0.9800000190734863 test loss: 1.0461214780807495 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4875 train loss: 0.9246739745140076 train acc: 0.9800000190734863 test loss: 1.044643521308899 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4876 train loss: 0.9248201847076416 train acc: 0.9800000190734863 test loss: 1.040398120880127 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4877 train loss: 0.9246907234191895 train acc: 0.9800000190734863 test loss: 1.0422637462615967 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4878 train loss: 0.9246925115585327 train acc: 0.9800000190734863 test loss: 1.0440593957901 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4879 train loss: 0.9247722029685974 train acc: 0.9800000190734863 test loss: 1.044541597366333 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4880 train loss: 0.9248148202896118 train acc: 0.9800000190734863 test loss: 1.0441116094589233 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4881 train loss: 0.9247753024101257 train acc: 0.9800000190734863 test loss: 1.043584942817688 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4882 train loss: 0.9247404336929321 train acc: 0.9800000190734863 test loss: 1.0432456731796265 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4883 train loss: 0.9247432947158813 train acc: 0.9800000190734863 test loss: 1.0425379276275635 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4884 train loss: 0.9247036576271057 train acc: 0.9800000190734863 test loss: 1.0462124347686768 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4885 train loss: 0.924699068069458 train acc: 0.9800000190734863 test loss: 1.0430474281311035 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4886 train loss: 0.9247197508811951 train acc: 0.9800000190734863 test loss: 1.0472534894943237 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4887 train loss: 0.9246793985366821 train acc: 0.9800000190734863 test loss: 1.0464301109313965 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4888 train loss: 0.9247554540634155 train acc: 0.9800000190734863 test loss: 1.0469030141830444 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4889 train loss: 0.9246888756752014 train acc: 0.9800000190734863 test loss: 1.0416765213012695 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4890 train loss: 0.9246785044670105 train acc: 0.9800000190734863 test loss: 1.0409268140792847 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4891 train loss: 0.9247695803642273 train acc: 0.9800000190734863 test loss: 1.044274926185608 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4892 train loss: 0.9248026013374329 train acc: 0.9800000190734863 test loss: 1.0426331758499146 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4893 train loss: 0.924775242805481 train acc: 0.9800000190734863 test loss: 1.0528101921081543 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4894 train loss: 0.9247319102287292 train acc: 0.9800000190734863 test loss: 1.0464831590652466 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4895 train loss: 0.9247604608535767 train acc: 0.9800000190734863 test loss: 1.0418944358825684 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4896 train loss: 0.9246785044670105 train acc: 0.9800000190734863 test loss: 1.0472580194473267 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4897 train loss: 0.9246907234191895 train acc: 0.9800000190734863 test loss: 1.0430574417114258 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4898 train loss: 0.9246766567230225 train acc: 0.9800000190734863 test loss: 1.0442124605178833 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4899 train loss: 0.9247900247573853 train acc: 0.9800000190734863 test loss: 1.0414470434188843 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4900 train loss: 0.9248126149177551 train acc: 0.9800000190734863 test loss: 1.0433306694030762 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4901 train loss: 0.9248147010803223 train acc: 0.9800000190734863 test loss: 1.0401707887649536 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4902 train loss: 0.924689531326294 train acc: 0.9800000190734863 test loss: 1.042184591293335 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4903 train loss: 0.9247836470603943 train acc: 0.9800000190734863 test loss: 1.0421452522277832 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4904 train loss: 0.9247069358825684 train acc: 0.9800000190734863 test loss: 1.0442593097686768 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4905 train loss: 0.9247308969497681 train acc: 0.9800000190734863 test loss: 1.04203462600708 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4906 train loss: 0.9247502088546753 train acc: 0.9800000190734863 test loss: 1.0466638803482056 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4907 train loss: 0.924668550491333 train acc: 0.9800000190734863 test loss: 1.0431076288223267 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4908 train loss: 0.9246588945388794 train acc: 0.9800000190734863 test loss: 1.04253351688385 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4909 train loss: 0.9246854186058044 train acc: 0.9800000190734863 test loss: 1.043237566947937 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4910 train loss: 0.9247047901153564 train acc: 0.9800000190734863 test loss: 1.0440232753753662 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4911 train loss: 0.9247303605079651 train acc: 0.9800000190734863 test loss: 1.036975383758545 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4912 train loss: 0.9247746467590332 train acc: 0.9800000190734863 test loss: 1.0448170900344849 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4913 train loss: 0.9247074127197266 train acc: 0.9800000190734863 test loss: 1.0428364276885986 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4914 train loss: 0.9246971011161804 train acc: 0.9800000190734863 test loss: 1.0420063734054565 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4915 train loss: 0.9247096180915833 train acc: 0.9800000190734863 test loss: 1.0446573495864868 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4916 train loss: 0.9246827960014343 train acc: 0.9800000190734863 test loss: 1.0379165410995483 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4917 train loss: 0.9247106909751892 train acc: 0.9800000190734863 test loss: 1.044942855834961 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4918 train loss: 0.9247294664382935 train acc: 0.9800000190734863 test loss: 1.0515729188919067 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4919 train loss: 0.9246842861175537 train acc: 0.9800000190734863 test loss: 1.0482776165008545 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4920 train loss: 0.9242827892303467 train acc: 0.9800000190734863 test loss: 1.045546293258667 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4921 train loss: 0.9242208003997803 train acc: 0.9800000190734863 test loss: 1.0416805744171143 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4922 train loss: 0.9224046468734741 train acc: 0.9825000166893005 test loss: 1.043269395828247 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4923 train loss: 0.9226773977279663 train acc: 0.9825000166893005 test loss: 1.0317015647888184 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4924 train loss: 0.9223106503486633 train acc: 0.9825000166893005 test loss: 1.0412179231643677 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4925 train loss: 0.9222042560577393 train acc: 0.9825000166893005 test loss: 1.0463050603866577 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4926 train loss: 0.92228102684021 train acc: 0.9825000166893005 test loss: 1.0418773889541626 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4927 train loss: 0.9223509430885315 train acc: 0.9825000166893005 test loss: 1.0368458032608032 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4928 train loss: 0.9222273826599121 train acc: 0.9825000166893005 test loss: 1.0453388690948486 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4929 train loss: 0.9222711324691772 train acc: 0.9825000166893005 test loss: 1.042148232460022 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4930 train loss: 0.9222303032875061 train acc: 0.9825000166893005 test loss: 1.0310264825820923 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4931 train loss: 0.9221951961517334 train acc: 0.9825000166893005 test loss: 1.0388460159301758 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4932 train loss: 0.9221948385238647 train acc: 0.9825000166893005 test loss: 1.0384289026260376 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4933 train loss: 0.9222543239593506 train acc: 0.9825000166893005 test loss: 1.0414416790008545 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4934 train loss: 0.9221840500831604 train acc: 0.9825000166893005 test loss: 1.0444828271865845 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4935 train loss: 0.9221815466880798 train acc: 0.9825000166893005 test loss: 1.0417873859405518 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4936 train loss: 0.9222954511642456 train acc: 0.9825000166893005 test loss: 1.044249415397644 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4937 train loss: 0.9221904277801514 train acc: 0.9825000166893005 test loss: 1.0383801460266113 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4938 train loss: 0.9223174452781677 train acc: 0.9825000166893005 test loss: 1.0355128049850464 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4939 train loss: 0.9222576022148132 train acc: 0.9825000166893005 test loss: 1.0349684953689575 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4940 train loss: 0.9221910834312439 train acc: 0.9825000166893005 test loss: 1.0343879461288452 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4941 train loss: 0.9222387075424194 train acc: 0.9825000166893005 test loss: 1.042629361152649 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4942 train loss: 0.9222280979156494 train acc: 0.9825000166893005 test loss: 1.042655348777771 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4943 train loss: 0.9222201704978943 train acc: 0.9825000166893005 test loss: 1.0377964973449707 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4944 train loss: 0.9221804141998291 train acc: 0.9825000166893005 test loss: 1.0359877347946167 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4945 train loss: 0.9221860766410828 train acc: 0.9825000166893005 test loss: 1.0400232076644897 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4946 train loss: 0.9222255945205688 train acc: 0.9825000166893005 test loss: 1.0316792726516724 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4947 train loss: 0.9222546219825745 train acc: 0.9825000166893005 test loss: 1.037988305091858 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4948 train loss: 0.9221920967102051 train acc: 0.9825000166893005 test loss: 1.0403043031692505 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4949 train loss: 0.9221971035003662 train acc: 0.9825000166893005 test loss: 1.03633713722229 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4950 train loss: 0.9221919178962708 train acc: 0.9825000166893005 test loss: 1.0474034547805786 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4951 train loss: 0.9222410321235657 train acc: 0.9825000166893005 test loss: 1.0427943468093872 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4952 train loss: 0.9221916198730469 train acc: 0.9825000166893005 test loss: 1.0437877178192139 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4953 train loss: 0.9222257733345032 train acc: 0.9825000166893005 test loss: 1.0317497253417969 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4954 train loss: 0.9222683906555176 train acc: 0.9825000166893005 test loss: 1.05180025100708 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4955 train loss: 0.9221804738044739 train acc: 0.9825000166893005 test loss: 1.041458249092102 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4956 train loss: 0.922214686870575 train acc: 0.9825000166893005 test loss: 1.0423250198364258 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4957 train loss: 0.9222620129585266 train acc: 0.9825000166893005 test loss: 1.0420511960983276 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4958 train loss: 0.9222167730331421 train acc: 0.9825000166893005 test loss: 1.030178427696228 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4959 train loss: 0.9222242832183838 train acc: 0.9825000166893005 test loss: 1.0417852401733398 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4960 train loss: 0.9221804738044739 train acc: 0.9825000166893005 test loss: 1.0465630292892456 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4961 train loss: 0.9222453236579895 train acc: 0.9825000166893005 test loss: 1.0441181659698486 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4962 train loss: 0.9222513437271118 train acc: 0.9825000166893005 test loss: 1.0371549129486084 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4963 train loss: 0.9222801327705383 train acc: 0.9825000166893005 test loss: 1.038343071937561 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4964 train loss: 0.9222122430801392 train acc: 0.9825000166893005 test loss: 1.041585922241211 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4965 train loss: 0.9222003221511841 train acc: 0.9825000166893005 test loss: 1.0369036197662354 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4966 train loss: 0.9222017526626587 train acc: 0.9825000166893005 test loss: 1.0448086261749268 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4967 train loss: 0.9221932888031006 train acc: 0.9825000166893005 test loss: 1.0356850624084473 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4968 train loss: 0.9222578406333923 train acc: 0.9825000166893005 test loss: 1.0344041585922241 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4969 train loss: 0.9221806526184082 train acc: 0.9825000166893005 test loss: 1.0394988059997559 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4970 train loss: 0.9221813082695007 train acc: 0.9825000166893005 test loss: 1.0448262691497803 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4971 train loss: 0.9222381711006165 train acc: 0.9825000166893005 test loss: 1.04779851436615 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4972 train loss: 0.922294020652771 train acc: 0.9825000166893005 test loss: 1.0382977724075317 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4973 train loss: 0.9221997261047363 train acc: 0.9825000166893005 test loss: 1.043504238128662 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4974 train loss: 0.9221851825714111 train acc: 0.9825000166893005 test loss: 1.0449215173721313 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4975 train loss: 0.9222126007080078 train acc: 0.9825000166893005 test loss: 1.0334575176239014 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4976 train loss: 0.9222347140312195 train acc: 0.9825000166893005 test loss: 1.033147931098938 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4977 train loss: 0.9222447276115417 train acc: 0.9825000166893005 test loss: 1.04265558719635 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4978 train loss: 0.9222177863121033 train acc: 0.9825000166893005 test loss: 1.038723349571228 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4979 train loss: 0.922191858291626 train acc: 0.9825000166893005 test loss: 1.0457135438919067 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4980 train loss: 0.9222826361656189 train acc: 0.9825000166893005 test loss: 1.0400176048278809 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4981 train loss: 0.9222975373268127 train acc: 0.9825000166893005 test loss: 1.0375651121139526 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4982 train loss: 0.9221765398979187 train acc: 0.9825000166893005 test loss: 1.0369558334350586 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4983 train loss: 0.9222180247306824 train acc: 0.9825000166893005 test loss: 1.0391740798950195 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4984 train loss: 0.9222215414047241 train acc: 0.9825000166893005 test loss: 1.0428452491760254 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4985 train loss: 0.9222221970558167 train acc: 0.9825000166893005 test loss: 1.041430950164795 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4986 train loss: 0.9222613573074341 train acc: 0.9825000166893005 test loss: 1.0302824974060059 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4987 train loss: 0.922207772731781 train acc: 0.9825000166893005 test loss: 1.0420119762420654 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4988 train loss: 0.9222070574760437 train acc: 0.9825000166893005 test loss: 1.0416138172149658 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4989 train loss: 0.9223009347915649 train acc: 0.9825000166893005 test loss: 1.0331249237060547 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4990 train loss: 0.9221804141998291 train acc: 0.9825000166893005 test loss: 1.0390278100967407 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4991 train loss: 0.9222627282142639 train acc: 0.9825000166893005 test loss: 1.044212818145752 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4992 train loss: 0.9221487641334534 train acc: 0.9825000166893005 test loss: 1.0275425910949707 best test loss: 1.0130972862243652 test acc: 0.8799999952316284\n",
      "Epoch 4993 train loss: 0.9222156405448914 train acc: 0.9825000166893005 test loss: 1.0468255281448364 best test loss: 1.0130972862243652 test acc: 0.8500000238418579\n",
      "Epoch 4994 train loss: 0.9222090840339661 train acc: 0.9825000166893005 test loss: 1.0354702472686768 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4995 train loss: 0.922243058681488 train acc: 0.9825000166893005 test loss: 1.0412112474441528 best test loss: 1.0130972862243652 test acc: 0.8600000143051147\n",
      "Epoch 4996 train loss: 0.9221881031990051 train acc: 0.9825000166893005 test loss: 1.0374839305877686 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4997 train loss: 0.9221118688583374 train acc: 0.9825000166893005 test loss: 1.0341912508010864 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4998 train loss: 0.9222772717475891 train acc: 0.9825000166893005 test loss: 1.0365290641784668 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n",
      "Epoch 4999 train loss: 0.9218870401382446 train acc: 0.9825000166893005 test loss: 1.0288211107254028 best test loss: 1.0130972862243652 test acc: 0.8700000047683716\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "for epoch in range(5000):\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model(inputs_train)\n",
    "    train_loss = criterion(preds_train, outputs_train)\n",
    "    train_loss.backward()\n",
    "    train_acc = (preds_train.argmax(1) == outputs_train.argmax(1)).float().mean()\n",
    "    optimizer.step()\n",
    "\n",
    "    preds_test = model(inputs_test)\n",
    "    test_loss = criterion(preds_test, outputs_test)\n",
    "    test_acc = (preds_test.argmax(1) == outputs_test.argmax(1)).float().mean()\n",
    "    if test_loss < best_test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'server/action_classifier.pt')\n",
    "\n",
    "    print('Epoch {} train loss: {} train acc: {} test loss: {} best test loss: {} test acc: {}'.format(epoch, train_loss, train_acc, test_loss, best_test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActionClassifier(\n",
       "  (fc1): Linear(in_features=768, out_features=384, bias=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (fc2): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (fc3): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout3): Dropout(p=0.2, inplace=False)\n",
       "  (fc4): Linear(in_features=384, out_features=384, bias=True)\n",
       "  (dropout4): Dropout(p=0.2, inplace=False)\n",
       "  (fc5): Linear(in_features=384, out_features=5, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_classifier = ActionClassifier()\n",
    "action_classifier.load_state_dict(torch.load('server/action_classifier.pt'))\n",
    "action_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(text):\n",
    "    input_ = torch.Tensor(encoder.encode([text]))\n",
    "    output = action_classifier(input_)\n",
    "    return classes[output.argmax(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "treehacks24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
